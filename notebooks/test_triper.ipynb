{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "376e8ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wly/.conda/envs/triper/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/wly/szl_all_code/triper-project')\n",
    "from triper.model.builder import from_pretrained_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f26ffbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=1,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87659257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Building Triper model from components...\n",
      "   LLaVA model: /sda1/llava-v1.5-13b\n",
      "   Audio encoder: /sda1/glm-4-voice-tokenizer\n",
      "   Audio projector: Built from config\n",
      "   Freeze LLaVA: True\n",
      "üîÑ Loading LLaVA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:56<00:00, 18.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLaVA model loaded: LlavaLlamaForCausalLM\n",
      "üîÑ Building audio encoder...\n",
      "üîÑ Building audio encoder: whisper_vq\n",
      "‚úÖ WhisperVQEncoder loaded from /sda1/glm-4-voice-tokenizer\n",
      "   Actual hidden size: 1280\n",
      "üîÑ Moving audio encoder to device: cuda:0\n",
      "üîí Audio encoder parameters frozen\n",
      "‚úÖ Audio encoder built and moved to cuda:0: WhisperVQEncoder\n",
      "üîÑ Creating Triper model...\n",
      "üîÑ Building audio projector...\n",
      "üîß AudioProjector config:\n",
      "  audio_hidden_size: 1280\n",
      "  hidden_size: 5120\n",
      "  projector_type: mlp2x_gelu\n",
      "‚úÖ AudioProjector created successfully\n",
      "‚úÖ Audio projector built: AudioProjector\n",
      "‚úÖ TriperModel initialized with config: triper\n",
      "üîÑ Moving Triper model to device: cuda:0\n",
      "‚úÖ LLaVA model attached: LlavaLlamaForCausalLM\n",
      "üîí LLaVA model parameters frozen\n",
      "üéµ Audio encoder attached: WhisperVQEncoder\n",
      "üì¶ Components set: tokenizer(LlamaTokenizer), processor(CLIPImageProcessor), context_len(2048)\n",
      "‚úÖ Triper model created successfully!\n",
      "\n",
      "üèóÔ∏è  Triper Model Summary\n",
      "============================================================\n",
      "üì¶ Components:\n",
      "  ü¶ô LLaVA: ‚úÖ (LlavaLlamaForCausalLM)\n",
      "  üéµ Audio Encoder: ‚úÖ (WhisperVQEncoder) üîí External (Frozen)\n",
      "  üîó Audio Projector: ‚úÖ (AudioProjector) üîì Trainable\n",
      "  üìù Tokenizer: ‚úÖ (LlamaTokenizer) üîí External\n",
      "  üñºÔ∏è Image Processor: ‚úÖ (CLIPImageProcessor) üîí External\n",
      "\n",
      "üìä Trainable Parameters:\n",
      "  Total: 13,383,627,776\n",
      "  Trainable: 32,788,480 (0.2%)\n",
      "    llava: 13,350,839,296 (0.0% trainable) üîí\n",
      "    audio_projector: 32,788,480 (100.0% trainable) üîì\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 13383627776,\n",
       " 'trainable_params': 32788480,\n",
       " 'components': {'llava': {'total': 13350839296, 'trainable': 0},\n",
       "  'audio_projector': {'total': 32788480, 'trainable': 32788480}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_config = {\n",
    "    'mm_audio_encoder': 'whisper_vq',\n",
    "    'audio_hidden_size': 1280,  # WhisperËæìÂá∫Áª¥Â∫¶\n",
    "    'audio_model_path': '/sda1/glm-4-voice-tokenizer',\n",
    "    'audio_projector_type': 'mlp2x_gelu',\n",
    "    'audio_projector_hidden_dim': 2048,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "tokenizer, triper_model, image_processor, context_len, audio_encoder = from_pretrained_components(\n",
    "    llava_model_path=\"/sda1/llava-v1.5-13b\",\n",
    "    audio_encoder_path=\"/sda1/glm-4-voice-tokenizer\",\n",
    "    audio_projector_path=None,\n",
    "    audio_config=audio_config,\n",
    "    freeze_llava=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "triper_model.get_parameter_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4db79e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéµ Audio features device after extraction: cuda:0\n",
      "Audio input shape: torch.Size([1, 375, 1280])\n"
     ]
    }
   ],
   "source": [
    "audio_path = '/home/wly/szl_all_code/triper-project/tests/audio.wav'\n",
    "audio_input = audio_encoder(audio_path)\n",
    "print(f\"Audio input shape: {audio_input.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b744322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: (336, 336)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "# Âä†ËΩΩÂõæÂÉè\n",
    "def load_image(image_path):\n",
    "    \"\"\"Âä†ËΩΩÂπ∂È¢ÑÂ§ÑÁêÜÂõæÂÉè\"\"\"\n",
    "    image = Image.open(image_path).convert('RGB').resize((336, 336))\n",
    "    return image\n",
    "\n",
    "image_path = '/home/wly/szl_all_code/triper-project/tests/cat.jpg'\n",
    "image = load_image(image_path)\n",
    "print(f\"Image size: {image.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e5f62e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.mm_utils import get_model_name_from_path, tokenizer_image_token, process_images\n",
    "# Ëé∑ÂèñÊ®°ÂûãÈÖçÁΩÆ\n",
    "model_cfg = triper_model.config if hasattr(triper_model, \"config\") else None\n",
    "\n",
    "# Â§ÑÁêÜÂõæÂÉèÂπ∂Ëé∑ÂèñÂõæÂÉèÂº†Èáè\n",
    "image_tensor = process_images([image], image_processor, model_cfg=model_cfg)[0]\n",
    "image_tensor = image_tensor.unsqueeze(0)  # Ê∑ªÂä†ÊâπÊ¨°Áª¥Â∫¶\n",
    "\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "prompt = \"USER: <image>\\n What's the content of the image? ASSISTANT:\"\n",
    "\n",
    "# tokenizer Â§ÑÁêÜÊñáÊú¨\n",
    "input_ids = tokenizer_image_token(\n",
    "    prompt,\n",
    "    tokenizer=tokenizer,\n",
    "    image_token_index=IMAGE_TOKEN_INDEX,\n",
    "    return_tensors='pt',\n",
    ").to(triper_model.device)\n",
    "if len(input_ids.shape) == 1:\n",
    "    input_ids = input_ids.unsqueeze(0)  # Ê∑ªÂä†ÊâπÊ¨°Áª¥Â∫¶\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba0d7bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéµ Processing audio features:\n",
      "  Input audio shape: torch.Size([1, 375, 1280])\n",
      "  Input audio dtype: torch.float32\n",
      "  Input audio device: cuda:0\n",
      "üéµ Audio features device (passthrough): cuda:0\n",
      "  Target dtype: torch.bfloat16\n",
      "  Encoded audio dtype: torch.float32\n",
      "  üîÑ Converting encoded audio to torch.bfloat16\n",
      "üéµ AudioProjector forward:\n",
      "  Input shape: torch.Size([1, 375, 1280])\n",
      "  Input dtype: torch.bfloat16\n",
      "  Input device: cuda:0\n",
      "  Model dtype: torch.float32\n",
      "  üîÑ Converting input from torch.bfloat16 to torch.float32\n",
      "  Output shape: torch.Size([1, 375, 5120])\n",
      "  Output dtype: torch.float32\n",
      "  Output device: cuda:0\n",
      "  Audio embeds shape: torch.Size([1, 375, 5120])\n",
      "  Audio embeds dtype: torch.float32\n",
      "  Inputs embeds shape: torch.Size([1, 597, 5120])\n",
      "  Inputs embeds dtype: torch.bfloat16\n",
      "  üîÑ Converting audio embeds to match inputs_embeds dtype\n",
      "  Final result shape: torch.Size([1, 972, 5120])\n",
      "  Final result dtype: torch.bfloat16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 972, 32000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = triper_model(\n",
    "    input_ids=input_ids,\n",
    "    images=image_tensor,\n",
    "    audio_features=audio_input\n",
    ")\n",
    "result['logits'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
