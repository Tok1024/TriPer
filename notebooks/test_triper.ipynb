{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "376e8ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wly/.conda/envs/triper/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/wly/szl_all_code/triper-project')\n",
    "from triper.model.builder import from_pretrained_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87659257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Building Triper model from components...\n",
      "   LLaVA model: /sda1/llava-v1.5-13b\n",
      "   Audio encoder: /sda1/glm-4-voice-tokenizer\n",
      "   Audio projector: Built from config\n",
      "   Freeze LLaVA: True\n",
      "ğŸ”„ Loading LLaVA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLaVA model loaded: LlavaLlamaForCausalLM\n",
      "ğŸ”„ Building audio encoder...\n",
      "ğŸ”„ Building audio encoder: whisper_vq\n",
      "âœ… WhisperVQEncoder loaded from /sda1/glm-4-voice-tokenizer\n",
      "   Actual hidden size: 1280\n",
      "ğŸ”„ Moving audio encoder to device: cuda:0\n",
      "ğŸ”’ Audio encoder parameters frozen\n",
      "âœ… Audio encoder built and moved to cuda:0: WhisperVQEncoder\n",
      "ğŸ”„ Creating Triper model...\n",
      "ğŸ”„ Building audio projector...\n",
      "ğŸ”§ AudioProjector config:\n",
      "  audio_hidden_size: 1280\n",
      "  hidden_size: 5120\n",
      "  projector_type: mlp2x_gelu\n",
      "âœ… AudioProjector created successfully\n",
      "âœ… Audio projector built: AudioProjector\n",
      "âœ… TriperModel initialized with config: triper\n",
      "ğŸ”„ Moving Triper model to device: cuda:0\n",
      "âœ… LLaVA model attached: LlavaLlamaForCausalLM\n",
      "ğŸ”’ LLaVA model parameters frozen\n",
      "ğŸµ Audio encoder attached: WhisperVQEncoder\n",
      "ğŸ“¦ Components set: tokenizer(LlamaTokenizer), processor(CLIPImageProcessor), context_len(2048)\n",
      "âœ… Triper model created successfully!\n",
      "\n",
      "ğŸ—ï¸  Triper Model Summary\n",
      "============================================================\n",
      "ğŸ“¦ Components:\n",
      "  ğŸ¦™ LLaVA: âœ… (LlavaLlamaForCausalLM)\n",
      "  ğŸµ Audio Encoder: âœ… (WhisperVQEncoder) ğŸ”’ External (Frozen)\n",
      "  ğŸ”— Audio Projector: âœ… (AudioProjector) ğŸ”“ Trainable\n",
      "  ğŸ“ Tokenizer: âœ… (LlamaTokenizer) ğŸ”’ External\n",
      "  ğŸ–¼ï¸ Image Processor: âœ… (CLIPImageProcessor) ğŸ”’ External\n",
      "\n",
      "ğŸ“Š Trainable Parameters:\n",
      "  Total: 13,383,627,776\n",
      "  Trainable: 32,788,480 (0.2%)\n",
      "    llava: 13,350,839,296 (0.0% trainable) ğŸ”’\n",
      "    audio_projector: 32,788,480 (100.0% trainable) ğŸ”“\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 13383627776,\n",
       " 'trainable_params': 32788480,\n",
       " 'components': {'llava': {'total': 13350839296, 'trainable': 0},\n",
       "  'audio_projector': {'total': 32788480, 'trainable': 32788480}}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_config = {\n",
    "    'mm_audio_encoder': 'whisper_vq',\n",
    "    'audio_hidden_size': 1280,  # Whisperè¾“å‡ºç»´åº¦\n",
    "    'audio_model_path': '/sda1/glm-4-voice-tokenizer',\n",
    "    'audio_projector_type': 'mlp2x_gelu',\n",
    "    'audio_projector_hidden_dim': 2048,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "tokenizer, triper_model, image_processor, context_len, audio_encoder = from_pretrained_components(\n",
    "    llava_model_path=\"/sda1/llava-v1.5-13b\",\n",
    "    audio_encoder_path=\"/sda1/glm-4-voice-tokenizer\",\n",
    "    audio_projector_path=None,\n",
    "    audio_config=audio_config,\n",
    "    freeze_llava=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "triper_model.get_parameter_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4db79e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸµ Audio features device after extraction: cuda:0\n",
      "Audio input shape: torch.Size([1, 375, 1280])\n"
     ]
    }
   ],
   "source": [
    "audio_path = '/home/wly/szl_all_code/triper-project/tests/audio.wav'\n",
    "audio_input = audio_encoder(audio_path)\n",
    "print(f\"Audio input shape: {audio_input.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b744322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: (336, 336)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "# åŠ è½½å›¾åƒ\n",
    "def load_image(image_path):\n",
    "    \"\"\"åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ\"\"\"\n",
    "    image = Image.open(image_path).convert('RGB').resize((336, 336))\n",
    "    return image\n",
    "\n",
    "image_path = '/home/wly/szl_all_code/triper-project/tests/cat.jpg'\n",
    "image = load_image(image_path)\n",
    "print(f\"Image size: {image.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e5f62e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.mm_utils import get_model_name_from_path, tokenizer_image_token, process_images\n",
    "# è·å–æ¨¡å‹é…ç½®\n",
    "model_cfg = triper_model.config if hasattr(triper_model, \"config\") else None\n",
    "\n",
    "# å¤„ç†å›¾åƒå¹¶è·å–å›¾åƒå¼ é‡\n",
    "image_tensor = process_images([image], image_processor, model_cfg=model_cfg)[0]\n",
    "image_tensor = image_tensor.unsqueeze(0)  # æ·»åŠ æ‰¹æ¬¡ç»´åº¦\n",
    "\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "prompt = \"USER: <image>\\n What's the content of the image? ASSISTANT:\"\n",
    "\n",
    "# tokenizer å¤„ç†æ–‡æœ¬\n",
    "input_ids = tokenizer_image_token(\n",
    "    prompt,\n",
    "    tokenizer=tokenizer,\n",
    "    image_token_index=IMAGE_TOKEN_INDEX,\n",
    "    return_tensors='pt',\n",
    ").to(triper_model.device)\n",
    "if len(input_ids.shape) == 1:\n",
    "    input_ids = input_ids.unsqueeze(0)  # æ·»åŠ æ‰¹æ¬¡ç»´åº¦\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba0d7bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸµ Processing audio features:\n",
      "  Input audio shape: torch.Size([1, 375, 1280])\n",
      "  Input audio dtype: torch.float32\n",
      "  Input audio device: cuda:0\n",
      "ğŸµ Audio features device (passthrough): cuda:0\n",
      "  Target dtype: torch.bfloat16\n",
      "  Encoded audio dtype: torch.float32\n",
      "  ğŸ”„ Converting encoded audio to torch.bfloat16\n",
      "ğŸµ AudioProjector forward:\n",
      "  Input shape: torch.Size([1, 375, 1280])\n",
      "  Input dtype: torch.bfloat16\n",
      "  Input device: cuda:0\n",
      "  Model dtype: torch.float32\n",
      "  ğŸ”„ Converting input from torch.bfloat16 to torch.float32\n",
      "  Output shape: torch.Size([1, 375, 5120])\n",
      "  Output dtype: torch.float32\n",
      "  Output device: cuda:0\n",
      "  Audio embeds shape: torch.Size([1, 375, 5120])\n",
      "  Audio embeds dtype: torch.float32\n",
      "  Inputs embeds shape: torch.Size([1, 597, 5120])\n",
      "  Inputs embeds dtype: torch.bfloat16\n",
      "  ğŸ”„ Converting audio embeds to match inputs_embeds dtype\n",
      "  Final result shape: torch.Size([1, 972, 5120])\n",
      "  Final result dtype: torch.bfloat16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 972, 32000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = triper_model(\n",
    "    input_ids=input_ids,\n",
    "    images=image_tensor,\n",
    "    audio_features=audio_input\n",
    ")\n",
    "result['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3957414e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨ä»ä»¥ä¸‹è·¯å¾„åŠ è½½æ•°æ®é›†æè¿°æ–‡ä»¶: /home/wly/szl_all_code/triper-project/data/simple_data_20_samples.json\n",
      "å‘ç° 20 ä¸ªæ•°æ®æ ·æœ¬ã€‚\n",
      "è­¦å‘Š: æ— æ³•åŠ è½½éŸ³é¢‘æ–‡ä»¶ /path/to/media/files/00001.wav: Error opening '/path/to/media/files/00001.wav': System error.\n",
      "ä½¿ç”¨é™éŸ³ä½œä¸ºæ›¿ä»£\n",
      "è­¦å‘Š: æ— æ³•åŠ è½½å›¾åƒæ–‡ä»¶ /path/to/media/files/00001.jpg: [Errno 2] No such file or directory: '/path/to/media/files/00001.jpg'\n",
      "ä½¿ç”¨ç©ºç™½å›¾åƒä½œä¸ºæ›¿ä»£\n",
      "{'total_samples': 20, 'missing_audio': 20, 'missing_video': 20, 'empty_conversations': 5, 'emotion_distribution': {'anger': 1, 'surprise': 4, 'disgust': 3, 'neutral': 11, 'happy': 1}, 'speaker_distribution': {'Monica Geller': 5, 'Joey Tribbiani': 2, 'Chandler Bing': 6, 'Phoebe Buffay': 3}}\n"
     ]
    }
   ],
   "source": [
    "from triper.data.triper_dataset import TriperDataset\n",
    "# åˆ›å»ºæ•°æ®é›†å®ä¾‹\n",
    "dataset = TriperDataset(\n",
    "    json_path=\"/home/wly/szl_all_code/triper-project/data/simple_data_20_samples.json\",\n",
    "    media_root_path=\"/path/to/media/files\",\n",
    "    image_processor=image_processor,\n",
    "    audio_processor=audio_encoder\n",
    ")\n",
    "\n",
    "# è·å–å•ä¸ªæ ·æœ¬\n",
    "sample = dataset[0]\n",
    "\n",
    "# éªŒè¯æ•°æ®é›†\n",
    "stats = dataset.validate_dataset()\n",
    "print(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
