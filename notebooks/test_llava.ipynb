{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bce74546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wly/.conda/envs/triper/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/wly/szl_all_code/triper-project')\n",
    "from llava.mm_utils import get_model_name_from_path, tokenizer_image_token, process_images\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "from llava.conversation import conv_templates\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4422fc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.25s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path = '/sda1/llava-v1.5-13b'\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "        model_path=model_path,\n",
    "        model_base=None,\n",
    "        model_name=get_model_name_from_path(model_path),\n",
    "        device_map='auto'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e05ba8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: (336, 336)\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½å›¾åƒ\n",
    "def load_image(image_path):\n",
    "    \"\"\"åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ\"\"\"\n",
    "    image = Image.open(image_path).convert('RGB').resize((336, 336))\n",
    "    return image\n",
    "\n",
    "image_path = '/home/wly/szl_all_code/triper-project/tests/cat.jpg'\n",
    "image = load_image(image_path)\n",
    "print(f\"Image size: {image.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97a17b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è·å–æ¨¡å‹é…ç½®\n",
    "model_cfg = model.config if hasattr(model, \"config\") else None\n",
    "\n",
    "# å¤„ç†å›¾åƒå¹¶è·å–å›¾åƒå¼ é‡\n",
    "image_tensor = process_images([image], image_processor, model_cfg=model_cfg)[0]\n",
    "image_tensor = image_tensor.unsqueeze(0)  # æ·»åŠ æ‰¹æ¬¡ç»´åº¦\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "385513ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"USER: <image>\\n What's the content of the image? ASSISTANT:\"\n",
    "\n",
    "# tokenizer å¤„ç†æ–‡æœ¬\n",
    "input_ids = tokenizer_image_token(\n",
    "    prompt,\n",
    "    tokenizer=tokenizer,\n",
    "    image_token_index=IMAGE_TOKEN_INDEX,\n",
    "    return_tensors='pt',\n",
    ").to(model.device)\n",
    "if len(input_ids.shape) == 1:\n",
    "    input_ids = input_ids.unsqueeze(0)  # æ·»åŠ æ‰¹æ¬¡ç»´åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b5c28e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 597, 5120])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. å‡†å¤‡å¤šæ¨¡æ€è¾“å…¥\n",
    "with torch.no_grad():\n",
    "    # ä½¿ç”¨æ¨¡å‹çš„å¤šæ¨¡æ€å¤„ç†æ–¹æ³•\n",
    "    (\n",
    "        _,\n",
    "        position_ids,\n",
    "        attention_mask,\n",
    "        past_key_values,\n",
    "        inputs_embeds,\n",
    "        labels\n",
    "    ) = model.prepare_inputs_labels_for_multimodal(\n",
    "        input_ids=input_ids,\n",
    "        position_ids=None,\n",
    "        attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        labels=None,\n",
    "        images=image_tensor\n",
    "    )\n",
    "    \n",
    "inputs_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c77ec8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å¤šæ¨¡æ€ç‰¹å¾æå–å®Œæˆ\n",
      "è¾“å…¥ embeddings shape: torch.Size([1, 597, 5120])\n",
      "æœ€ç»ˆéšè—çŠ¶æ€ shape: torch.Size([1, 597, 5120])\n",
      "éšè—å±‚æ•°é‡: 41\n",
      "æ•°æ®ç±»å‹: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "def get_multimodal_features(model, tokenizer, image_tensor, input_ids):\n",
    "    \"\"\"è·å–å¤šæ¨¡æ€èåˆåçš„ç‰¹å¾å‘é‡\"\"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 1. è·å–èåˆåçš„ embeddings\n",
    "        (\n",
    "            input_ids_processed,\n",
    "            position_ids,\n",
    "            attention_mask,\n",
    "            past_key_values,\n",
    "            inputs_embeds,\n",
    "            labels\n",
    "        ) = model.prepare_inputs_labels_for_multimodal(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=None,\n",
    "            attention_mask=None,\n",
    "            past_key_values=None,\n",
    "            labels=None,\n",
    "            images=image_tensor.unsqueeze(0)  # æ·»åŠ batchç»´åº¦\n",
    "        )\n",
    "        \n",
    "        # 2. é€šè¿‡æ¨¡å‹è·å–è¾“å‡ºï¼ˆåŒ…å«æ‰€æœ‰éšè—çŠ¶æ€ï¼‰\n",
    "        outputs = model.model(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            output_hidden_states=True,  # è·å–æ‰€æœ‰éšè—å±‚\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        return {\n",
    "            'inputs_embeds': inputs_embeds,\n",
    "            'hidden_states': outputs.hidden_states,  # æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€\n",
    "            'last_hidden_state': outputs.last_hidden_state,\n",
    "            'attention_mask': attention_mask,\n",
    "            'position_ids': position_ids,\n",
    "            \n",
    "        }\n",
    "\n",
    "# è·å–å¤šæ¨¡æ€ç‰¹å¾\n",
    "multimodal_features = get_multimodal_features(model, tokenizer, image_tensor, input_ids)\n",
    "\n",
    "print(f\"âœ… å¤šæ¨¡æ€ç‰¹å¾æå–å®Œæˆ\")\n",
    "print(f\"è¾“å…¥ embeddings shape: {multimodal_features['inputs_embeds'].shape}\")\n",
    "print(f\"æœ€ç»ˆéšè—çŠ¶æ€ shape: {multimodal_features['last_hidden_state'].shape}\")\n",
    "print(f\"éšè—å±‚æ•°é‡: {len(multimodal_features['hidden_states'])}\")\n",
    "print(f\"æ•°æ®ç±»å‹: {multimodal_features['last_hidden_state'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87de0990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– æ­£åœ¨ç”ŸæˆLLaVAå“åº”ï¼ˆä½¿ç”¨æ­£ç¡®å‚æ•°æ ¼å¼ï¼‰...\n",
      "è¾“å…¥IDå½¢çŠ¶: torch.Size([1, 22]), å›¾åƒå¼ é‡å½¢çŠ¶: torch.Size([1, 3, 336, 336])\n",
      "âœ… æ­£ç¡®å‚æ•°æ ¼å¼æˆåŠŸï¼\n",
      "=== LLaVA ç”Ÿæˆç»“æœ ===\n",
      "æ ·æœ¬ 1:\n",
      "The image features a cat standing on a wooden floor, eating food from a bowl.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_llava_response_correct(model, tokenizer, image_tensor, input_ids, max_new_tokens=200):\n",
    "    \"\"\"ä½¿ç”¨æ¨¡å‹å®é™…æ”¯æŒçš„å‚æ•°æ ¼å¼\"\"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # æ ¹æ®æ¨¡å‹ç­¾åï¼Œä½¿ç”¨æ­£ç¡®çš„å‚æ•°æ ¼å¼\n",
    "        # inputs å¯¹åº” input_idsï¼Œimages å¯¹åº”å›¾åƒï¼Œimage_sizes å¯¹åº”å›¾åƒå°ºå¯¸\n",
    "        image_sizes = [image_tensor.shape[-2:]]  # [height, width]\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            inputs=input_ids,  # ä½¿ç”¨ inputs è€Œä¸æ˜¯ input_ids\n",
    "            images=image_tensor,\n",
    "            image_sizes=image_sizes,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            use_cache=False,\n",
    "            pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,\n",
    "        )\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# å°è¯•ä½¿ç”¨æ­£ç¡®çš„å‚æ•°æ ¼å¼\n",
    "print(\"ğŸ¤– æ­£åœ¨ç”ŸæˆLLaVAå“åº”ï¼ˆä½¿ç”¨æ­£ç¡®å‚æ•°æ ¼å¼ï¼‰...\")\n",
    "print(f\"è¾“å…¥IDå½¢çŠ¶: {input_ids.shape}, å›¾åƒå¼ é‡å½¢çŠ¶: {image_tensor.shape}\")\n",
    "\n",
    "\n",
    "generated_outputs = generate_llava_response_correct(model, tokenizer, image_tensor, input_ids)\n",
    "print(\"âœ… æ­£ç¡®å‚æ•°æ ¼å¼æˆåŠŸï¼\")\n",
    "\n",
    "# è§£ç è¾“å‡ºæ–‡æœ¬\n",
    "generated_text = tokenizer.batch_decode(generated_outputs, skip_special_tokens=True)\n",
    "\n",
    "print(\"=== LLaVA ç”Ÿæˆç»“æœ ===\")\n",
    "for i, text in enumerate(generated_text):\n",
    "    print(f\"æ ·æœ¬ {i+1}:\")\n",
    "    print(text)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # æå–åªæœ‰Assistantçš„å›ç­”éƒ¨åˆ†\n",
    "    if \"ASSISTANT:\" in text:\n",
    "        assistant_response = text.split(\"ASSISTANT:\")[-1].strip()\n",
    "        print(f\"Assistantå›ç­”: {assistant_response}\")\n",
    "        print(\"-\" * 50)\n",
    "            \n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
