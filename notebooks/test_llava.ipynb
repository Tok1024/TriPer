{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bce74546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wly/.conda/envs/triper/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/wly/szl_all_code/triper-project')\n",
    "from llava.mm_utils import get_model_name_from_path, tokenizer_image_token, process_images\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "from llava.conversation import conv_templates\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4422fc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.25s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path = '/sda1/llava-v1.5-13b'\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "        model_path=model_path,\n",
    "        model_base=None,\n",
    "        model_name=get_model_name_from_path(model_path),\n",
    "        device_map='auto'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e05ba8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: (336, 336)\n"
     ]
    }
   ],
   "source": [
    "# 加载图像\n",
    "def load_image(image_path):\n",
    "    \"\"\"加载并预处理图像\"\"\"\n",
    "    image = Image.open(image_path).convert('RGB').resize((336, 336))\n",
    "    return image\n",
    "\n",
    "image_path = '/home/wly/szl_all_code/triper-project/tests/cat.jpg'\n",
    "image = load_image(image_path)\n",
    "print(f\"Image size: {image.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97a17b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取模型配置\n",
    "model_cfg = model.config if hasattr(model, \"config\") else None\n",
    "\n",
    "# 处理图像并获取图像张量\n",
    "image_tensor = process_images([image], image_processor, model_cfg=model_cfg)[0]\n",
    "image_tensor = image_tensor.unsqueeze(0)  # 添加批次维度\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "385513ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"USER: <image>\\n What's the content of the image? ASSISTANT:\"\n",
    "\n",
    "# tokenizer 处理文本\n",
    "input_ids = tokenizer_image_token(\n",
    "    prompt,\n",
    "    tokenizer=tokenizer,\n",
    "    image_token_index=IMAGE_TOKEN_INDEX,\n",
    "    return_tensors='pt',\n",
    ").to(model.device)\n",
    "if len(input_ids.shape) == 1:\n",
    "    input_ids = input_ids.unsqueeze(0)  # 添加批次维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b5c28e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 597, 5120])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 准备多模态输入\n",
    "with torch.no_grad():\n",
    "    # 使用模型的多模态处理方法\n",
    "    (\n",
    "        _,\n",
    "        position_ids,\n",
    "        attention_mask,\n",
    "        past_key_values,\n",
    "        inputs_embeds,\n",
    "        labels\n",
    "    ) = model.prepare_inputs_labels_for_multimodal(\n",
    "        input_ids=input_ids,\n",
    "        position_ids=None,\n",
    "        attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        labels=None,\n",
    "        images=image_tensor\n",
    "    )\n",
    "    \n",
    "inputs_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c77ec8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 多模态特征提取完成\n",
      "输入 embeddings shape: torch.Size([1, 597, 5120])\n",
      "最终隐藏状态 shape: torch.Size([1, 597, 5120])\n",
      "隐藏层数量: 41\n",
      "数据类型: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "def get_multimodal_features(model, tokenizer, image_tensor, input_ids):\n",
    "    \"\"\"获取多模态融合后的特征向量\"\"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 1. 获取融合后的 embeddings\n",
    "        (\n",
    "            input_ids_processed,\n",
    "            position_ids,\n",
    "            attention_mask,\n",
    "            past_key_values,\n",
    "            inputs_embeds,\n",
    "            labels\n",
    "        ) = model.prepare_inputs_labels_for_multimodal(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=None,\n",
    "            attention_mask=None,\n",
    "            past_key_values=None,\n",
    "            labels=None,\n",
    "            images=image_tensor.unsqueeze(0)  # 添加batch维度\n",
    "        )\n",
    "        \n",
    "        # 2. 通过模型获取输出（包含所有隐藏状态）\n",
    "        outputs = model.model(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            output_hidden_states=True,  # 获取所有隐藏层\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        return {\n",
    "            'inputs_embeds': inputs_embeds,\n",
    "            'hidden_states': outputs.hidden_states,  # 所有层的隐藏状态\n",
    "            'last_hidden_state': outputs.last_hidden_state,\n",
    "            'attention_mask': attention_mask,\n",
    "            'position_ids': position_ids,\n",
    "            \n",
    "        }\n",
    "\n",
    "# 获取多模态特征\n",
    "multimodal_features = get_multimodal_features(model, tokenizer, image_tensor, input_ids)\n",
    "\n",
    "print(f\"✅ 多模态特征提取完成\")\n",
    "print(f\"输入 embeddings shape: {multimodal_features['inputs_embeds'].shape}\")\n",
    "print(f\"最终隐藏状态 shape: {multimodal_features['last_hidden_state'].shape}\")\n",
    "print(f\"隐藏层数量: {len(multimodal_features['hidden_states'])}\")\n",
    "print(f\"数据类型: {multimodal_features['last_hidden_state'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87de0990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 正在生成LLaVA响应（使用正确参数格式）...\n",
      "输入ID形状: torch.Size([1, 22]), 图像张量形状: torch.Size([1, 3, 336, 336])\n",
      "✅ 正确参数格式成功！\n",
      "=== LLaVA 生成结果 ===\n",
      "样本 1:\n",
      "The image features a cat standing on a wooden floor, eating food from a bowl.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_llava_response_correct(model, tokenizer, image_tensor, input_ids, max_new_tokens=200):\n",
    "    \"\"\"使用模型实际支持的参数格式\"\"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 根据模型签名，使用正确的参数格式\n",
    "        # inputs 对应 input_ids，images 对应图像，image_sizes 对应图像尺寸\n",
    "        image_sizes = [image_tensor.shape[-2:]]  # [height, width]\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            inputs=input_ids,  # 使用 inputs 而不是 input_ids\n",
    "            images=image_tensor,\n",
    "            image_sizes=image_sizes,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            use_cache=False,\n",
    "            pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,\n",
    "        )\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# 尝试使用正确的参数格式\n",
    "print(\"🤖 正在生成LLaVA响应（使用正确参数格式）...\")\n",
    "print(f\"输入ID形状: {input_ids.shape}, 图像张量形状: {image_tensor.shape}\")\n",
    "\n",
    "\n",
    "generated_outputs = generate_llava_response_correct(model, tokenizer, image_tensor, input_ids)\n",
    "print(\"✅ 正确参数格式成功！\")\n",
    "\n",
    "# 解码输出文本\n",
    "generated_text = tokenizer.batch_decode(generated_outputs, skip_special_tokens=True)\n",
    "\n",
    "print(\"=== LLaVA 生成结果 ===\")\n",
    "for i, text in enumerate(generated_text):\n",
    "    print(f\"样本 {i+1}:\")\n",
    "    print(text)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 提取只有Assistant的回答部分\n",
    "    if \"ASSISTANT:\" in text:\n",
    "        assistant_response = text.split(\"ASSISTANT:\")[-1].strip()\n",
    "        print(f\"Assistant回答: {assistant_response}\")\n",
    "        print(\"-\" * 50)\n",
    "            \n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
