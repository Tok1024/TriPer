{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d87b3c1",
   "metadata": {},
   "source": [
    "测试音频的tokenize, 将wav转换成向量, 后续和llava的向量进行拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed9dd090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/wly/szl_all_code/triper-project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "314b0643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wly/.conda/envs/triper/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from speech_tokenizer.modeling_whisper import WhisperVQEncoder\n",
    "from speech_tokenizer.utils import extract_speech_token\n",
    "\n",
    "# 1. 设置模型路径和设备\n",
    "tokenizer_path = \"/sda1/glm-4-voice-tokenizer\" # 或者您本地的路径\n",
    "device = \"cuda:0\" \n",
    "\n",
    "# 2. 加载模型和特征提取器\n",
    "whisper_model = WhisperVQEncoder.from_pretrained(tokenizer_path).eval().to(device)\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(tokenizer_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37953c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "音频文件 '/home/wly/szl_all_code/triper-project/tests/介绍大语言模型.wav' 被编码为了 58 个 token。\n",
      "部分 Token 示例: [10815, 5966, 7767, 11760, 14770, 11760, 13229, 11760, 11760, 11760]\n",
      "\n",
      "可用于LLM输入的字符串格式:\n",
      "<|begin_of_audio|><|audio_10815|><|audio_5966|><|audio_7767|><|audio_11760|><|audio_14770|><|audio_11760|><|audio_13229|><|audio_11760|><|audio_11760|><|audio_11760|><|audio_11760|><|audio_11760|><|audio_11760|><|audio_11760|><|audio_11760|><|audio_11760|><|audio_11760|><|audio_15643|><|audio_14725|><|audio_2671|><|audio_14164|><|audio_8431|><|audio_13786|><|audio_12459|><|audio_10426|><|audio_4811|><|audio_5242|><|audio_14023|><|audio_1878|><|audio_5024|><|audio_7393|><|audio_16240|><|audio_12515|><|audio_9761|><|audio_8572|><|audio_5736|><|audio_15485|><|audio_12607|><|audio_14023|><|audio_3192|><|audio_14066|><|audio_3207|><|audio_5460|><|audio_4278|><|audio_13305|><|audio_10977|><|audio_12037|><|audio_13472|><|audio_5539|><|audio_1656|><|audio_4898|><|audio_9374|><|audio_15513|><|audio_15513|><|audio_3616|><|audio_13229|><|audio_11760|><|audio_4978|><|end_of_audio|>\n"
     ]
    }
   ],
   "source": [
    "# 3. 准备音频文件\n",
    "audio_paths = [\"/home/wly/szl_all_code/triper-project/tests/介绍大语言模型.wav\"] \n",
    "\n",
    "# 4. 提取音频 token\n",
    "# 这个函数会处理所有必要的步骤：加载、重采样、特征提取、编码\n",
    "audio_tokens_list = extract_speech_token(whisper_model, feature_extractor, audio_paths)\n",
    "\n",
    "# 5. 查看结果\n",
    "# audio_tokens_list 是一个列表，每个元素对应一个输入音频的 token 序列\n",
    "audio_tokens = audio_tokens_list[0] \n",
    "print(f\"音频文件 '{audio_paths[0]}' 被编码为了 {len(audio_tokens)} 个 token。\")\n",
    "print(\"部分 Token 示例:\", audio_tokens[:10])\n",
    "\n",
    "# 将 token 序列转换为字符串格式，以便输入到LLM\n",
    "audio_tokens_str = \"\".join([f\"<|audio_{x}|>\" for x in audio_tokens])\n",
    "audio_tokens_str = \"<|begin_of_audio|>\" + audio_tokens_str + \"<|end_of_audio|>\"\n",
    "print(\"\\n可用于LLM输入的字符串格式:\")\n",
    "print(audio_tokens_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e00b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "音频连续特征形状: torch.Size([1, 375, 1280])\n",
      "注意力掩码形状: torch.Size([1, 375])\n",
      "特征维度: 1280\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from speech_tokenizer.modeling_whisper import WhisperVQEncoder\n",
    "import torchaudio\n",
    "\n",
    "def extract_audio_features_continuous(model, feature_extractor, audio_path, return_attention_mask=True):\n",
    "    \"\"\"提取音频的连续特征向量\"\"\"\n",
    "    \n",
    "    # 1. 加载音频\n",
    "    audio, sample_rate = torchaudio.load(audio_path)\n",
    "    audio = audio.cuda()\n",
    "    \n",
    "    # 2. 重采样到16kHz\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(\n",
    "            orig_freq=sample_rate, new_freq=16000\n",
    "        ).to('cuda')\n",
    "        audio = resampler(audio)\n",
    "    \n",
    "    # 3. 转为单声道\n",
    "    if audio.shape[0] > 1:\n",
    "        audio = audio.mean(dim=0, keepdim=True)\n",
    "    audio = audio[0].cpu().numpy()\n",
    "    \n",
    "    # 4. 提取mel特征\n",
    "    features = feature_extractor(\n",
    "        audio, \n",
    "        sampling_rate=16000,\n",
    "        return_attention_mask=return_attention_mask, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    features = features.to(device=\"cuda\")\n",
    "    \n",
    "    # 5. 通过编码器获取连续特征\n",
    "    with torch.no_grad():\n",
    "        # 设置quantized_token_ids=None以获取连续特征\n",
    "        outputs = model(\n",
    "            input_features=features.input_features,\n",
    "            attention_mask=features.attention_mask,\n",
    "            quantized_token_ids=None  # 关键：不传入量化的token\n",
    "        )\n",
    "        \n",
    "        # 获取连续的隐藏状态\n",
    "        continuous_features = outputs.last_hidden_state  # [batch, seq_len, hidden_dim]\n",
    "        \n",
    "        if return_attention_mask:\n",
    "            # 计算attention_mask（因为音频可能有padding）\n",
    "            attention_mask = features.attention_mask\n",
    "            # 根据模型的stride调整attention_mask\n",
    "            stride = model.conv1.stride[0] * model.conv2.stride[0]\n",
    "            attention_mask = attention_mask[:, ::stride]\n",
    "            \n",
    "            # 如果有pooling，进一步调整\n",
    "            if hasattr(model.config, 'pooling_kernel_size') and model.config.pooling_kernel_size:\n",
    "                attention_mask = attention_mask[:, ::model.config.pooling_kernel_size]\n",
    "            \n",
    "            return continuous_features, attention_mask\n",
    "        else:\n",
    "            return continuous_features\n",
    "\n",
    "# 使用示例\n",
    "tokenizer_path = \"/sda1/glm-4-voice-tokenizer\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "whisper_model = WhisperVQEncoder.from_pretrained(tokenizer_path).eval().to(device)\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(tokenizer_path)\n",
    "\n",
    "audio_path = \"/home/wly/szl_all_code/triper-project/tests/audio.wav\"\n",
    "\n",
    "# 获取连续特征向量\n",
    "audio_features, attention_mask = extract_audio_features_continuous(\n",
    "    whisper_model, feature_extractor, audio_path\n",
    ")\n",
    "\n",
    "print(f\"音频连续特征形状: {audio_features.shape}\")  # [1, seq_len, hidden_dim]\n",
    "print(f\"注意力掩码形状: {attention_mask.shape}\")\n",
    "print(f\"特征维度: {audio_features.shape[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3718a502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "压缩后音频特征: torch.Size([1, 64, 1280])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AudioCompressor(nn.Module):\n",
    "    \"\"\"可学习的音频特征压缩器\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_seq_len, hidden_dim=None):\n",
    "        super().__init__()\n",
    "        self.output_seq_len = output_seq_len\n",
    "        hidden_dim = hidden_dim or input_dim\n",
    "        \n",
    "        # 注意力池化\n",
    "        self.attention_pool = nn.MultiheadAttention(\n",
    "            embed_dim=input_dim,\n",
    "            num_heads=8,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 可学习的查询向量\n",
    "        self.queries = nn.Parameter(torch.randn(output_seq_len, input_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, input_dim]\n",
    "        Returns:\n",
    "            compressed: [batch, output_seq_len, input_dim]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # 扩展查询向量到batch维度\n",
    "        queries = self.queries.unsqueeze(0).expand(batch_size, -1, -1)  # [batch, output_seq_len, input_dim]\n",
    "        compressed, _ = self.attention_pool(queries, x, x)\n",
    "        \n",
    "        return compressed\n",
    "\n",
    "# 使用示例\n",
    "compressor = AudioCompressor(\n",
    "    input_dim=1280, \n",
    "    output_seq_len=64  # 压缩到64个token\n",
    ").cuda()\n",
    "\n",
    "audio_features_compressed = compressor(audio_features)\n",
    "print(f\"压缩后音频特征: {audio_features_compressed.shape}\")  # [1, 64, 1280]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3977debe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP 投影层, 模仿LLaVA设计\n",
    "class AudioMLP(nn.Module):\n",
    "    \"\"\"音频特征的MLP投影层\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=1280, hidden_dim=2048, output_dim=5120):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(output_dim)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"初始化权重\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # 使用Xavier初始化\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                nn.init.ones_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, input_dim]\n",
    "        Returns:\n",
    "            projected: [batch, seq_len, output_dim]\n",
    "        \"\"\"\n",
    "        projected = self.projector(x)\n",
    "        projected = self.layer_norm(projected)\n",
    "        return projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc859f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP投影后的音频特征: torch.Size([1, 64, 5120])\n"
     ]
    }
   ],
   "source": [
    "mlp_projector = AudioMLP(\n",
    "    input_dim=1280,  # 输入维度与Whisper的输出一致\n",
    "    hidden_dim=2048,  # 隐藏层维度\n",
    "    output_dim=5120  # 最终输出维度\n",
    ").to(device)\n",
    "projected_features = mlp_projector(audio_features_compressed)\n",
    "print(f\"MLP投影后的音频特征: {projected_features.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
