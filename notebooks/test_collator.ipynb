{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80c9e66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wly/.conda/envs/triper/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Building Triper model from components...\n",
      "   LLaVA model: /sda1/llava-v1.5-13b\n",
      "   Audio encoder: /sda1/glm-4-voice-tokenizer\n",
      "   Audio projector: Built from config\n",
      "   Freeze LLaVA: True\n",
      "🔄 Loading LLaVA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.99s/it]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Configuring image tokens...\n",
      "添加图像token: <image>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "更新IMAGE_TOKEN_INDEX为: 32000\n",
      "🔄 Building audio encoder...\n",
      "✅ WhisperVQEncoder loaded from /sda1/glm-4-voice-tokenizer\n",
      "🔄 Moving audio encoder to device: cuda:3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TriperModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔒 Audio encoder parameters frozen\n",
      "✅ Audio encoder built and moved to cuda:3: WhisperVQEncoder\n",
      "🔄 Creating Triper model...\n",
      "🔄 Building audio projector...\n",
      "🔧 AudioProjector config:\n",
      "  audio_hidden_size: 1280\n",
      "  hidden_size: 5120\n",
      "  projector_type: mlp2x_gelu\n",
      "✅ AudioProjector created successfully\n",
      "✅ Audio projector built: AudioProjector\n",
      "✅ TriperModel initialized with config: triper\n",
      "🔄 Moving Triper model to device: cuda:3\n",
      "✅ LLaVA model attached: LlavaLlamaForCausalLM\n",
      "🔒 LLaVA model parameters frozen\n",
      "🎵 Audio encoder attached: WhisperVQEncoder\n",
      "📦 Components set: tokenizer(LlamaTokenizer), processor(CLIPImageProcessor), context_len(2048)\n",
      "✅ Triper model created successfully!\n",
      "\n",
      "🏗️  Triper Model Summary\n",
      "============================================================\n",
      "📦 Components:\n",
      "  🦙 LLaVA: ✅ (LlavaLlamaForCausalLM)\n",
      "  🎵 Audio Encoder: ✅ (WhisperVQEncoder) 🔒 External (Frozen)\n",
      "  🔗 Audio Projector: ✅ (AudioProjector) 🔓 Trainable\n",
      "  📝 Tokenizer: ✅ (LlamaTokenizer) 🔒 External\n",
      "  🖼️ Image Processor: ✅ (CLIPImageProcessor) 🔒 External\n",
      "\n",
      "📊 Trainable Parameters:\n",
      "  Total: 13,383,638,016\n",
      "  Trainable: 32,788,480 (0.2%)\n",
      "    llava: 13,350,849,536 (0.0% trainable) 🔒\n",
      "    audio_projector: 32,788,480 (100.0% trainable) 🔓\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path = [p for p in sys.path if 'triper-project' not in p]\n",
    "sys.path.append('/home/wly/szl_all_code/triper-project')\n",
    "\n",
    "# 清理缓存\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# 加载模型\n",
    "from triper.model import from_pretrained_components\n",
    "from triper.data import TriperDataset, TriperDataCollator\n",
    "\n",
    "audio_config = {\n",
    "    'mm_audio_encoder': 'whisper_vq',\n",
    "    'audio_hidden_size': 1280,\n",
    "    'audio_model_path': '/sda1/glm-4-voice-tokenizer',\n",
    "    'audio_projector_type': 'mlp2x_gelu',\n",
    "    'audio_projector_hidden_dim': 2048,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "tokenizer, triper_model, image_processor, context_len, audio_encoder = from_pretrained_components(\n",
    "    llava_model_path=\"/sda1/llava-v1.5-13b\",\n",
    "    audio_encoder_path=\"/sda1/glm-4-voice-tokenizer\",\n",
    "    audio_projector_path=None,\n",
    "    audio_config=audio_config,\n",
    "    freeze_llava=True,\n",
    "    device_map=\"cuda:3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4efbcfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从以下路径加载数据集描述文件: /home/wly/szl_all_code/triper-project/data/simple_data_20_samples.json\n",
      "发现 20 个数据样本。\n",
      "数据集模式: raw\n",
      "音频文件夹: /home/wly/szl_all_code/triper-project/data/audio\n",
      "视频文件夹: /home/wly/szl_all_code/triper-project/data/video\n",
      "图像文件夹: /home/wly/szl_all_code/triper-project/data/images\n"
     ]
    }
   ],
   "source": [
    "dataset = TriperDataset(\n",
    "    json_path='/home/wly/szl_all_code/triper-project/data/simple_data_20_samples.json',\n",
    "    media_root_path='/home/wly/szl_all_code/triper-project/data',\n",
    ")\n",
    "\n",
    "collator = TriperDataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    image_processor=image_processor,\n",
    "    audio_processor=audio_encoder,\n",
    "    model_cfg=triper_model.llava_model.config\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2480da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Monica Geller (anger): There's nothing to tell! He's just some guy I work with!...\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Joey Tribbiani (surprise): C'mon, you're going out with the guy! There's gotta be something ...\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "No conversation available....\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Chandler Bing (neutral): So does he have a hump? A hump and a hairpiece?...\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "No conversation available....\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Phoebe Buffay (neutral): Wait, does he eat chalk?...\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "No conversation available....\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Phoebe Buffay (neutral): Just, 'cause, I don't want her to go through what I went through wi...\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Monica Geller (neutral): Okay, everybody relax. ...\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Monica Geller (neutral): This is not even a date. It's just two people going out to dinner a...\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Chandler Bing (neutral): Sounds like a date to me....\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "No conversation available....\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Chandler Bing (neutral): Alright, so I'm back in high school, I'm standing in the middle of ...\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Monica Geller (neutral): Oh, yeah. Had that dream....\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Chandler Bing (neutral): Then I look down, and I realize there's a phone... there....\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Joey Tribbiani (neutral): Instead of...?...\n",
      "📝 原始文本长度范围: 8 - 48\n",
      "⚠️ 文本长度不一致，将padding到: 48\n",
      "✅ 批量tokenization完成: input_ids shape: torch.Size([16, 48])\n",
      "✅ 所有样本文本长度统一为: 48\n",
      "✅ 图像处理成功: torch.Size([16, 3, 336, 336])\n",
      "✅ 音频批量处理完成: torch.Size([16, 64, 1280])\n",
      "🔥 TriperModel.forward called:\n",
      "  input_ids: torch.Size([16, 48])\n",
      "  images: torch.Size([16, 3, 336, 336])\n",
      "  audio_features: torch.Size([16, 64, 1280])\n",
      "  past_key_values: 0 layers\n",
      "  📸 LLaVA处理图像...\n",
      "  LLaVA处理后embeds: torch.Size([16, 623, 5120])\n",
      "  🔍 最终验证:\n",
      "    inputs_embeds: torch.Size([16, 623, 5120])\n",
      "    attention_mask: None\n",
      "✅ 批量推理成功！\n",
      "输出logits形状: torch.Size([16, 623, 32001])\n"
     ]
    }
   ],
   "source": [
    "# 测试批量推理\n",
    "batch_size = 16\n",
    "batch_samples = [dataset[i] for i in range(batch_size)]\n",
    "batch_result = collator(batch_samples)\n",
    "\n",
    "# 🔧 安全移动到设备\n",
    "device_batch = {}\n",
    "for k, v in batch_result.items():\n",
    "    if hasattr(v, 'to'):\n",
    "        device_batch[k] = v.to(triper_model.device)\n",
    "    else:\n",
    "        print(f\"保持原值: {k} 类型={type(v)}\")\n",
    "        device_batch[k] = v\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = triper_model(\n",
    "        input_ids=device_batch['input_ids'],\n",
    "        images=device_batch['images'],\n",
    "        audio_features=device_batch['audio_features']\n",
    "        # 🔧 不传递attention_mask和labels，让模型自己处理\n",
    "    )\n",
    "    print(\"✅ 批量推理成功！\")\n",
    "    print(f\"输出logits形状: {output['logits'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6203617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Monica Geller (anger): There's nothing to tell! He's just some guy I work with!...\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Joey Tribbiani (surprise): C'mon, you're going out with the guy! There's gotta be something ...\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "No conversation available....\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Chandler Bing (neutral): So does he have a hump? A hump and a hairpiece?...\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "No conversation available....\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Phoebe Buffay (neutral): Wait, does he eat chalk?...\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "No conversation available....\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Phoebe Buffay (neutral): Just, 'cause, I don't want her to go through what I went through wi...\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Monica Geller (neutral): Okay, everybody relax. ...\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Monica Geller (neutral): This is not even a date. It's just two people going out to dinner a...\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Chandler Bing (neutral): Sounds like a date to me....\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "No conversation available....\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Chandler Bing (neutral): Alright, so I'm back in high school, I'm standing in the middle of ...\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Monica Geller (neutral): Oh, yeah. Had that dream....\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Chandler Bing (neutral): Then I look down, and I realize there's a phone... there....\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Joey Tribbiani (neutral): Instead of...?...\n",
      "📝 原始文本长度范围: 8 - 48\n",
      "⚠️ 文本长度不一致，将padding到: 48\n",
      "✅ 批量tokenization完成: input_ids shape: torch.Size([16, 48])\n",
      "✅ 所有样本文本长度统一为: 48\n",
      "✅ 图像处理成功: torch.Size([16, 3, 336, 336])\n",
      "✅ 音频批量处理完成: torch.Size([16, 64, 1280])\n",
      "🚀 TriperModel.generate called:\n",
      "  input_ids: torch.Size([16, 48])\n",
      "  images: torch.Size([16, 3, 336, 336])\n",
      "  audio_features: torch.Size([16, 64, 1280])\n",
      "🎵 检测到音频输入，准备多模态embeddings...\n",
      "📸 LLaVA处理图像...\n",
      "LLaVA处理后embeds: torch.Size([16, 623, 5120])\n",
      "🎵 集成音频特征...\n",
      "🔄 AudioProjector forward called with input shape: torch.Size([16, 64, 1280])\n",
      "🎵 音频特征插入完成:\n",
      "  原始embeds: torch.Size([16, 623, 5120])\n",
      "  音频embeds: torch.Size([16, 64, 5120])\n",
      "  合并后embeds: torch.Size([16, 687, 5120])\n",
      "  合并后attention_mask: torch.Size([16, 687])\n",
      "最终embeds: torch.Size([16, 687, 5120])\n",
      "最终attention_mask: torch.Size([16, 687])\n",
      "🚀 调用LLaVA.generate with inputs_embeds...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 290.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 225.88 MiB is free. Including non-PyTorch memory, this process has 47.29 GiB memory in use. Of the allocated memory 41.63 GiB is allocated by PyTorch, and 5.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m         device_batch[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 在您的notebook中修改这部分\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mtriper_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 现在可以直接传入\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maudio_features\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m生成结果形状: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# 🔧 修复：由于generate_with_embeds只返回新生成的tokens\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# 我们需要根据实际情况来解码\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/szl_all_code/triper-project/triper/model/triper_arch.py:465\u001b[0m, in \u001b[0;36mTriperModel.generate\u001b[0;34m(self, input_ids, images, image_sizes, audio_features, attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🚀 调用LLaVA.generate with inputs_embeds...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# 🔧 注意：LLaVA的generate方法内部会调用super().generate()\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# 而super().generate()是transformers的标准generate，支持inputs_embeds\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllava_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__bases__\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllava_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/transformers/generation/utils.py:2597\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2589\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2590\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2591\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2592\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2593\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2594\u001b[0m     )\n\u001b[1;32m   2596\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2597\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2598\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2608\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2609\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2610\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2611\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2612\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2613\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2614\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/transformers/generation/utils.py:3557\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3554\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3557\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3558\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/szl_all_code/triper-project/llava/model/language_model/llava_llama.py:90\u001b[0m, in \u001b[0;36mLlavaLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, image_sizes, return_dict, cache_position)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     (\n\u001b[1;32m     74\u001b[0m         input_ids,\n\u001b[1;32m     75\u001b[0m         position_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m         image_sizes\n\u001b[1;32m     88\u001b[0m     )\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:688\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    684\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    685\u001b[0m )\n\u001b[1;32m    687\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:453\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    451\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 453\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/transformers/modeling_layers.py:48\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:324\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    323\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 324\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    327\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:162\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 162\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 290.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 225.88 MiB is free. Including non-PyTorch memory, this process has 47.29 GiB memory in use. Of the allocated memory 41.63 GiB is allocated by PyTorch, and 5.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# 测试批量推理\n",
    "batch_size = 16\n",
    "batch_samples = [dataset[i] for i in range(batch_size)]\n",
    "batch_result = collator(batch_samples)\n",
    "\n",
    "# 🔧 安全移动到设备\n",
    "device_batch = {}\n",
    "for k, v in batch_result.items():\n",
    "    if hasattr(v, 'to'):\n",
    "        device_batch[k] = v.to(triper_model.device)\n",
    "    else:\n",
    "        print(f\"保持原值: {k} 类型={type(v)}\")\n",
    "        device_batch[k] = v\n",
    "        \n",
    "# 在您的notebook中修改这部分\n",
    "response = triper_model.generate(\n",
    "    input_ids=device_batch['input_ids'],\n",
    "    attention_mask=device_batch['attention_mask'],  # 现在可以直接传入\n",
    "    images=device_batch['images'],\n",
    "    audio_features=device_batch['audio_features'],\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.1,\n",
    "    do_sample=True,\n",
    "    top_p=0.8,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(f\"生成结果形状: {response.shape}\")\n",
    "\n",
    "# 🔧 修复：由于generate_with_embeds只返回新生成的tokens\n",
    "# 我们需要根据实际情况来解码\n",
    "if response.shape[1] == 50:  # 只是新生成的tokens\n",
    "    generated_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "    print(f\"生成的文本: '{generated_text}'\")\n",
    "else:  # 完整序列\n",
    "    original_len = device_batch['input_ids'].shape[1]\n",
    "    generated_part = response[0, original_len:]\n",
    "    generated_text = tokenizer.decode(generated_part, skip_special_tokens=True)\n",
    "    print(f\"生成的文本: '{generated_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8033320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 测试单个样本生成...\n",
      "📝 对话预测格式:\n",
      "<image>\n",
      "USER: Based on what you see and hear in this scene, what would Chandler Bing say?\n",
      "ASSISTANT:\n",
      "📝 原始文本长度范围: 31 - 31\n",
      "✅ 批量tokenization完成: input_ids shape: torch.Size([1, 31])\n",
      "✅ 所有样本文本长度统一为: 31\n",
      "✅ 图像处理成功: torch.Size([1, 3, 336, 336])\n",
      "✅ 音频批量处理完成: torch.Size([1, 64, 1280])\n",
      "单样本数据形状:\n",
      "  input_ids: torch.Size([1, 31])\n",
      "  attention_mask: torch.Size([1, 31])\n",
      "  images: torch.Size([1, 3, 336, 336])\n",
      "  audio_features: torch.Size([1, 64, 1280])\n",
      "修复后attention_mask: torch.Size([1, 31])\n",
      "🚀 TriperModel.generate called:\n",
      "  input_ids: torch.Size([1, 31])\n",
      "  images: torch.Size([1, 3, 336, 336])\n",
      "  audio_features: torch.Size([1, 64, 1280])\n",
      "🎵 检测到音频输入，准备多模态embeddings...\n",
      "📸 LLaVA处理图像...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaVA处理后embeds: torch.Size([1, 606, 5120])\n",
      "🎵 集成音频特征...\n",
      "🔄 AudioProjector forward called with input shape: torch.Size([1, 64, 1280])\n",
      "🎵 音频特征插入完成:\n",
      "  原始embeds: torch.Size([1, 606, 5120])\n",
      "  音频embeds: torch.Size([1, 64, 5120])\n",
      "  合并后embeds: torch.Size([1, 670, 5120])\n",
      "  合并后attention_mask: torch.Size([1, 670])\n",
      "最终embeds: torch.Size([1, 670, 5120])\n",
      "最终attention_mask: torch.Size([1, 670])\n",
      "🚀 调用LLaVA.generate with inputs_embeds...\n",
      "生成结果形状: torch.Size([1, 1])\n",
      "生成的文本: ''\n",
      "完整对话: ''\n"
     ]
    }
   ],
   "source": [
    "# 🔧 先测试单个样本，避免批量问题\n",
    "print(\"🧪 测试单个样本生成...\")\n",
    "\n",
    "# 取第一个样本\n",
    "single_sample = [dataset[10]]\n",
    "single_batch = collator(single_sample)\n",
    "\n",
    "# 移动到设备\n",
    "single_device_batch = {}\n",
    "for k, v in single_batch.items():\n",
    "    if hasattr(v, 'to'):\n",
    "        single_device_batch[k] = v.to(triper_model.device)\n",
    "    else:\n",
    "        single_device_batch[k] = v\n",
    "\n",
    "print(f\"单样本数据形状:\")\n",
    "print(f\"  input_ids: {single_device_batch['input_ids'].shape}\")\n",
    "print(f\"  attention_mask: {single_device_batch['attention_mask'].shape}\")\n",
    "print(f\"  images: {single_device_batch['images'].shape}\")\n",
    "print(f\"  audio_features: {single_device_batch['audio_features'].shape}\")\n",
    "\n",
    "# 🔧 修复attention_mask长度问题\n",
    "text_len = single_device_batch['input_ids'].shape[1]\n",
    "text_attention_mask = single_device_batch['attention_mask'][:, :text_len]\n",
    "\n",
    "print(f\"修复后attention_mask: {text_attention_mask.shape}\")\n",
    "\n",
    "# 测试生成\n",
    "response = triper_model.generate(\n",
    "    input_ids=single_device_batch['input_ids'],\n",
    "    attention_mask=text_attention_mask,  # 使用修复后的attention_mask\n",
    "    images=single_device_batch['images'],\n",
    "    audio_features=single_device_batch['audio_features'],\n",
    "    max_new_tokens=512,  # 先用少量token测试\n",
    "    temperature=0.1,\n",
    "    do_sample=False,  # 先用贪心搜索\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(f\"生成结果形状: {response.shape}\")\n",
    "\n",
    "# 解码结果\n",
    "original_len = single_device_batch['input_ids'].shape[1]\n",
    "if response.shape[1] > original_len:\n",
    "    # 返回完整序列，截取新生成的部分\n",
    "    generated_part = response[0, original_len:]\n",
    "    generated_text = tokenizer.decode(generated_part, skip_special_tokens=True)\n",
    "    print(f\"新生成的文本: '{generated_text}'\")\n",
    "else:\n",
    "    # 只返回新生成的tokens\n",
    "    generated_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "    print(f\"生成的文本: '{generated_text}'\")\n",
    "\n",
    "# 显示完整对话\n",
    "full_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "print(f\"完整对话: '{full_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "641dff13",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'single_device_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pure_llava_response \u001b[38;5;241m=\u001b[39m triper_model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m----> 2\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39m\u001b[43msingle_device_batch\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      3\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mtext_attention_mask,\n\u001b[1;32m      4\u001b[0m     images\u001b[38;5;241m=\u001b[39msingle_device_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      5\u001b[0m     audio_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# 不传音频\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m      7\u001b[0m     min_length\u001b[38;5;241m=\u001b[39msingle_device_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      8\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m,\n\u001b[1;32m     10\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m     11\u001b[0m     pad_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[1;32m     12\u001b[0m     eos_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m     13\u001b[0m     early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m纯LLaVA生成结果: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpure_llava_response\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pure_llava_response\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m original_len:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'single_device_batch' is not defined"
     ]
    }
   ],
   "source": [
    "pure_llava_response = triper_model.generate(\n",
    "    input_ids=single_device_batch['input_ids'],\n",
    "    attention_mask=text_attention_mask,\n",
    "    images=single_device_batch['images'],\n",
    "    audio_features=None,  # 不传音频\n",
    "    max_new_tokens=50,\n",
    "    min_length=single_device_batch['input_ids'].shape[1] + 10,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    early_stopping=False,\n",
    ")\n",
    "\n",
    "print(f\"纯LLaVA生成结果: {pure_llava_response.shape}\")\n",
    "\n",
    "if pure_llava_response.shape[1] > original_len:\n",
    "    pure_text = tokenizer.decode(pure_llava_response[0, original_len:], skip_special_tokens=True)\n",
    "    print(f\"纯LLaVA生成文本: '{pure_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "025823bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "简单prompt: 'USER: What do you see in this image?\n",
      "ASSISTANT:'\n",
      "简单input_ids: torch.Size([1, 17])\n",
      "单独图文生成结果: torch.Size([1, 82])\n",
      "生成文本: '. The person appears to be engaged in a conversation or using the phone for some purpose. The room has a cozy atmosphere, with a couch located in the background and a chair placed nearby. A potted plant can be seen in the corner of the room, adding a touch of greenery to the space.'\n"
     ]
    }
   ],
   "source": [
    "# 手动构建一个简单的prompt\n",
    "simple_prompt = \"USER: What do you see in this image?\\nASSISTANT:\"\n",
    "simple_input_ids = tokenizer.encode(simple_prompt, return_tensors=\"pt\").to(triper_model.device)\n",
    "\n",
    "print(f\"简单prompt: '{simple_prompt}'\")\n",
    "print(f\"简单input_ids: {simple_input_ids.shape}\")\n",
    "\n",
    "# 测试极简生成\n",
    "simple_response = triper_model.llava_model.generate(\n",
    "    inputs=simple_input_ids,\n",
    "    images=single_device_batch['images'],\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,  # 贪心\n",
    "    temperature=None,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "print(f\"单独图文生成结果: {simple_response.shape}\")\n",
    "if simple_response.shape[1] > simple_input_ids.shape[1]:\n",
    "    simple_text = tokenizer.decode(simple_response[0, simple_input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    print(f\"生成文本: '{simple_text}'\")\n",
    "else:\n",
    "    print(\"❌ 生成也失败了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2aee3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 测试对话预测格式...\n",
      "原始对话: [{'speaker': 'Chandler Bing', 'text': 'Sounds like a date to me.', 'emotion': 'neutral'}]\n",
      "对话预测prompt: '<image>\n",
      "USER: In this scene, Chandler Bing says: 'Sounds like a date to me.'. What would be a natural response?\n",
      "ASSISTANT:'\n",
      "预测的对话: 'is ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais'\n"
     ]
    }
   ],
   "source": [
    "from triper.constants import DEFAULT_IMAGE_TOKEN\n",
    "# 测试新的对话预测格式\n",
    "print(\"🧪 测试对话预测格式...\")\n",
    "\n",
    "# 手动构建对话预测prompt\n",
    "sample_data = dataset[10]\n",
    "print(f\"原始对话: {sample_data.get('conversation', [])}\")\n",
    "\n",
    "# 模拟新格式\n",
    "if sample_data.get('conversation'):\n",
    "    first_turn = sample_data['conversation'][0]\n",
    "    speaker = first_turn.get('speaker', 'Person') \n",
    "    text = first_turn.get('text', '')\n",
    "    \n",
    "    prediction_prompt = f\"{DEFAULT_IMAGE_TOKEN}\\nUSER: In this scene, {speaker} says: '{text}'. What would be a natural response?\\nASSISTANT:\"\n",
    "else:\n",
    "    prediction_prompt = f\"{DEFAULT_IMAGE_TOKEN}\\nUSER: What conversation would happen in this scene?\\nASSISTANT:\"\n",
    "\n",
    "prediction_ids = tokenizer.encode(prediction_prompt, return_tensors=\"pt\").to(triper_model.device)\n",
    "\n",
    "print(f\"对话预测prompt: '{prediction_prompt}'\")\n",
    "\n",
    "# 测试生成\n",
    "prediction_response = triper_model.llava_model.generate(\n",
    "    inputs=prediction_ids,\n",
    "    images=single_device_batch['images'],\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "if prediction_response.shape[1] > prediction_ids.shape[1]:\n",
    "    pred_text = tokenizer.decode(prediction_response[0, prediction_ids.shape[1]:], skip_special_tokens=True)\n",
    "    print(f\"预测的对话: '{pred_text}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
