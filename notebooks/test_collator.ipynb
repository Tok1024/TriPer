{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80c9e66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wly/.conda/envs/triper/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Building Triper model from components...\n",
      "   LLaVA model: /sda1/llava-v1.5-13b\n",
      "   Audio encoder: /sda1/glm-4-voice-tokenizer\n",
      "   Audio projector: Built from config\n",
      "   Freeze LLaVA: True\n",
      "ğŸ”„ Loading LLaVA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.99s/it]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Configuring image tokens...\n",
      "æ·»åŠ å›¾åƒtoken: <image>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ›´æ–°IMAGE_TOKEN_INDEXä¸º: 32000\n",
      "ğŸ”„ Building audio encoder...\n",
      "âœ… WhisperVQEncoder loaded from /sda1/glm-4-voice-tokenizer\n",
      "ğŸ”„ Moving audio encoder to device: cuda:3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TriperModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”’ Audio encoder parameters frozen\n",
      "âœ… Audio encoder built and moved to cuda:3: WhisperVQEncoder\n",
      "ğŸ”„ Creating Triper model...\n",
      "ğŸ”„ Building audio projector...\n",
      "ğŸ”§ AudioProjector config:\n",
      "  audio_hidden_size: 1280\n",
      "  hidden_size: 5120\n",
      "  projector_type: mlp2x_gelu\n",
      "âœ… AudioProjector created successfully\n",
      "âœ… Audio projector built: AudioProjector\n",
      "âœ… TriperModel initialized with config: triper\n",
      "ğŸ”„ Moving Triper model to device: cuda:3\n",
      "âœ… LLaVA model attached: LlavaLlamaForCausalLM\n",
      "ğŸ”’ LLaVA model parameters frozen\n",
      "ğŸµ Audio encoder attached: WhisperVQEncoder\n",
      "ğŸ“¦ Components set: tokenizer(LlamaTokenizer), processor(CLIPImageProcessor), context_len(2048)\n",
      "âœ… Triper model created successfully!\n",
      "\n",
      "ğŸ—ï¸  Triper Model Summary\n",
      "============================================================\n",
      "ğŸ“¦ Components:\n",
      "  ğŸ¦™ LLaVA: âœ… (LlavaLlamaForCausalLM)\n",
      "  ğŸµ Audio Encoder: âœ… (WhisperVQEncoder) ğŸ”’ External (Frozen)\n",
      "  ğŸ”— Audio Projector: âœ… (AudioProjector) ğŸ”“ Trainable\n",
      "  ğŸ“ Tokenizer: âœ… (LlamaTokenizer) ğŸ”’ External\n",
      "  ğŸ–¼ï¸ Image Processor: âœ… (CLIPImageProcessor) ğŸ”’ External\n",
      "\n",
      "ğŸ“Š Trainable Parameters:\n",
      "  Total: 13,383,638,016\n",
      "  Trainable: 32,788,480 (0.2%)\n",
      "    llava: 13,350,849,536 (0.0% trainable) ğŸ”’\n",
      "    audio_projector: 32,788,480 (100.0% trainable) ğŸ”“\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path = [p for p in sys.path if 'triper-project' not in p]\n",
    "sys.path.append('/home/wly/szl_all_code/triper-project')\n",
    "\n",
    "# æ¸…ç†ç¼“å­˜\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "from triper.model import from_pretrained_components\n",
    "from triper.data import TriperDataset, TriperDataCollator\n",
    "\n",
    "audio_config = {\n",
    "    'mm_audio_encoder': 'whisper_vq',\n",
    "    'audio_hidden_size': 1280,\n",
    "    'audio_model_path': '/sda1/glm-4-voice-tokenizer',\n",
    "    'audio_projector_type': 'mlp2x_gelu',\n",
    "    'audio_projector_hidden_dim': 2048,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "tokenizer, triper_model, image_processor, context_len, audio_encoder = from_pretrained_components(\n",
    "    llava_model_path=\"/sda1/llava-v1.5-13b\",\n",
    "    audio_encoder_path=\"/sda1/glm-4-voice-tokenizer\",\n",
    "    audio_projector_path=None,\n",
    "    audio_config=audio_config,\n",
    "    freeze_llava=True,\n",
    "    device_map=\"cuda:3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4efbcfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨ä»ä»¥ä¸‹è·¯å¾„åŠ è½½æ•°æ®é›†æè¿°æ–‡ä»¶: /home/wly/szl_all_code/triper-project/data/simple_data_20_samples.json\n",
      "å‘ç° 20 ä¸ªæ•°æ®æ ·æœ¬ã€‚\n",
      "æ•°æ®é›†æ¨¡å¼: raw\n",
      "éŸ³é¢‘æ–‡ä»¶å¤¹: /home/wly/szl_all_code/triper-project/data/audio\n",
      "è§†é¢‘æ–‡ä»¶å¤¹: /home/wly/szl_all_code/triper-project/data/video\n",
      "å›¾åƒæ–‡ä»¶å¤¹: /home/wly/szl_all_code/triper-project/data/images\n"
     ]
    }
   ],
   "source": [
    "dataset = TriperDataset(\n",
    "    json_path='/home/wly/szl_all_code/triper-project/data/simple_data_20_samples.json',\n",
    "    media_root_path='/home/wly/szl_all_code/triper-project/data',\n",
    ")\n",
    "\n",
    "collator = TriperDataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    image_processor=image_processor,\n",
    "    audio_processor=audio_encoder,\n",
    "    model_cfg=triper_model.llava_model.config\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2480da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Monica Geller (anger): There's nothing to tell! He's just some guy I work with!...\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Joey Tribbiani (surprise): C'mon, you're going out with the guy! There's gotta be something ...\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "No conversation available....\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Chandler Bing (neutral): So does he have a hump? A hump and a hairpiece?...\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "No conversation available....\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Phoebe Buffay (neutral): Wait, does he eat chalk?...\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "No conversation available....\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Phoebe Buffay (neutral): Just, 'cause, I don't want her to go through what I went through wi...\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Monica Geller (neutral): Okay, everybody relax. ...\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Monica Geller (neutral): This is not even a date. It's just two people going out to dinner a...\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Chandler Bing (neutral): Sounds like a date to me....\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "No conversation available....\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Chandler Bing (neutral): Alright, so I'm back in high school, I'm standing in the middle of ...\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Monica Geller (neutral): Oh, yeah. Had that dream....\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Chandler Bing (neutral): Then I look down, and I realize there's a phone... there....\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Joey Tribbiani (neutral): Instead of...?...\n",
      "ğŸ“ åŸå§‹æ–‡æœ¬é•¿åº¦èŒƒå›´: 8 - 48\n",
      "âš ï¸ æ–‡æœ¬é•¿åº¦ä¸ä¸€è‡´ï¼Œå°†paddingåˆ°: 48\n",
      "âœ… æ‰¹é‡tokenizationå®Œæˆ: input_ids shape: torch.Size([16, 48])\n",
      "âœ… æ‰€æœ‰æ ·æœ¬æ–‡æœ¬é•¿åº¦ç»Ÿä¸€ä¸º: 48\n",
      "âœ… å›¾åƒå¤„ç†æˆåŠŸ: torch.Size([16, 3, 336, 336])\n",
      "âœ… éŸ³é¢‘æ‰¹é‡å¤„ç†å®Œæˆ: torch.Size([16, 64, 1280])\n",
      "ğŸ”¥ TriperModel.forward called:\n",
      "  input_ids: torch.Size([16, 48])\n",
      "  images: torch.Size([16, 3, 336, 336])\n",
      "  audio_features: torch.Size([16, 64, 1280])\n",
      "  past_key_values: 0 layers\n",
      "  ğŸ“¸ LLaVAå¤„ç†å›¾åƒ...\n",
      "  LLaVAå¤„ç†åembeds: torch.Size([16, 623, 5120])\n",
      "  ğŸ” æœ€ç»ˆéªŒè¯:\n",
      "    inputs_embeds: torch.Size([16, 623, 5120])\n",
      "    attention_mask: None\n",
      "âœ… æ‰¹é‡æ¨ç†æˆåŠŸï¼\n",
      "è¾“å‡ºlogitså½¢çŠ¶: torch.Size([16, 623, 32001])\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•æ‰¹é‡æ¨ç†\n",
    "batch_size = 16\n",
    "batch_samples = [dataset[i] for i in range(batch_size)]\n",
    "batch_result = collator(batch_samples)\n",
    "\n",
    "# ğŸ”§ å®‰å…¨ç§»åŠ¨åˆ°è®¾å¤‡\n",
    "device_batch = {}\n",
    "for k, v in batch_result.items():\n",
    "    if hasattr(v, 'to'):\n",
    "        device_batch[k] = v.to(triper_model.device)\n",
    "    else:\n",
    "        print(f\"ä¿æŒåŸå€¼: {k} ç±»å‹={type(v)}\")\n",
    "        device_batch[k] = v\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = triper_model(\n",
    "        input_ids=device_batch['input_ids'],\n",
    "        images=device_batch['images'],\n",
    "        audio_features=device_batch['audio_features']\n",
    "        # ğŸ”§ ä¸ä¼ é€’attention_maskå’Œlabelsï¼Œè®©æ¨¡å‹è‡ªå·±å¤„ç†\n",
    "    )\n",
    "    print(\"âœ… æ‰¹é‡æ¨ç†æˆåŠŸï¼\")\n",
    "    print(f\"è¾“å‡ºlogitså½¢çŠ¶: {output['logits'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6203617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Monica Geller (anger): There's nothing to tell! He's just some guy I work with!...\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Joey Tribbiani (surprise): C'mon, you're going out with the guy! There's gotta be something ...\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "No conversation available....\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Chandler Bing (neutral): So does he have a hump? A hump and a hairpiece?...\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "No conversation available....\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Phoebe Buffay (neutral): Wait, does he eat chalk?...\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "No conversation available....\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Phoebe Buffay (neutral): Just, 'cause, I don't want her to go through what I went through wi...\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Monica Geller (neutral): Okay, everybody relax. ...\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Monica Geller (neutral): This is not even a date. It's just two people going out to dinner a...\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Chandler Bing (neutral): Sounds like a date to me....\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "No conversation available....\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Chandler Bing (neutral): Alright, so I'm back in high school, I'm standing in the middle of ...\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Monica Geller (neutral): Oh, yeah. Had that dream....\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Chandler Bing (neutral): Then I look down, and I realize there's a phone... there....\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Joey Tribbiani (neutral): Instead of...?...\n",
      "ğŸ“ åŸå§‹æ–‡æœ¬é•¿åº¦èŒƒå›´: 8 - 48\n",
      "âš ï¸ æ–‡æœ¬é•¿åº¦ä¸ä¸€è‡´ï¼Œå°†paddingåˆ°: 48\n",
      "âœ… æ‰¹é‡tokenizationå®Œæˆ: input_ids shape: torch.Size([16, 48])\n",
      "âœ… æ‰€æœ‰æ ·æœ¬æ–‡æœ¬é•¿åº¦ç»Ÿä¸€ä¸º: 48\n",
      "âœ… å›¾åƒå¤„ç†æˆåŠŸ: torch.Size([16, 3, 336, 336])\n",
      "âœ… éŸ³é¢‘æ‰¹é‡å¤„ç†å®Œæˆ: torch.Size([16, 64, 1280])\n",
      "ğŸš€ TriperModel.generate called:\n",
      "  input_ids: torch.Size([16, 48])\n",
      "  images: torch.Size([16, 3, 336, 336])\n",
      "  audio_features: torch.Size([16, 64, 1280])\n",
      "ğŸµ æ£€æµ‹åˆ°éŸ³é¢‘è¾“å…¥ï¼Œå‡†å¤‡å¤šæ¨¡æ€embeddings...\n",
      "ğŸ“¸ LLaVAå¤„ç†å›¾åƒ...\n",
      "LLaVAå¤„ç†åembeds: torch.Size([16, 623, 5120])\n",
      "ğŸµ é›†æˆéŸ³é¢‘ç‰¹å¾...\n",
      "ğŸ”„ AudioProjector forward called with input shape: torch.Size([16, 64, 1280])\n",
      "ğŸµ éŸ³é¢‘ç‰¹å¾æ’å…¥å®Œæˆ:\n",
      "  åŸå§‹embeds: torch.Size([16, 623, 5120])\n",
      "  éŸ³é¢‘embeds: torch.Size([16, 64, 5120])\n",
      "  åˆå¹¶åembeds: torch.Size([16, 687, 5120])\n",
      "  åˆå¹¶åattention_mask: torch.Size([16, 687])\n",
      "æœ€ç»ˆembeds: torch.Size([16, 687, 5120])\n",
      "æœ€ç»ˆattention_mask: torch.Size([16, 687])\n",
      "ğŸš€ è°ƒç”¨LLaVA.generate with inputs_embeds...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 290.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 225.88 MiB is free. Including non-PyTorch memory, this process has 47.29 GiB memory in use. Of the allocated memory 41.63 GiB is allocated by PyTorch, and 5.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m         device_batch[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# åœ¨æ‚¨çš„notebookä¸­ä¿®æ”¹è¿™éƒ¨åˆ†\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mtriper_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ç°åœ¨å¯ä»¥ç›´æ¥ä¼ å…¥\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maudio_features\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mç”Ÿæˆç»“æœå½¢çŠ¶: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# ğŸ”§ ä¿®å¤ï¼šç”±äºgenerate_with_embedsåªè¿”å›æ–°ç”Ÿæˆçš„tokens\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# æˆ‘ä»¬éœ€è¦æ ¹æ®å®é™…æƒ…å†µæ¥è§£ç \u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/szl_all_code/triper-project/triper/model/triper_arch.py:465\u001b[0m, in \u001b[0;36mTriperModel.generate\u001b[0;34m(self, input_ids, images, image_sizes, audio_features, attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸš€ è°ƒç”¨LLaVA.generate with inputs_embeds...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# ğŸ”§ æ³¨æ„ï¼šLLaVAçš„generateæ–¹æ³•å†…éƒ¨ä¼šè°ƒç”¨super().generate()\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# è€Œsuper().generate()æ˜¯transformersçš„æ ‡å‡†generateï¼Œæ”¯æŒinputs_embeds\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllava_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__bases__\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllava_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/transformers/generation/utils.py:2597\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2589\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2590\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2591\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2592\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2593\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2594\u001b[0m     )\n\u001b[1;32m   2596\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2597\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2598\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2608\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2609\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2610\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2611\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2612\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2613\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2614\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/transformers/generation/utils.py:3557\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3554\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3557\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3558\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/szl_all_code/triper-project/llava/model/language_model/llava_llama.py:90\u001b[0m, in \u001b[0;36mLlavaLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, image_sizes, return_dict, cache_position)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     (\n\u001b[1;32m     74\u001b[0m         input_ids,\n\u001b[1;32m     75\u001b[0m         position_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m         image_sizes\n\u001b[1;32m     88\u001b[0m     )\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:688\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    684\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    685\u001b[0m )\n\u001b[1;32m    687\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:453\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    451\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 453\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/transformers/modeling_layers.py:48\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:324\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    323\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 324\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    327\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:162\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 162\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 290.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 225.88 MiB is free. Including non-PyTorch memory, this process has 47.29 GiB memory in use. Of the allocated memory 41.63 GiB is allocated by PyTorch, and 5.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•æ‰¹é‡æ¨ç†\n",
    "batch_size = 16\n",
    "batch_samples = [dataset[i] for i in range(batch_size)]\n",
    "batch_result = collator(batch_samples)\n",
    "\n",
    "# ğŸ”§ å®‰å…¨ç§»åŠ¨åˆ°è®¾å¤‡\n",
    "device_batch = {}\n",
    "for k, v in batch_result.items():\n",
    "    if hasattr(v, 'to'):\n",
    "        device_batch[k] = v.to(triper_model.device)\n",
    "    else:\n",
    "        print(f\"ä¿æŒåŸå€¼: {k} ç±»å‹={type(v)}\")\n",
    "        device_batch[k] = v\n",
    "        \n",
    "# åœ¨æ‚¨çš„notebookä¸­ä¿®æ”¹è¿™éƒ¨åˆ†\n",
    "response = triper_model.generate(\n",
    "    input_ids=device_batch['input_ids'],\n",
    "    attention_mask=device_batch['attention_mask'],  # ç°åœ¨å¯ä»¥ç›´æ¥ä¼ å…¥\n",
    "    images=device_batch['images'],\n",
    "    audio_features=device_batch['audio_features'],\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.1,\n",
    "    do_sample=True,\n",
    "    top_p=0.8,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(f\"ç”Ÿæˆç»“æœå½¢çŠ¶: {response.shape}\")\n",
    "\n",
    "# ğŸ”§ ä¿®å¤ï¼šç”±äºgenerate_with_embedsåªè¿”å›æ–°ç”Ÿæˆçš„tokens\n",
    "# æˆ‘ä»¬éœ€è¦æ ¹æ®å®é™…æƒ…å†µæ¥è§£ç \n",
    "if response.shape[1] == 50:  # åªæ˜¯æ–°ç”Ÿæˆçš„tokens\n",
    "    generated_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "    print(f\"ç”Ÿæˆçš„æ–‡æœ¬: '{generated_text}'\")\n",
    "else:  # å®Œæ•´åºåˆ—\n",
    "    original_len = device_batch['input_ids'].shape[1]\n",
    "    generated_part = response[0, original_len:]\n",
    "    generated_text = tokenizer.decode(generated_part, skip_special_tokens=True)\n",
    "    print(f\"ç”Ÿæˆçš„æ–‡æœ¬: '{generated_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8033320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª æµ‹è¯•å•ä¸ªæ ·æœ¬ç”Ÿæˆ...\n",
      "ğŸ“ å¯¹è¯é¢„æµ‹æ ¼å¼:\n",
      "<image>\n",
      "USER: Based on what you see and hear in this scene, what would Chandler Bing say?\n",
      "ASSISTANT:\n",
      "ğŸ“ åŸå§‹æ–‡æœ¬é•¿åº¦èŒƒå›´: 31 - 31\n",
      "âœ… æ‰¹é‡tokenizationå®Œæˆ: input_ids shape: torch.Size([1, 31])\n",
      "âœ… æ‰€æœ‰æ ·æœ¬æ–‡æœ¬é•¿åº¦ç»Ÿä¸€ä¸º: 31\n",
      "âœ… å›¾åƒå¤„ç†æˆåŠŸ: torch.Size([1, 3, 336, 336])\n",
      "âœ… éŸ³é¢‘æ‰¹é‡å¤„ç†å®Œæˆ: torch.Size([1, 64, 1280])\n",
      "å•æ ·æœ¬æ•°æ®å½¢çŠ¶:\n",
      "  input_ids: torch.Size([1, 31])\n",
      "  attention_mask: torch.Size([1, 31])\n",
      "  images: torch.Size([1, 3, 336, 336])\n",
      "  audio_features: torch.Size([1, 64, 1280])\n",
      "ä¿®å¤åattention_mask: torch.Size([1, 31])\n",
      "ğŸš€ TriperModel.generate called:\n",
      "  input_ids: torch.Size([1, 31])\n",
      "  images: torch.Size([1, 3, 336, 336])\n",
      "  audio_features: torch.Size([1, 64, 1280])\n",
      "ğŸµ æ£€æµ‹åˆ°éŸ³é¢‘è¾“å…¥ï¼Œå‡†å¤‡å¤šæ¨¡æ€embeddings...\n",
      "ğŸ“¸ LLaVAå¤„ç†å›¾åƒ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaVAå¤„ç†åembeds: torch.Size([1, 606, 5120])\n",
      "ğŸµ é›†æˆéŸ³é¢‘ç‰¹å¾...\n",
      "ğŸ”„ AudioProjector forward called with input shape: torch.Size([1, 64, 1280])\n",
      "ğŸµ éŸ³é¢‘ç‰¹å¾æ’å…¥å®Œæˆ:\n",
      "  åŸå§‹embeds: torch.Size([1, 606, 5120])\n",
      "  éŸ³é¢‘embeds: torch.Size([1, 64, 5120])\n",
      "  åˆå¹¶åembeds: torch.Size([1, 670, 5120])\n",
      "  åˆå¹¶åattention_mask: torch.Size([1, 670])\n",
      "æœ€ç»ˆembeds: torch.Size([1, 670, 5120])\n",
      "æœ€ç»ˆattention_mask: torch.Size([1, 670])\n",
      "ğŸš€ è°ƒç”¨LLaVA.generate with inputs_embeds...\n",
      "ç”Ÿæˆç»“æœå½¢çŠ¶: torch.Size([1, 1])\n",
      "ç”Ÿæˆçš„æ–‡æœ¬: ''\n",
      "å®Œæ•´å¯¹è¯: ''\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ å…ˆæµ‹è¯•å•ä¸ªæ ·æœ¬ï¼Œé¿å…æ‰¹é‡é—®é¢˜\n",
    "print(\"ğŸ§ª æµ‹è¯•å•ä¸ªæ ·æœ¬ç”Ÿæˆ...\")\n",
    "\n",
    "# å–ç¬¬ä¸€ä¸ªæ ·æœ¬\n",
    "single_sample = [dataset[10]]\n",
    "single_batch = collator(single_sample)\n",
    "\n",
    "# ç§»åŠ¨åˆ°è®¾å¤‡\n",
    "single_device_batch = {}\n",
    "for k, v in single_batch.items():\n",
    "    if hasattr(v, 'to'):\n",
    "        single_device_batch[k] = v.to(triper_model.device)\n",
    "    else:\n",
    "        single_device_batch[k] = v\n",
    "\n",
    "print(f\"å•æ ·æœ¬æ•°æ®å½¢çŠ¶:\")\n",
    "print(f\"  input_ids: {single_device_batch['input_ids'].shape}\")\n",
    "print(f\"  attention_mask: {single_device_batch['attention_mask'].shape}\")\n",
    "print(f\"  images: {single_device_batch['images'].shape}\")\n",
    "print(f\"  audio_features: {single_device_batch['audio_features'].shape}\")\n",
    "\n",
    "# ğŸ”§ ä¿®å¤attention_maské•¿åº¦é—®é¢˜\n",
    "text_len = single_device_batch['input_ids'].shape[1]\n",
    "text_attention_mask = single_device_batch['attention_mask'][:, :text_len]\n",
    "\n",
    "print(f\"ä¿®å¤åattention_mask: {text_attention_mask.shape}\")\n",
    "\n",
    "# æµ‹è¯•ç”Ÿæˆ\n",
    "response = triper_model.generate(\n",
    "    input_ids=single_device_batch['input_ids'],\n",
    "    attention_mask=text_attention_mask,  # ä½¿ç”¨ä¿®å¤åçš„attention_mask\n",
    "    images=single_device_batch['images'],\n",
    "    audio_features=single_device_batch['audio_features'],\n",
    "    max_new_tokens=512,  # å…ˆç”¨å°‘é‡tokenæµ‹è¯•\n",
    "    temperature=0.1,\n",
    "    do_sample=False,  # å…ˆç”¨è´ªå¿ƒæœç´¢\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(f\"ç”Ÿæˆç»“æœå½¢çŠ¶: {response.shape}\")\n",
    "\n",
    "# è§£ç ç»“æœ\n",
    "original_len = single_device_batch['input_ids'].shape[1]\n",
    "if response.shape[1] > original_len:\n",
    "    # è¿”å›å®Œæ•´åºåˆ—ï¼Œæˆªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†\n",
    "    generated_part = response[0, original_len:]\n",
    "    generated_text = tokenizer.decode(generated_part, skip_special_tokens=True)\n",
    "    print(f\"æ–°ç”Ÿæˆçš„æ–‡æœ¬: '{generated_text}'\")\n",
    "else:\n",
    "    # åªè¿”å›æ–°ç”Ÿæˆçš„tokens\n",
    "    generated_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "    print(f\"ç”Ÿæˆçš„æ–‡æœ¬: '{generated_text}'\")\n",
    "\n",
    "# æ˜¾ç¤ºå®Œæ•´å¯¹è¯\n",
    "full_text = tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "print(f\"å®Œæ•´å¯¹è¯: '{full_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "641dff13",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'single_device_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pure_llava_response \u001b[38;5;241m=\u001b[39m triper_model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m----> 2\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39m\u001b[43msingle_device_batch\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      3\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mtext_attention_mask,\n\u001b[1;32m      4\u001b[0m     images\u001b[38;5;241m=\u001b[39msingle_device_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      5\u001b[0m     audio_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# ä¸ä¼ éŸ³é¢‘\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m      7\u001b[0m     min_length\u001b[38;5;241m=\u001b[39msingle_device_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      8\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m,\n\u001b[1;32m     10\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m     11\u001b[0m     pad_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[1;32m     12\u001b[0m     eos_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m     13\u001b[0m     early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mçº¯LLaVAç”Ÿæˆç»“æœ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpure_llava_response\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pure_llava_response\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m original_len:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'single_device_batch' is not defined"
     ]
    }
   ],
   "source": [
    "pure_llava_response = triper_model.generate(\n",
    "    input_ids=single_device_batch['input_ids'],\n",
    "    attention_mask=text_attention_mask,\n",
    "    images=single_device_batch['images'],\n",
    "    audio_features=None,  # ä¸ä¼ éŸ³é¢‘\n",
    "    max_new_tokens=50,\n",
    "    min_length=single_device_batch['input_ids'].shape[1] + 10,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    early_stopping=False,\n",
    ")\n",
    "\n",
    "print(f\"çº¯LLaVAç”Ÿæˆç»“æœ: {pure_llava_response.shape}\")\n",
    "\n",
    "if pure_llava_response.shape[1] > original_len:\n",
    "    pure_text = tokenizer.decode(pure_llava_response[0, original_len:], skip_special_tokens=True)\n",
    "    print(f\"çº¯LLaVAç”Ÿæˆæ–‡æœ¬: '{pure_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "025823bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç®€å•prompt: 'USER: What do you see in this image?\n",
      "ASSISTANT:'\n",
      "ç®€å•input_ids: torch.Size([1, 17])\n",
      "å•ç‹¬å›¾æ–‡ç”Ÿæˆç»“æœ: torch.Size([1, 82])\n",
      "ç”Ÿæˆæ–‡æœ¬: '. The person appears to be engaged in a conversation or using the phone for some purpose. The room has a cozy atmosphere, with a couch located in the background and a chair placed nearby. A potted plant can be seen in the corner of the room, adding a touch of greenery to the space.'\n"
     ]
    }
   ],
   "source": [
    "# æ‰‹åŠ¨æ„å»ºä¸€ä¸ªç®€å•çš„prompt\n",
    "simple_prompt = \"USER: What do you see in this image?\\nASSISTANT:\"\n",
    "simple_input_ids = tokenizer.encode(simple_prompt, return_tensors=\"pt\").to(triper_model.device)\n",
    "\n",
    "print(f\"ç®€å•prompt: '{simple_prompt}'\")\n",
    "print(f\"ç®€å•input_ids: {simple_input_ids.shape}\")\n",
    "\n",
    "# æµ‹è¯•æç®€ç”Ÿæˆ\n",
    "simple_response = triper_model.llava_model.generate(\n",
    "    inputs=simple_input_ids,\n",
    "    images=single_device_batch['images'],\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,  # è´ªå¿ƒ\n",
    "    temperature=None,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "print(f\"å•ç‹¬å›¾æ–‡ç”Ÿæˆç»“æœ: {simple_response.shape}\")\n",
    "if simple_response.shape[1] > simple_input_ids.shape[1]:\n",
    "    simple_text = tokenizer.decode(simple_response[0, simple_input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    print(f\"ç”Ÿæˆæ–‡æœ¬: '{simple_text}'\")\n",
    "else:\n",
    "    print(\"âŒ ç”Ÿæˆä¹Ÿå¤±è´¥äº†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2aee3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª æµ‹è¯•å¯¹è¯é¢„æµ‹æ ¼å¼...\n",
      "åŸå§‹å¯¹è¯: [{'speaker': 'Chandler Bing', 'text': 'Sounds like a date to me.', 'emotion': 'neutral'}]\n",
      "å¯¹è¯é¢„æµ‹prompt: '<image>\n",
      "USER: In this scene, Chandler Bing says: 'Sounds like a date to me.'. What would be a natural response?\n",
      "ASSISTANT:'\n",
      "é¢„æµ‹çš„å¯¹è¯: 'is ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais ais'\n"
     ]
    }
   ],
   "source": [
    "from triper.constants import DEFAULT_IMAGE_TOKEN\n",
    "# æµ‹è¯•æ–°çš„å¯¹è¯é¢„æµ‹æ ¼å¼\n",
    "print(\"ğŸ§ª æµ‹è¯•å¯¹è¯é¢„æµ‹æ ¼å¼...\")\n",
    "\n",
    "# æ‰‹åŠ¨æ„å»ºå¯¹è¯é¢„æµ‹prompt\n",
    "sample_data = dataset[10]\n",
    "print(f\"åŸå§‹å¯¹è¯: {sample_data.get('conversation', [])}\")\n",
    "\n",
    "# æ¨¡æ‹Ÿæ–°æ ¼å¼\n",
    "if sample_data.get('conversation'):\n",
    "    first_turn = sample_data['conversation'][0]\n",
    "    speaker = first_turn.get('speaker', 'Person') \n",
    "    text = first_turn.get('text', '')\n",
    "    \n",
    "    prediction_prompt = f\"{DEFAULT_IMAGE_TOKEN}\\nUSER: In this scene, {speaker} says: '{text}'. What would be a natural response?\\nASSISTANT:\"\n",
    "else:\n",
    "    prediction_prompt = f\"{DEFAULT_IMAGE_TOKEN}\\nUSER: What conversation would happen in this scene?\\nASSISTANT:\"\n",
    "\n",
    "prediction_ids = tokenizer.encode(prediction_prompt, return_tensors=\"pt\").to(triper_model.device)\n",
    "\n",
    "print(f\"å¯¹è¯é¢„æµ‹prompt: '{prediction_prompt}'\")\n",
    "\n",
    "# æµ‹è¯•ç”Ÿæˆ\n",
    "prediction_response = triper_model.llava_model.generate(\n",
    "    inputs=prediction_ids,\n",
    "    images=single_device_batch['images'],\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "if prediction_response.shape[1] > prediction_ids.shape[1]:\n",
    "    pred_text = tokenizer.decode(prediction_response[0, prediction_ids.shape[1]:], skip_special_tokens=True)\n",
    "    print(f\"é¢„æµ‹çš„å¯¹è¯: '{pred_text}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
