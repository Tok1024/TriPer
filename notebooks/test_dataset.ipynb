{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a99339ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/wly/szl_all_code/triper-project')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3903bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wly/.conda/envs/triper/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Building Triper model from components...\n",
      "   LLaVA model: /sda1/llava-v1.5-13b\n",
      "   Audio encoder: /sda1/glm-4-voice-tokenizer\n",
      "   Audio projector: Built from config\n",
      "   Freeze LLaVA: True\n",
      "🔄 Loading LLaVA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.11s/it]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Configuring image tokens...\n",
      "添加图像token: <image>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "更新IMAGE_TOKEN_INDEX为: 32000\n",
      "🔄 Building audio encoder...\n",
      "✅ WhisperVQEncoder loaded from /sda1/glm-4-voice-tokenizer\n",
      "🔄 Moving audio encoder to device: cuda:2\n",
      "🔒 Audio encoder parameters frozen\n",
      "✅ Audio encoder built and moved to cuda:2: WhisperVQEncoder\n",
      "🔄 Creating Triper model...\n",
      "🔄 Building audio projector...\n",
      "🔧 AudioProjector config:\n",
      "  audio_hidden_size: 1280\n",
      "  hidden_size: 5120\n",
      "  projector_type: mlp2x_gelu\n",
      "✅ AudioProjector created successfully\n",
      "✅ Audio projector built: AudioProjector\n",
      "✅ TriperModel initialized with config: triper\n",
      "🔄 Moving Triper model to device: cuda:2\n",
      "✅ LLaVA model attached: LlavaLlamaForCausalLM\n",
      "🔒 LLaVA model parameters frozen\n",
      "🎵 Audio encoder attached: WhisperVQEncoder\n",
      "📦 Components set: tokenizer(LlamaTokenizer), processor(CLIPImageProcessor), context_len(2048)\n",
      "✅ Triper model created successfully!\n",
      "\n",
      "🏗️  Triper Model Summary\n",
      "============================================================\n",
      "📦 Components:\n",
      "  🦙 LLaVA: ✅ (LlavaLlamaForCausalLM)\n",
      "  🎵 Audio Encoder: ✅ (WhisperVQEncoder) 🔒 External (Frozen)\n",
      "  🔗 Audio Projector: ✅ (AudioProjector) 🔓 Trainable\n",
      "  📝 Tokenizer: ✅ (LlamaTokenizer) 🔒 External\n",
      "  🖼️ Image Processor: ✅ (CLIPImageProcessor) 🔒 External\n",
      "\n",
      "📊 Trainable Parameters:\n",
      "  Total: 13,383,638,016\n",
      "  Trainable: 32,788,480 (0.2%)\n",
      "    llava: 13,350,849,536 (0.0% trainable) 🔒\n",
      "    audio_projector: 32,788,480 (100.0% trainable) 🔓\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 13383638016,\n",
       " 'trainable_params': 32788480,\n",
       " 'components': {'llava': {'total': 13350849536, 'trainable': 0},\n",
       "  'audio_projector': {'total': 32788480, 'trainable': 32788480}}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from triper.model import from_pretrained_components\n",
    "audio_config = {\n",
    "    'mm_audio_encoder': 'whisper_vq',\n",
    "    'audio_hidden_size': 1280,  # Whisper输出维度\n",
    "    'audio_model_path': '/sda1/glm-4-voice-tokenizer',\n",
    "    'audio_projector_type': 'mlp2x_gelu',\n",
    "    'audio_projector_hidden_dim': 2048,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "tokenizer, triper_model, image_processor, context_len, audio_encoder = from_pretrained_components(\n",
    "    llava_model_path=\"/sda1/llava-v1.5-13b\",\n",
    "    audio_encoder_path=\"/sda1/glm-4-voice-tokenizer\",\n",
    "    audio_projector_path=None,\n",
    "    audio_config=audio_config,\n",
    "    freeze_llava=True,\n",
    "    device_map=\"cuda:2\"\n",
    ")\n",
    "triper_model.get_parameter_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58434ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从以下路径加载数据集描述文件: /home/wly/szl_all_code/triper-project/data/simple_data_20_samples.json\n",
      "发现 20 个数据样本。\n",
      "数据集模式: raw\n",
      "音频文件夹: /home/wly/szl_all_code/triper-project/data/audio\n",
      "视频文件夹: /home/wly/szl_all_code/triper-project/data/video\n",
      "图像文件夹: /home/wly/szl_all_code/triper-project/data/images\n"
     ]
    }
   ],
   "source": [
    "from triper.data import TriperDataset, TriperDataCollator\n",
    "from torch.utils.data import DataLoader\n",
    "# 创建数据集\n",
    "dataset = TriperDataset(\n",
    "    json_path='/home/wly/szl_all_code/triper-project/data/simple_data_20_samples.json',\n",
    "    media_root_path='/home/wly/szl_all_code/triper-project/data',\n",
    "    mode=\"raw\"\n",
    ")\n",
    "\n",
    "# 创建collator（在这里传入正确的model_cfg）\n",
    "collator = TriperDataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    image_processor=image_processor,\n",
    "    audio_processor=audio_encoder,\n",
    "    model_cfg=triper_model.llava_model.config\n",
    ")\n",
    "\n",
    "# 创建DataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=4, \n",
    "    collate_fn=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c3063c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 测试数据流程 ===\n",
      "推理前显存使用:\n",
      "GPU 0: 0.00GB / 0.00GB\n",
      "GPU 1: 0.00GB / 0.00GB\n",
      "GPU 2: 0.00GB / 0.00GB\n",
      "GPU 3: 26.44GB / 27.84GB\n",
      "GPU 4: 0.00GB / 0.00GB\n",
      "样本结构: dict_keys(['id', 'audio_path', 'image_path', 'conversation', 'scene_description', 'metadata', 'has_audio', 'has_image'])\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Monica Geller (anger): There's nothing to tell! He's just some guy I work with!...\n",
      "📝 批量tokenization完成: input_ids shape: torch.Size([1, 29])\n",
      "🖼️ LLaVA process_images 处理成功，shape: torch.Size([1, 3, 336, 336])\n",
      "🎵 Final audio batch shape: torch.Size([1, 64, 1280])\n",
      "📏 序列长度: 文本=29, 图像=576, 音频=64, 总计=669\n",
      "📏 Labels shape: torch.Size([1, 29])\n",
      "\n",
      "✅ Collator处理成功！\n",
      "Batch结果:\n",
      "  input_ids: torch.Size([1, 29]) (dtype: torch.int64)\n",
      "  attention_mask: torch.Size([1, 29]) (dtype: torch.int64)\n",
      "  labels: torch.Size([1, 29]) (dtype: torch.int64)\n",
      "  images: torch.Size([1, 3, 336, 336]) (dtype: torch.float32)\n",
      "  audio_features: torch.Size([1, 64, 1280]) (dtype: torch.float32)\n",
      "\n",
      "=== 验证张量 ===\n",
      "✅ input_ids: 是有效的torch.Tensor\n",
      "✅ attention_mask: 是有效的torch.Tensor\n",
      "✅ labels: 是有效的torch.Tensor\n",
      "✅ images: 是有效的torch.Tensor\n",
      "✅ audio_features: 是有效的torch.Tensor\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 更安全的测试代码\n",
    "print(\"=== 测试数据流程 ===\")\n",
    "\n",
    "# 在推理前监控显存\n",
    "import torch\n",
    "print(\"推理前显存使用:\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.memory_allocated(i)/1024**3:.2f}GB / {torch.cuda.memory_reserved(i)/1024**3:.2f}GB\")\n",
    "\n",
    "# 清理缓存\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 测试单个样本\n",
    "single_sample = dataset[0]\n",
    "print(\"样本结构:\", single_sample.keys())\n",
    "\n",
    "# 测试collator\n",
    "try:\n",
    "    batch_result = collator([single_sample])\n",
    "    \n",
    "    print(\"\\n✅ Collator处理成功！\")\n",
    "    print(\"Batch结果:\")\n",
    "    for key, value in batch_result.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"  {key}: {value.shape} (dtype: {value.dtype})\")\n",
    "        else:\n",
    "            print(f\"  {key}: {type(value)}\")\n",
    "    \n",
    "    # 验证张量格式\n",
    "    print(\"\\n=== 验证张量 ===\")\n",
    "    for key, value in batch_result.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"✅ {key}: 是有效的torch.Tensor\")\n",
    "        else:\n",
    "            print(f\"❌ {key}: 不是torch.Tensor，类型为 {type(value)}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"❌ Collator失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c7bc993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 测试单个样本推理 ===\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Monica Geller (anger): There's nothing to tell! He's just some guy I work with!...\n",
      "📝 批量tokenization完成: input_ids shape: torch.Size([1, 29])\n",
      "🖼️ LLaVA process_images 处理成功，shape: torch.Size([1, 3, 336, 336])\n",
      "🎵 Final audio batch shape: torch.Size([1, 64, 1280])\n",
      "📏 序列长度: 文本=29, 图像=576, 音频=64, 总计=669\n",
      "📏 Labels shape: torch.Size([1, 29])\n",
      "单个样本结构: dict_keys(['id', 'audio_path', 'image_path', 'conversation', 'scene_description', 'metadata', 'has_audio', 'has_image'])\n",
      "初始input ids形状: torch.Size([1, 29])\n",
      "images shape: torch.Size([1, 3, 336, 336])\n",
      "处理图像后, embedding形状: torch.Size([1, 29, 5120])\n",
      "  🔄 Converting encoded audio to torch.bfloat16\n",
      "🔄 AudioProjector forward called with input shape: torch.Size([1, 64, 1280])\n",
      "  Inputs embeds shape: torch.Size([1, 29, 5120])\n",
      "  Inputs embeds dtype: torch.bfloat16\n",
      "  🔄 Converting audio embeds to match inputs_embeds dtype\n",
      "  Final result shape: torch.Size([1, 93, 5120])\n",
      "  Final result dtype: torch.bfloat16\n",
      "插入音频后, 嵌入形状: torch.Size([1, 93, 5120])\n",
      "单个样本推理成功！输出结构: odict_keys(['logits', 'past_key_values'])\n",
      "输出logits形状: torch.Size([1, 93, 32001])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 在推理代码中使用\n",
    "print(\"\\n=== 测试单个样本推理 ===\")\n",
    "single_sample = dataset[0]\n",
    "batch_result = collator([single_sample])\n",
    "print(\"单个样本结构:\", single_sample.keys())\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_result = {k: v.to(triper_model.device) for k, v in batch_result.items()}\n",
    "    \n",
    "    try:\n",
    "        output = triper_model(\n",
    "            input_ids=batch_result['input_ids'],\n",
    "            attention_mask=batch_result['attention_mask'],\n",
    "            images=batch_result['images'],\n",
    "            audio_features=batch_result['audio_features']\n",
    "        )\n",
    "        print(\"单个样本推理成功！输出结构:\", output.keys())\n",
    "        print(\"输出logits形状:\", output['logits'].shape)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"推理失败: {e}\")\n",
    "        if \"device\" in str(e).lower():\n",
    "            print(\"这可能是多GPU模型的设备分布问题\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abfbbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 测试批量数据推理 ===\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Monica Geller (anger): There's nothing to tell! He's just some guy I work with!...\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Joey Tribbiani (surprise): C'mon, you're going out with the guy! There's gotta be something ...\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "No conversation available....\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Chandler Bing (neutral): So does he have a hump? A hump and a hairpiece?...\n",
      "📝 批量tokenization完成: input_ids shape: torch.Size([4, 38])\n",
      "🖼️ LLaVA process_images 处理成功，shape: torch.Size([4, 3, 336, 336])\n",
      "🎵 Final audio batch shape: torch.Size([4, 64, 1280])\n",
      "📏 序列长度: 文本=38, 图像=576, 音频=64, 总计=678\n",
      "📏 Labels shape: torch.Size([4, 38])\n",
      "初始input ids形状: torch.Size([4, 38])\n",
      "images shape: torch.Size([4, 3, 336, 336])\n",
      "处理图像后, embedding形状: torch.Size([4, 38, 5120])\n",
      "  🔄 Converting encoded audio to torch.bfloat16\n",
      "🔄 AudioProjector forward called with input shape: torch.Size([4, 64, 1280])\n",
      "  Inputs embeds shape: torch.Size([4, 38, 5120])\n",
      "  Inputs embeds dtype: torch.bfloat16\n",
      "  🔄 Converting audio embeds to match inputs_embeds dtype\n",
      "  Final result shape: torch.Size([4, 102, 5120])\n",
      "  Final result dtype: torch.bfloat16\n",
      "插入音频后, 嵌入形状: torch.Size([4, 102, 5120])\n",
      "批量推理失败: The expanded size of the tensor (102) must match the existing size (38) at non-singleton dimension 3.  Target sizes: [4, 40, 102, 102].  Tensor sizes: [4, 1, 102, 38]\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "No conversation available....\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Phoebe Buffay (neutral): Wait, does he eat chalk?...\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "No conversation available....\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Phoebe Buffay (neutral): Just, 'cause, I don't want her to go through what I went through wi...\n",
      "📝 批量tokenization完成: input_ids shape: torch.Size([4, 38])\n",
      "🖼️ LLaVA process_images 处理成功，shape: torch.Size([4, 3, 336, 336])\n",
      "🎵 Final audio batch shape: torch.Size([4, 64, 1280])\n",
      "📏 序列长度: 文本=38, 图像=576, 音频=64, 总计=678\n",
      "📏 Labels shape: torch.Size([4, 38])\n",
      "初始input ids形状: torch.Size([4, 38])\n",
      "images shape: torch.Size([4, 3, 336, 336])\n",
      "处理图像后, embedding形状: torch.Size([4, 38, 5120])\n",
      "  🔄 Converting encoded audio to torch.bfloat16\n",
      "🔄 AudioProjector forward called with input shape: torch.Size([4, 64, 1280])\n",
      "  Inputs embeds shape: torch.Size([4, 38, 5120])\n",
      "  Inputs embeds dtype: torch.bfloat16\n",
      "  🔄 Converting audio embeds to match inputs_embeds dtype\n",
      "  Final result shape: torch.Size([4, 102, 5120])\n",
      "  Final result dtype: torch.bfloat16\n",
      "插入音频后, 嵌入形状: torch.Size([4, 102, 5120])\n",
      "批量推理失败: The expanded size of the tensor (102) must match the existing size (38) at non-singleton dimension 3.  Target sizes: [4, 40, 102, 102].  Tensor sizes: [4, 1, 102, 38]\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Monica Geller (neutral): Okay, everybody relax. ...\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Monica Geller (neutral): This is not even a date. It's just two people going out to dinner a...\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Chandler Bing (neutral): Sounds like a date to me....\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "No conversation available....\n",
      "📝 批量tokenization完成: input_ids shape: torch.Size([4, 35])\n",
      "🖼️ LLaVA process_images 处理成功，shape: torch.Size([4, 3, 336, 336])\n",
      "🎵 Final audio batch shape: torch.Size([4, 64, 1280])\n",
      "📏 序列长度: 文本=35, 图像=576, 音频=64, 总计=675\n",
      "📏 Labels shape: torch.Size([4, 35])\n",
      "初始input ids形状: torch.Size([4, 35])\n",
      "images shape: torch.Size([4, 3, 336, 336])\n",
      "处理图像后, embedding形状: torch.Size([4, 35, 5120])\n",
      "  🔄 Converting encoded audio to torch.bfloat16\n",
      "🔄 AudioProjector forward called with input shape: torch.Size([4, 64, 1280])\n",
      "  Inputs embeds shape: torch.Size([4, 35, 5120])\n",
      "  Inputs embeds dtype: torch.bfloat16\n",
      "  🔄 Converting audio embeds to match inputs_embeds dtype\n",
      "  Final result shape: torch.Size([4, 99, 5120])\n",
      "  Final result dtype: torch.bfloat16\n",
      "插入音频后, 嵌入形状: torch.Size([4, 99, 5120])\n",
      "批量推理失败: The expanded size of the tensor (99) must match the existing size (35) at non-singleton dimension 3.  Target sizes: [4, 40, 99, 99].  Tensor sizes: [4, 1, 99, 35]\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Chandler Bing (neutral): Alright, so I'm back in high school, I'm standing in the middle of ...\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Monica Geller (neutral): Oh, yeah. Had that dream....\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Chandler Bing (neutral): Then I look down, and I realize there's a phone... there....\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Joey Tribbiani (neutral): Instead of...?...\n",
      "📝 批量tokenization完成: input_ids shape: torch.Size([4, 48])\n",
      "🖼️ LLaVA process_images 处理成功，shape: torch.Size([4, 3, 336, 336])\n",
      "🎵 Final audio batch shape: torch.Size([4, 64, 1280])\n",
      "📏 序列长度: 文本=48, 图像=576, 音频=64, 总计=688\n",
      "📏 Labels shape: torch.Size([4, 48])\n",
      "初始input ids形状: torch.Size([4, 48])\n",
      "images shape: torch.Size([4, 3, 336, 336])\n",
      "处理图像后, embedding形状: torch.Size([4, 48, 5120])\n",
      "  🔄 Converting encoded audio to torch.bfloat16\n",
      "🔄 AudioProjector forward called with input shape: torch.Size([4, 64, 1280])\n",
      "  Inputs embeds shape: torch.Size([4, 48, 5120])\n",
      "  Inputs embeds dtype: torch.bfloat16\n",
      "  🔄 Converting audio embeds to match inputs_embeds dtype\n",
      "  Final result shape: torch.Size([4, 112, 5120])\n",
      "  Final result dtype: torch.bfloat16\n",
      "插入音频后, 嵌入形状: torch.Size([4, 112, 5120])\n",
      "批量推理失败: The expanded size of the tensor (112) must match the existing size (48) at non-singleton dimension 3.  Target sizes: [4, 40, 112, 112].  Tensor sizes: [4, 1, 112, 48]\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Chandler Bing (surprise): That's right....\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Monica Geller (disgust): Never had that dream....\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "Chandler Bing (surprise): All of a sudden, the phone starts to ring. ...\n",
      "📝 构建的对话文本（包含图像token）: <image>\n",
      "No conversation available....\n",
      "📝 批量tokenization完成: input_ids shape: torch.Size([4, 25])\n",
      "🖼️ LLaVA process_images 处理成功，shape: torch.Size([4, 3, 336, 336])\n",
      "🎵 Final audio batch shape: torch.Size([4, 64, 1280])\n",
      "📏 序列长度: 文本=25, 图像=576, 音频=64, 总计=665\n",
      "📏 Labels shape: torch.Size([4, 25])\n",
      "初始input ids形状: torch.Size([4, 25])\n",
      "images shape: torch.Size([4, 3, 336, 336])\n",
      "处理图像后, embedding形状: torch.Size([4, 25, 5120])\n",
      "  🔄 Converting encoded audio to torch.bfloat16\n",
      "🔄 AudioProjector forward called with input shape: torch.Size([4, 64, 1280])\n",
      "  Inputs embeds shape: torch.Size([4, 25, 5120])\n",
      "  Inputs embeds dtype: torch.bfloat16\n",
      "  🔄 Converting audio embeds to match inputs_embeds dtype\n",
      "  Final result shape: torch.Size([4, 89, 5120])\n",
      "  Final result dtype: torch.bfloat16\n",
      "插入音频后, 嵌入形状: torch.Size([4, 89, 5120])\n",
      "批量推理失败: The expanded size of the tensor (89) must match the existing size (25) at non-singleton dimension 3.  Target sizes: [4, 40, 89, 89].  Tensor sizes: [4, 1, 89, 25]\n"
     ]
    }
   ],
   "source": [
    "# 测试 collator\n",
    "print(\"\\n=== 测试批量数据推理 ===\")\n",
    "for batch in dataloader:\n",
    "    try:\n",
    "        batch_result = {k: v.to(triper_model.device) for k, v in batch.items()}\n",
    "        \n",
    "        output = triper_model(\n",
    "            input_ids=batch_result['input_ids'],\n",
    "            images=batch_result['images'],\n",
    "            audio_features=batch_result['audio_features'],\n",
    "            attention_mask=batch_result['attention_mask'],\n",
    "        )\n",
    "        \n",
    "        print(\"批量推理成功！输出结构:\", output.keys())\n",
    "        print(\"输出logits形状:\", output['logits'].shape)\n",
    "        \n",
    "        break\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        print(f\"批量推理失败: {e}\")\n",
    "        if \"device\" in str(e).lower():\n",
    "            print(\"这可能是多GPU模型的设备分布问题\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05dbacc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 当前设备状态:\n",
      "CUDA设备数量: 5\n",
      "当前设备: 0\n",
      "Triper模型设备: cuda:2\n",
      "LLaVA模型设备: cuda:2\n",
      "⚠️ 检测到设备不一致，正在移动模型到cuda:0...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 62.56 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 23.19 GiB is allocated by PyTorch, and 2.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m triper_device \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠️ 检测到设备不一致，正在移动模型到cuda:0...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m     triper_model \u001b[38;5;241m=\u001b[39m \u001b[43mtriper_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ 模型已移动到: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mnext\u001b[39m(triper_model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 验证设备一致性\u001b[39;00m\n",
      "File \u001b[0;32m~/szl_all_code/triper-project/triper/model/triper_arch.py:347\u001b[0m, in \u001b[0;36mTriperModel.to\u001b[0;34m(self, device_or_dtype)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"重写to方法，支持设备和数据类型转换\"\"\"\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# 移动主模型\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_or_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;66;03m# 移动外部音频编码器\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_audio_encoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/transformers/modeling_utils.py:3851\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3846\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3847\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3848\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3849\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3850\u001b[0m         )\n\u001b[0;32m-> 3851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 915 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 62.56 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 23.19 GiB is allocated by PyTorch, and 2.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# 🔧 设备状态诊断和修复\n",
    "import torch\n",
    "print(\"🔍 当前设备状态:\")\n",
    "print(f\"CUDA设备数量: {torch.cuda.device_count()}\")\n",
    "print(f\"当前设备: {torch.cuda.current_device()}\")\n",
    "\n",
    "# 检查模型实际所在设备\n",
    "triper_device = next(triper_model.parameters()).device\n",
    "llava_device = next(triper_model.llava_model.parameters()).device\n",
    "print(f\"Triper模型设备: {triper_device}\")\n",
    "print(f\"LLaVA模型设备: {llava_device}\")\n",
    "\n",
    "# 🚨 如果设备不一致，强制移动到cuda:0\n",
    "if triper_device != torch.device('cuda:0'):\n",
    "    print(f\"⚠️ 检测到设备不一致，正在移动模型到cuda:0...\")\n",
    "    triper_model = triper_model.to('cuda:0')\n",
    "    print(f\"✅ 模型已移动到: {next(triper_model.parameters()).device}\")\n",
    "\n",
    "# 验证设备一致性\n",
    "print(f\"最终设备状态:\")\n",
    "print(f\"  Triper: {next(triper_model.parameters()).device}\")\n",
    "print(f\"  LLaVA: {next(triper_model.llava_model.parameters()).device}\")\n",
    "print(f\"  Audio: {next(triper_model.audio_encoder.parameters()).device}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
