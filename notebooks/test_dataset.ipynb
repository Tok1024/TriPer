{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a99339ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/wly/szl_all_code/triper-project')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3903bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wly/.conda/envs/triper/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Building Triper model from components...\n",
      "   LLaVA model: /sda1/llava-v1.5-13b\n",
      "   Audio encoder: /sda1/glm-4-voice-tokenizer\n",
      "   Audio projector: Built from config\n",
      "   Freeze LLaVA: True\n",
      "ğŸ”„ Loading LLaVA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:06<00:00,  2.11s/it]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Configuring image tokens...\n",
      "æ·»åŠ å›¾åƒtoken: <image>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ›´æ–°IMAGE_TOKEN_INDEXä¸º: 32000\n",
      "ğŸ”„ Building audio encoder...\n",
      "âœ… WhisperVQEncoder loaded from /sda1/glm-4-voice-tokenizer\n",
      "ğŸ”„ Moving audio encoder to device: cuda:2\n",
      "ğŸ”’ Audio encoder parameters frozen\n",
      "âœ… Audio encoder built and moved to cuda:2: WhisperVQEncoder\n",
      "ğŸ”„ Creating Triper model...\n",
      "ğŸ”„ Building audio projector...\n",
      "ğŸ”§ AudioProjector config:\n",
      "  audio_hidden_size: 1280\n",
      "  hidden_size: 5120\n",
      "  projector_type: mlp2x_gelu\n",
      "âœ… AudioProjector created successfully\n",
      "âœ… Audio projector built: AudioProjector\n",
      "âœ… TriperModel initialized with config: triper\n",
      "ğŸ”„ Moving Triper model to device: cuda:2\n",
      "âœ… LLaVA model attached: LlavaLlamaForCausalLM\n",
      "ğŸ”’ LLaVA model parameters frozen\n",
      "ğŸµ Audio encoder attached: WhisperVQEncoder\n",
      "ğŸ“¦ Components set: tokenizer(LlamaTokenizer), processor(CLIPImageProcessor), context_len(2048)\n",
      "âœ… Triper model created successfully!\n",
      "\n",
      "ğŸ—ï¸  Triper Model Summary\n",
      "============================================================\n",
      "ğŸ“¦ Components:\n",
      "  ğŸ¦™ LLaVA: âœ… (LlavaLlamaForCausalLM)\n",
      "  ğŸµ Audio Encoder: âœ… (WhisperVQEncoder) ğŸ”’ External (Frozen)\n",
      "  ğŸ”— Audio Projector: âœ… (AudioProjector) ğŸ”“ Trainable\n",
      "  ğŸ“ Tokenizer: âœ… (LlamaTokenizer) ğŸ”’ External\n",
      "  ğŸ–¼ï¸ Image Processor: âœ… (CLIPImageProcessor) ğŸ”’ External\n",
      "\n",
      "ğŸ“Š Trainable Parameters:\n",
      "  Total: 13,383,638,016\n",
      "  Trainable: 32,788,480 (0.2%)\n",
      "    llava: 13,350,849,536 (0.0% trainable) ğŸ”’\n",
      "    audio_projector: 32,788,480 (100.0% trainable) ğŸ”“\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 13383638016,\n",
       " 'trainable_params': 32788480,\n",
       " 'components': {'llava': {'total': 13350849536, 'trainable': 0},\n",
       "  'audio_projector': {'total': 32788480, 'trainable': 32788480}}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from triper.model import from_pretrained_components\n",
    "audio_config = {\n",
    "    'mm_audio_encoder': 'whisper_vq',\n",
    "    'audio_hidden_size': 1280,  # Whisperè¾“å‡ºç»´åº¦\n",
    "    'audio_model_path': '/sda1/glm-4-voice-tokenizer',\n",
    "    'audio_projector_type': 'mlp2x_gelu',\n",
    "    'audio_projector_hidden_dim': 2048,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "tokenizer, triper_model, image_processor, context_len, audio_encoder = from_pretrained_components(\n",
    "    llava_model_path=\"/sda1/llava-v1.5-13b\",\n",
    "    audio_encoder_path=\"/sda1/glm-4-voice-tokenizer\",\n",
    "    audio_projector_path=None,\n",
    "    audio_config=audio_config,\n",
    "    freeze_llava=True,\n",
    "    device_map=\"cuda:2\"\n",
    ")\n",
    "triper_model.get_parameter_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58434ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨ä»ä»¥ä¸‹è·¯å¾„åŠ è½½æ•°æ®é›†æè¿°æ–‡ä»¶: /home/wly/szl_all_code/triper-project/data/simple_data_20_samples.json\n",
      "å‘ç° 20 ä¸ªæ•°æ®æ ·æœ¬ã€‚\n",
      "æ•°æ®é›†æ¨¡å¼: raw\n",
      "éŸ³é¢‘æ–‡ä»¶å¤¹: /home/wly/szl_all_code/triper-project/data/audio\n",
      "è§†é¢‘æ–‡ä»¶å¤¹: /home/wly/szl_all_code/triper-project/data/video\n",
      "å›¾åƒæ–‡ä»¶å¤¹: /home/wly/szl_all_code/triper-project/data/images\n"
     ]
    }
   ],
   "source": [
    "from triper.data import TriperDataset, TriperDataCollator\n",
    "from torch.utils.data import DataLoader\n",
    "# åˆ›å»ºæ•°æ®é›†\n",
    "dataset = TriperDataset(\n",
    "    json_path='/home/wly/szl_all_code/triper-project/data/simple_data_20_samples.json',\n",
    "    media_root_path='/home/wly/szl_all_code/triper-project/data',\n",
    "    mode=\"raw\"\n",
    ")\n",
    "\n",
    "# åˆ›å»ºcollatorï¼ˆåœ¨è¿™é‡Œä¼ å…¥æ­£ç¡®çš„model_cfgï¼‰\n",
    "collator = TriperDataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    image_processor=image_processor,\n",
    "    audio_processor=audio_encoder,\n",
    "    model_cfg=triper_model.llava_model.config\n",
    ")\n",
    "\n",
    "# åˆ›å»ºDataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=4, \n",
    "    collate_fn=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c3063c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== æµ‹è¯•æ•°æ®æµç¨‹ ===\n",
      "æ¨ç†å‰æ˜¾å­˜ä½¿ç”¨:\n",
      "GPU 0: 0.00GB / 0.00GB\n",
      "GPU 1: 0.00GB / 0.00GB\n",
      "GPU 2: 0.00GB / 0.00GB\n",
      "GPU 3: 26.44GB / 27.84GB\n",
      "GPU 4: 0.00GB / 0.00GB\n",
      "æ ·æœ¬ç»“æ„: dict_keys(['id', 'audio_path', 'image_path', 'conversation', 'scene_description', 'metadata', 'has_audio', 'has_image'])\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Monica Geller (anger): There's nothing to tell! He's just some guy I work with!...\n",
      "ğŸ“ æ‰¹é‡tokenizationå®Œæˆ: input_ids shape: torch.Size([1, 29])\n",
      "ğŸ–¼ï¸ LLaVA process_images å¤„ç†æˆåŠŸï¼Œshape: torch.Size([1, 3, 336, 336])\n",
      "ğŸµ Final audio batch shape: torch.Size([1, 64, 1280])\n",
      "ğŸ“ åºåˆ—é•¿åº¦: æ–‡æœ¬=29, å›¾åƒ=576, éŸ³é¢‘=64, æ€»è®¡=669\n",
      "ğŸ“ Labels shape: torch.Size([1, 29])\n",
      "\n",
      "âœ… Collatorå¤„ç†æˆåŠŸï¼\n",
      "Batchç»“æœ:\n",
      "  input_ids: torch.Size([1, 29]) (dtype: torch.int64)\n",
      "  attention_mask: torch.Size([1, 29]) (dtype: torch.int64)\n",
      "  labels: torch.Size([1, 29]) (dtype: torch.int64)\n",
      "  images: torch.Size([1, 3, 336, 336]) (dtype: torch.float32)\n",
      "  audio_features: torch.Size([1, 64, 1280]) (dtype: torch.float32)\n",
      "\n",
      "=== éªŒè¯å¼ é‡ ===\n",
      "âœ… input_ids: æ˜¯æœ‰æ•ˆçš„torch.Tensor\n",
      "âœ… attention_mask: æ˜¯æœ‰æ•ˆçš„torch.Tensor\n",
      "âœ… labels: æ˜¯æœ‰æ•ˆçš„torch.Tensor\n",
      "âœ… images: æ˜¯æœ‰æ•ˆçš„torch.Tensor\n",
      "âœ… audio_features: æ˜¯æœ‰æ•ˆçš„torch.Tensor\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# æ›´å®‰å…¨çš„æµ‹è¯•ä»£ç \n",
    "print(\"=== æµ‹è¯•æ•°æ®æµç¨‹ ===\")\n",
    "\n",
    "# åœ¨æ¨ç†å‰ç›‘æ§æ˜¾å­˜\n",
    "import torch\n",
    "print(\"æ¨ç†å‰æ˜¾å­˜ä½¿ç”¨:\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.memory_allocated(i)/1024**3:.2f}GB / {torch.cuda.memory_reserved(i)/1024**3:.2f}GB\")\n",
    "\n",
    "# æ¸…ç†ç¼“å­˜\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# æµ‹è¯•å•ä¸ªæ ·æœ¬\n",
    "single_sample = dataset[0]\n",
    "print(\"æ ·æœ¬ç»“æ„:\", single_sample.keys())\n",
    "\n",
    "# æµ‹è¯•collator\n",
    "try:\n",
    "    batch_result = collator([single_sample])\n",
    "    \n",
    "    print(\"\\nâœ… Collatorå¤„ç†æˆåŠŸï¼\")\n",
    "    print(\"Batchç»“æœ:\")\n",
    "    for key, value in batch_result.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"  {key}: {value.shape} (dtype: {value.dtype})\")\n",
    "        else:\n",
    "            print(f\"  {key}: {type(value)}\")\n",
    "    \n",
    "    # éªŒè¯å¼ é‡æ ¼å¼\n",
    "    print(\"\\n=== éªŒè¯å¼ é‡ ===\")\n",
    "    for key, value in batch_result.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"âœ… {key}: æ˜¯æœ‰æ•ˆçš„torch.Tensor\")\n",
    "        else:\n",
    "            print(f\"âŒ {key}: ä¸æ˜¯torch.Tensorï¼Œç±»å‹ä¸º {type(value)}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Collatorå¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c7bc993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== æµ‹è¯•å•ä¸ªæ ·æœ¬æ¨ç† ===\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Monica Geller (anger): There's nothing to tell! He's just some guy I work with!...\n",
      "ğŸ“ æ‰¹é‡tokenizationå®Œæˆ: input_ids shape: torch.Size([1, 29])\n",
      "ğŸ–¼ï¸ LLaVA process_images å¤„ç†æˆåŠŸï¼Œshape: torch.Size([1, 3, 336, 336])\n",
      "ğŸµ Final audio batch shape: torch.Size([1, 64, 1280])\n",
      "ğŸ“ åºåˆ—é•¿åº¦: æ–‡æœ¬=29, å›¾åƒ=576, éŸ³é¢‘=64, æ€»è®¡=669\n",
      "ğŸ“ Labels shape: torch.Size([1, 29])\n",
      "å•ä¸ªæ ·æœ¬ç»“æ„: dict_keys(['id', 'audio_path', 'image_path', 'conversation', 'scene_description', 'metadata', 'has_audio', 'has_image'])\n",
      "åˆå§‹input idså½¢çŠ¶: torch.Size([1, 29])\n",
      "images shape: torch.Size([1, 3, 336, 336])\n",
      "å¤„ç†å›¾åƒå, embeddingå½¢çŠ¶: torch.Size([1, 29, 5120])\n",
      "  ğŸ”„ Converting encoded audio to torch.bfloat16\n",
      "ğŸ”„ AudioProjector forward called with input shape: torch.Size([1, 64, 1280])\n",
      "  Inputs embeds shape: torch.Size([1, 29, 5120])\n",
      "  Inputs embeds dtype: torch.bfloat16\n",
      "  ğŸ”„ Converting audio embeds to match inputs_embeds dtype\n",
      "  Final result shape: torch.Size([1, 93, 5120])\n",
      "  Final result dtype: torch.bfloat16\n",
      "æ’å…¥éŸ³é¢‘å, åµŒå…¥å½¢çŠ¶: torch.Size([1, 93, 5120])\n",
      "å•ä¸ªæ ·æœ¬æ¨ç†æˆåŠŸï¼è¾“å‡ºç»“æ„: odict_keys(['logits', 'past_key_values'])\n",
      "è¾“å‡ºlogitså½¢çŠ¶: torch.Size([1, 93, 32001])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# åœ¨æ¨ç†ä»£ç ä¸­ä½¿ç”¨\n",
    "print(\"\\n=== æµ‹è¯•å•ä¸ªæ ·æœ¬æ¨ç† ===\")\n",
    "single_sample = dataset[0]\n",
    "batch_result = collator([single_sample])\n",
    "print(\"å•ä¸ªæ ·æœ¬ç»“æ„:\", single_sample.keys())\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_result = {k: v.to(triper_model.device) for k, v in batch_result.items()}\n",
    "    \n",
    "    try:\n",
    "        output = triper_model(\n",
    "            input_ids=batch_result['input_ids'],\n",
    "            attention_mask=batch_result['attention_mask'],\n",
    "            images=batch_result['images'],\n",
    "            audio_features=batch_result['audio_features']\n",
    "        )\n",
    "        print(\"å•ä¸ªæ ·æœ¬æ¨ç†æˆåŠŸï¼è¾“å‡ºç»“æ„:\", output.keys())\n",
    "        print(\"è¾“å‡ºlogitså½¢çŠ¶:\", output['logits'].shape)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"æ¨ç†å¤±è´¥: {e}\")\n",
    "        if \"device\" in str(e).lower():\n",
    "            print(\"è¿™å¯èƒ½æ˜¯å¤šGPUæ¨¡å‹çš„è®¾å¤‡åˆ†å¸ƒé—®é¢˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abfbbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== æµ‹è¯•æ‰¹é‡æ•°æ®æ¨ç† ===\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Monica Geller (anger): There's nothing to tell! He's just some guy I work with!...\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Joey Tribbiani (surprise): C'mon, you're going out with the guy! There's gotta be something ...\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "No conversation available....\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Chandler Bing (neutral): So does he have a hump? A hump and a hairpiece?...\n",
      "ğŸ“ æ‰¹é‡tokenizationå®Œæˆ: input_ids shape: torch.Size([4, 38])\n",
      "ğŸ–¼ï¸ LLaVA process_images å¤„ç†æˆåŠŸï¼Œshape: torch.Size([4, 3, 336, 336])\n",
      "ğŸµ Final audio batch shape: torch.Size([4, 64, 1280])\n",
      "ğŸ“ åºåˆ—é•¿åº¦: æ–‡æœ¬=38, å›¾åƒ=576, éŸ³é¢‘=64, æ€»è®¡=678\n",
      "ğŸ“ Labels shape: torch.Size([4, 38])\n",
      "åˆå§‹input idså½¢çŠ¶: torch.Size([4, 38])\n",
      "images shape: torch.Size([4, 3, 336, 336])\n",
      "å¤„ç†å›¾åƒå, embeddingå½¢çŠ¶: torch.Size([4, 38, 5120])\n",
      "  ğŸ”„ Converting encoded audio to torch.bfloat16\n",
      "ğŸ”„ AudioProjector forward called with input shape: torch.Size([4, 64, 1280])\n",
      "  Inputs embeds shape: torch.Size([4, 38, 5120])\n",
      "  Inputs embeds dtype: torch.bfloat16\n",
      "  ğŸ”„ Converting audio embeds to match inputs_embeds dtype\n",
      "  Final result shape: torch.Size([4, 102, 5120])\n",
      "  Final result dtype: torch.bfloat16\n",
      "æ’å…¥éŸ³é¢‘å, åµŒå…¥å½¢çŠ¶: torch.Size([4, 102, 5120])\n",
      "æ‰¹é‡æ¨ç†å¤±è´¥: The expanded size of the tensor (102) must match the existing size (38) at non-singleton dimension 3.  Target sizes: [4, 40, 102, 102].  Tensor sizes: [4, 1, 102, 38]\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "No conversation available....\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Phoebe Buffay (neutral): Wait, does he eat chalk?...\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "No conversation available....\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Phoebe Buffay (neutral): Just, 'cause, I don't want her to go through what I went through wi...\n",
      "ğŸ“ æ‰¹é‡tokenizationå®Œæˆ: input_ids shape: torch.Size([4, 38])\n",
      "ğŸ–¼ï¸ LLaVA process_images å¤„ç†æˆåŠŸï¼Œshape: torch.Size([4, 3, 336, 336])\n",
      "ğŸµ Final audio batch shape: torch.Size([4, 64, 1280])\n",
      "ğŸ“ åºåˆ—é•¿åº¦: æ–‡æœ¬=38, å›¾åƒ=576, éŸ³é¢‘=64, æ€»è®¡=678\n",
      "ğŸ“ Labels shape: torch.Size([4, 38])\n",
      "åˆå§‹input idså½¢çŠ¶: torch.Size([4, 38])\n",
      "images shape: torch.Size([4, 3, 336, 336])\n",
      "å¤„ç†å›¾åƒå, embeddingå½¢çŠ¶: torch.Size([4, 38, 5120])\n",
      "  ğŸ”„ Converting encoded audio to torch.bfloat16\n",
      "ğŸ”„ AudioProjector forward called with input shape: torch.Size([4, 64, 1280])\n",
      "  Inputs embeds shape: torch.Size([4, 38, 5120])\n",
      "  Inputs embeds dtype: torch.bfloat16\n",
      "  ğŸ”„ Converting audio embeds to match inputs_embeds dtype\n",
      "  Final result shape: torch.Size([4, 102, 5120])\n",
      "  Final result dtype: torch.bfloat16\n",
      "æ’å…¥éŸ³é¢‘å, åµŒå…¥å½¢çŠ¶: torch.Size([4, 102, 5120])\n",
      "æ‰¹é‡æ¨ç†å¤±è´¥: The expanded size of the tensor (102) must match the existing size (38) at non-singleton dimension 3.  Target sizes: [4, 40, 102, 102].  Tensor sizes: [4, 1, 102, 38]\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Monica Geller (neutral): Okay, everybody relax. ...\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Monica Geller (neutral): This is not even a date. It's just two people going out to dinner a...\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Chandler Bing (neutral): Sounds like a date to me....\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "No conversation available....\n",
      "ğŸ“ æ‰¹é‡tokenizationå®Œæˆ: input_ids shape: torch.Size([4, 35])\n",
      "ğŸ–¼ï¸ LLaVA process_images å¤„ç†æˆåŠŸï¼Œshape: torch.Size([4, 3, 336, 336])\n",
      "ğŸµ Final audio batch shape: torch.Size([4, 64, 1280])\n",
      "ğŸ“ åºåˆ—é•¿åº¦: æ–‡æœ¬=35, å›¾åƒ=576, éŸ³é¢‘=64, æ€»è®¡=675\n",
      "ğŸ“ Labels shape: torch.Size([4, 35])\n",
      "åˆå§‹input idså½¢çŠ¶: torch.Size([4, 35])\n",
      "images shape: torch.Size([4, 3, 336, 336])\n",
      "å¤„ç†å›¾åƒå, embeddingå½¢çŠ¶: torch.Size([4, 35, 5120])\n",
      "  ğŸ”„ Converting encoded audio to torch.bfloat16\n",
      "ğŸ”„ AudioProjector forward called with input shape: torch.Size([4, 64, 1280])\n",
      "  Inputs embeds shape: torch.Size([4, 35, 5120])\n",
      "  Inputs embeds dtype: torch.bfloat16\n",
      "  ğŸ”„ Converting audio embeds to match inputs_embeds dtype\n",
      "  Final result shape: torch.Size([4, 99, 5120])\n",
      "  Final result dtype: torch.bfloat16\n",
      "æ’å…¥éŸ³é¢‘å, åµŒå…¥å½¢çŠ¶: torch.Size([4, 99, 5120])\n",
      "æ‰¹é‡æ¨ç†å¤±è´¥: The expanded size of the tensor (99) must match the existing size (35) at non-singleton dimension 3.  Target sizes: [4, 40, 99, 99].  Tensor sizes: [4, 1, 99, 35]\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Chandler Bing (neutral): Alright, so I'm back in high school, I'm standing in the middle of ...\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Monica Geller (neutral): Oh, yeah. Had that dream....\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Chandler Bing (neutral): Then I look down, and I realize there's a phone... there....\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Joey Tribbiani (neutral): Instead of...?...\n",
      "ğŸ“ æ‰¹é‡tokenizationå®Œæˆ: input_ids shape: torch.Size([4, 48])\n",
      "ğŸ–¼ï¸ LLaVA process_images å¤„ç†æˆåŠŸï¼Œshape: torch.Size([4, 3, 336, 336])\n",
      "ğŸµ Final audio batch shape: torch.Size([4, 64, 1280])\n",
      "ğŸ“ åºåˆ—é•¿åº¦: æ–‡æœ¬=48, å›¾åƒ=576, éŸ³é¢‘=64, æ€»è®¡=688\n",
      "ğŸ“ Labels shape: torch.Size([4, 48])\n",
      "åˆå§‹input idså½¢çŠ¶: torch.Size([4, 48])\n",
      "images shape: torch.Size([4, 3, 336, 336])\n",
      "å¤„ç†å›¾åƒå, embeddingå½¢çŠ¶: torch.Size([4, 48, 5120])\n",
      "  ğŸ”„ Converting encoded audio to torch.bfloat16\n",
      "ğŸ”„ AudioProjector forward called with input shape: torch.Size([4, 64, 1280])\n",
      "  Inputs embeds shape: torch.Size([4, 48, 5120])\n",
      "  Inputs embeds dtype: torch.bfloat16\n",
      "  ğŸ”„ Converting audio embeds to match inputs_embeds dtype\n",
      "  Final result shape: torch.Size([4, 112, 5120])\n",
      "  Final result dtype: torch.bfloat16\n",
      "æ’å…¥éŸ³é¢‘å, åµŒå…¥å½¢çŠ¶: torch.Size([4, 112, 5120])\n",
      "æ‰¹é‡æ¨ç†å¤±è´¥: The expanded size of the tensor (112) must match the existing size (48) at non-singleton dimension 3.  Target sizes: [4, 40, 112, 112].  Tensor sizes: [4, 1, 112, 48]\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Chandler Bing (surprise): That's right....\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Monica Geller (disgust): Never had that dream....\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "Chandler Bing (surprise): All of a sudden, the phone starts to ring. ...\n",
      "ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: <image>\n",
      "No conversation available....\n",
      "ğŸ“ æ‰¹é‡tokenizationå®Œæˆ: input_ids shape: torch.Size([4, 25])\n",
      "ğŸ–¼ï¸ LLaVA process_images å¤„ç†æˆåŠŸï¼Œshape: torch.Size([4, 3, 336, 336])\n",
      "ğŸµ Final audio batch shape: torch.Size([4, 64, 1280])\n",
      "ğŸ“ åºåˆ—é•¿åº¦: æ–‡æœ¬=25, å›¾åƒ=576, éŸ³é¢‘=64, æ€»è®¡=665\n",
      "ğŸ“ Labels shape: torch.Size([4, 25])\n",
      "åˆå§‹input idså½¢çŠ¶: torch.Size([4, 25])\n",
      "images shape: torch.Size([4, 3, 336, 336])\n",
      "å¤„ç†å›¾åƒå, embeddingå½¢çŠ¶: torch.Size([4, 25, 5120])\n",
      "  ğŸ”„ Converting encoded audio to torch.bfloat16\n",
      "ğŸ”„ AudioProjector forward called with input shape: torch.Size([4, 64, 1280])\n",
      "  Inputs embeds shape: torch.Size([4, 25, 5120])\n",
      "  Inputs embeds dtype: torch.bfloat16\n",
      "  ğŸ”„ Converting audio embeds to match inputs_embeds dtype\n",
      "  Final result shape: torch.Size([4, 89, 5120])\n",
      "  Final result dtype: torch.bfloat16\n",
      "æ’å…¥éŸ³é¢‘å, åµŒå…¥å½¢çŠ¶: torch.Size([4, 89, 5120])\n",
      "æ‰¹é‡æ¨ç†å¤±è´¥: The expanded size of the tensor (89) must match the existing size (25) at non-singleton dimension 3.  Target sizes: [4, 40, 89, 89].  Tensor sizes: [4, 1, 89, 25]\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯• collator\n",
    "print(\"\\n=== æµ‹è¯•æ‰¹é‡æ•°æ®æ¨ç† ===\")\n",
    "for batch in dataloader:\n",
    "    try:\n",
    "        batch_result = {k: v.to(triper_model.device) for k, v in batch.items()}\n",
    "        \n",
    "        output = triper_model(\n",
    "            input_ids=batch_result['input_ids'],\n",
    "            images=batch_result['images'],\n",
    "            audio_features=batch_result['audio_features'],\n",
    "            attention_mask=batch_result['attention_mask'],\n",
    "        )\n",
    "        \n",
    "        print(\"æ‰¹é‡æ¨ç†æˆåŠŸï¼è¾“å‡ºç»“æ„:\", output.keys())\n",
    "        print(\"è¾“å‡ºlogitså½¢çŠ¶:\", output['logits'].shape)\n",
    "        \n",
    "        break\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        print(f\"æ‰¹é‡æ¨ç†å¤±è´¥: {e}\")\n",
    "        if \"device\" in str(e).lower():\n",
    "            print(\"è¿™å¯èƒ½æ˜¯å¤šGPUæ¨¡å‹çš„è®¾å¤‡åˆ†å¸ƒé—®é¢˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05dbacc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” å½“å‰è®¾å¤‡çŠ¶æ€:\n",
      "CUDAè®¾å¤‡æ•°é‡: 5\n",
      "å½“å‰è®¾å¤‡: 0\n",
      "Triperæ¨¡å‹è®¾å¤‡: cuda:2\n",
      "LLaVAæ¨¡å‹è®¾å¤‡: cuda:2\n",
      "âš ï¸ æ£€æµ‹åˆ°è®¾å¤‡ä¸ä¸€è‡´ï¼Œæ­£åœ¨ç§»åŠ¨æ¨¡å‹åˆ°cuda:0...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 62.56 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 23.19 GiB is allocated by PyTorch, and 2.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m triper_device \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâš ï¸ æ£€æµ‹åˆ°è®¾å¤‡ä¸ä¸€è‡´ï¼Œæ­£åœ¨ç§»åŠ¨æ¨¡å‹åˆ°cuda:0...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m     triper_model \u001b[38;5;241m=\u001b[39m \u001b[43mtriper_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mnext\u001b[39m(triper_model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# éªŒè¯è®¾å¤‡ä¸€è‡´æ€§\u001b[39;00m\n",
      "File \u001b[0;32m~/szl_all_code/triper-project/triper/model/triper_arch.py:347\u001b[0m, in \u001b[0;36mTriperModel.to\u001b[0;34m(self, device_or_dtype)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"é‡å†™toæ–¹æ³•ï¼Œæ”¯æŒè®¾å¤‡å’Œæ•°æ®ç±»å‹è½¬æ¢\"\"\"\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# ç§»åŠ¨ä¸»æ¨¡å‹\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_or_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;66;03m# ç§»åŠ¨å¤–éƒ¨éŸ³é¢‘ç¼–ç å™¨\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_audio_encoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/transformers/modeling_utils.py:3851\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3846\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3847\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3848\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3849\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3850\u001b[0m         )\n\u001b[0;32m-> 3851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 915 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/triper/lib/python3.10/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 62.56 MiB is free. Including non-PyTorch memory, this process has 23.56 GiB memory in use. Of the allocated memory 23.19 GiB is allocated by PyTorch, and 2.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ğŸ”§ è®¾å¤‡çŠ¶æ€è¯Šæ–­å’Œä¿®å¤\n",
    "import torch\n",
    "print(\"ğŸ” å½“å‰è®¾å¤‡çŠ¶æ€:\")\n",
    "print(f\"CUDAè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}\")\n",
    "print(f\"å½“å‰è®¾å¤‡: {torch.cuda.current_device()}\")\n",
    "\n",
    "# æ£€æŸ¥æ¨¡å‹å®é™…æ‰€åœ¨è®¾å¤‡\n",
    "triper_device = next(triper_model.parameters()).device\n",
    "llava_device = next(triper_model.llava_model.parameters()).device\n",
    "print(f\"Triperæ¨¡å‹è®¾å¤‡: {triper_device}\")\n",
    "print(f\"LLaVAæ¨¡å‹è®¾å¤‡: {llava_device}\")\n",
    "\n",
    "# ğŸš¨ å¦‚æœè®¾å¤‡ä¸ä¸€è‡´ï¼Œå¼ºåˆ¶ç§»åŠ¨åˆ°cuda:0\n",
    "if triper_device != torch.device('cuda:0'):\n",
    "    print(f\"âš ï¸ æ£€æµ‹åˆ°è®¾å¤‡ä¸ä¸€è‡´ï¼Œæ­£åœ¨ç§»åŠ¨æ¨¡å‹åˆ°cuda:0...\")\n",
    "    triper_model = triper_model.to('cuda:0')\n",
    "    print(f\"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°: {next(triper_model.parameters()).device}\")\n",
    "\n",
    "# éªŒè¯è®¾å¤‡ä¸€è‡´æ€§\n",
    "print(f\"æœ€ç»ˆè®¾å¤‡çŠ¶æ€:\")\n",
    "print(f\"  Triper: {next(triper_model.parameters()).device}\")\n",
    "print(f\"  LLaVA: {next(triper_model.llava_model.parameters()).device}\")\n",
    "print(f\"  Audio: {next(triper_model.audio_encoder.parameters()).device}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
