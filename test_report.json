================================================================================
ğŸš€ TRIPER MODEL COMPREHENSIVE TEST
================================================================================
âœ… GPUç¼“å­˜å·²æ¸…ç†

ğŸ“‹ æµ‹è¯•é…ç½®:
  llava_model_path: /sda1/llava-v1.5-13b
  audio_encoder_path: /sda1/glm-4-voice-tokenizer
  data_path: /home/wly/szl_all_code/triper-project/data/simple_data_20_samples.json
  media_root: /home/wly/szl_all_code/triper-project/data
  device: cuda:3
  freeze_llava: True

==================================================
ğŸ“¦ æµ‹è¯•1: æ¨¡å‹ç»„ä»¶åŠ è½½
==================================================
ğŸ”„ åŠ è½½æ¨¡å‹ç»„ä»¶...
ğŸ”„ Building Triper model from components...
   LLaVA model: /sda1/llava-v1.5-13b
   Audio encoder: /sda1/glm-4-voice-tokenizer
   Audio projector: Built from config
   Freeze LLaVA: True
ğŸ”„ Loading LLaVA model...
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards: 100%|
ğŸ”„ Building audio encoder...
âœ… WhisperVQEncoder loaded from /sda1/glm-4-voice-tokenizer
ğŸ”„ Moving audio encoder to device: cuda:3
ğŸ”’ Audio encoder parameters frozen
âœ… Audio encoder built and moved to cuda:3: WhisperVQEncoder
ğŸ”„ Creating Triper model...
TriperModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
ğŸ”„ Building audio projector...
ğŸ”§ AudioProjector config:
  audio_hidden_size: 1280
  hidden_size: 5120
  projector_type: mlp2x_gelu
âœ… AudioProjector created successfully
âœ… Audio projector built: AudioProjector
âœ… TriperModel initialized with config: triper
ğŸ”„ Moving Triper model to device: cuda:3
âœ… LLaVA model attached: LlavaLlamaForCausalLM
ğŸ”’ LLaVA model parameters frozen
ğŸµ Audio encoder attached: WhisperVQEncoder
ğŸ“¦ Components set: tokenizer(LlamaTokenizer), processor(CLIPImageProcessor), context_len(2048)
âœ… Triper model created successfully!

ğŸ—ï¸  Triper Model Summary
============================================================
ğŸ“¦ Components:
  ğŸ¦™ LLaVA: âœ… (LlavaLlamaForCausalLM)
  ğŸµ Audio Encoder: âœ… (WhisperVQEncoder) ğŸ”’ External (Frozen)
  ğŸ”— Audio Projector: âœ… (AudioProjector) ğŸ”“ Trainable
  ğŸ“ Tokenizer: âœ… (LlamaTokenizer) ğŸ”’ External
  ğŸ–¼ï¸ Image Processor: âœ… (CLIPImageProcessor) ğŸ”’ External

ğŸ“Š Trainable Parameters:
  Total: 13,383,627,776
  Trainable: 32,788,480 (0.2%)
    llava: 13,350,839,296 (0.0% trainable) ğŸ”’
    audio_projector: 32,788,480 (100.0% trainable) ğŸ”“
============================================================
âœ… æ¨¡å‹ç»„ä»¶åŠ è½½æˆåŠŸ

ğŸ—ï¸  Triper Model Summary
============================================================
ğŸ“¦ Components:
  ğŸ¦™ LLaVA: âœ… (LlavaLlamaForCausalLM)
  ğŸµ Audio Encoder: âœ… (WhisperVQEncoder) ğŸ”’ External (Frozen)
  ğŸ”— Audio Projector: âœ… (AudioProjector) ğŸ”“ Trainable
  ğŸ“ Tokenizer: âœ… (LlamaTokenizer) ğŸ”’ External
  ğŸ–¼ï¸ Image Processor: âœ… (CLIPImageProcessor) ğŸ”’ External

ğŸ“Š Trainable Parameters:
  Total: 13,383,627,776
  Trainable: 32,788,480 (0.2%)
    llava: 13,350,839,296 (0.0% trainable) ğŸ”’
    audio_projector: 32,788,480 (100.0% trainable) ğŸ”“
============================================================

==================================================
ğŸ“Š æµ‹è¯•2: æ•°æ®é›†å’ŒCollator
==================================================
ğŸ”„ åŠ è½½æ•°æ®é›†...
æ­£åœ¨ä»ä»¥ä¸‹è·¯å¾„åŠ è½½æ•°æ®é›†æè¿°æ–‡ä»¶: /home/wly/szl_all_code/triper-project/data/simple_data_20_samples.json
å‘ç° 20 ä¸ªæ•°æ®æ ·æœ¬ã€‚
æ•°æ®é›†æ¨¡å¼: raw
éŸ³é¢‘æ–‡ä»¶å¤¹: /home/wly/szl_all_code/triper-project/data/audio
è§†é¢‘æ–‡ä»¶å¤¹: /home/wly/szl_all_code/triper-project/data/video
å›¾åƒæ–‡ä»¶å¤¹: /home/wly/szl_all_code/triper-project/data/images
âœ… æ•°æ®é›†åŠ è½½æˆåŠŸï¼Œå…± 20 ä¸ªæ ·æœ¬
ğŸ”„ åˆ›å»ºæ•°æ®collator...
âœ… Collatoråˆ›å»ºæˆåŠŸ
ğŸ”„ æµ‹è¯•å•ä¸ªæ ·æœ¬...
æ ·æœ¬ç»“æ„: ['id', 'audio_path', 'image_path', 'conversation', 'scene_description', 'metadata', 'has_audio', 'has_image']
ğŸ”„ æµ‹è¯•collatorå¤„ç†...
ğŸ“ å¯¹è¯é¢„æµ‹æ ¼å¼:
<image>
USER: Based on what you see and hear in this scene, what would Monica Geller say?
ASSISTANT:
ğŸ“ åŸå§‹æ–‡æœ¬é•¿åº¦èŒƒå›´: 30 - 30
âœ… æ‰¹é‡tokenizationå®Œæˆ: input_ids shape: torch.Size([1, 30])
âœ… æ‰€æœ‰æ ·æœ¬æ–‡æœ¬é•¿åº¦ç»Ÿä¸€ä¸º: 30
âœ… å›¾åƒå¤„ç†æˆåŠŸ: torch.Size([1, 3, 336, 336])
âœ… éŸ³é¢‘æ‰¹é‡å¤„ç†å®Œæˆ: torch.Size([1, 64, 1280])
æ‰¹é‡æ•°æ®å½¢çŠ¶:
  input_ids: torch.Size([1, 30])
  attention_mask: torch.Size([1, 30])
  labels: torch.Size([1, 30])
  images: torch.Size([1, 3, 336, 336])
  audio_features: torch.Size([1, 64, 1280])

==================================================
ğŸ”¥ æµ‹è¯•3: æ¨¡å‹å‰å‘ä¼ æ’­
==================================================
ğŸ“ å¯¹è¯é¢„æµ‹æ ¼å¼:
<image>
USER: Based on what you see and hear in this scene, what would Monica Geller say?
ASSISTANT:
ğŸ“ å¯¹è¯é¢„æµ‹æ ¼å¼:
<image>
USER: Based on what you see and hear in this scene, what would Joey Tribbiani say?
ASSISTANT:
ğŸ“ åŸå§‹æ–‡æœ¬é•¿åº¦èŒƒå›´: 30 - 31
âš ï¸ æ–‡æœ¬é•¿åº¦ä¸ä¸€è‡´ï¼Œå°†paddingåˆ°: 31
âœ… æ‰¹é‡tokenizationå®Œæˆ: input_ids shape: torch.Size([2, 31])
âœ… æ‰€æœ‰æ ·æœ¬æ–‡æœ¬é•¿åº¦ç»Ÿä¸€ä¸º: 31
âœ… å›¾åƒå¤„ç†æˆåŠŸ: torch.Size([2, 3, 336, 336])
âœ… éŸ³é¢‘æ‰¹é‡å¤„ç†å®Œæˆ: torch.Size([2, 64, 1280])
ğŸ”„ æµ‹è¯•å®Œæ•´å‰å‘ä¼ æ’­ï¼ˆå›¾åƒ+éŸ³é¢‘ï¼‰...
ğŸ” TokenèŒƒå›´æ£€æŸ¥: vocab_size=32000, input_ids range=(-200, 29973)
ğŸ” Attention maskæ£€æŸ¥: dtype=torch.int64, å€¼åŸŸ=(0, 1)
ğŸ”¥ TriperModel.forward called:
  input_ids: torch.Size([2, 31])
  images: torch.Size([2, 3, 336, 336])
  audio_features: torch.Size([2, 64, 1280])
  past_key_values: 0 layers
  ğŸ“¸ LLaVAå¤„ç†å›¾åƒ...
  LLaVAå¤„ç†åembeds: torch.Size([2, 606, 5120])
  ğŸµ æ’å…¥éŸ³é¢‘ç‰¹å¾...
ğŸ”„ AudioProjector forward called with input shape: torch.Size([2, 64, 1280])
ğŸµ éŸ³é¢‘ç‰¹å¾æ’å…¥å®Œæˆ:
  åŸå§‹embeds: torch.Size([2, 606, 5120])
  éŸ³é¢‘embeds: torch.Size([2, 64, 5120])
  åˆå¹¶åembeds: torch.Size([2, 670, 5120])
  åˆå¹¶åattention_mask: torch.Size([2, 670])
  åˆå¹¶åembeds: torch.Size([2, 670, 5120])
  åˆå¹¶åattention_mask: torch.Size([2, 670])
  ğŸ” æœ€ç»ˆéªŒè¯:
    inputs_embeds: torch.Size([2, 670, 5120])
    attention_mask: torch.Size([2, 670])
ğŸ”„ æµ‹è¯•ä»…å›¾åƒå‰å‘ä¼ æ’­...
ğŸ” TokenèŒƒå›´æ£€æŸ¥: vocab_size=32000, input_ids range=(-200, 29973)
ğŸ” Attention maskæ£€æŸ¥: dtype=torch.int64, å€¼åŸŸ=(0, 1)
ğŸ”¥ TriperModel.forward called:
  input_ids: torch.Size([2, 31])
  images: torch.Size([2, 3, 336, 336])
  audio_features: None
  past_key_values: 0 layers
  ğŸ“¸ LLaVAå¤„ç†å›¾åƒ...
  LLaVAå¤„ç†åembeds: torch.Size([2, 606, 5120])
  ğŸ” æœ€ç»ˆéªŒè¯:
    inputs_embeds: torch.Size([2, 606, 5120])
    attention_mask: torch.Size([2, 606])
âœ… ä»…å›¾åƒå‰å‘ä¼ æ’­æˆåŠŸ: torch.Size([2, 606, 32000])
ğŸ”„ æµ‹è¯•çº¯æ–‡æœ¬å‰å‘ä¼ æ’­...
ğŸ“ çº¯æ–‡æœ¬prompt: USER: Tell me about artificial intelligence.
ASSISTANT:
ğŸ“ çº¯æ–‡æœ¬input_idså½¢çŠ¶: torch.Size([1, 15])
ğŸ” TokenèŒƒå›´æ£€æŸ¥: vocab_size=32000, input_ids range=(1, 29901)
ğŸ” Attention maskæ£€æŸ¥: dtype=torch.int64, å€¼åŸŸ=(1, 1)
ğŸ”¥ TriperModel.forward called:
  input_ids: torch.Size([1, 15])
  images: None
  audio_features: None
  past_key_values: 0 layers
  ğŸ“ çº¯æ–‡æœ¬è¾“å…¥ï¼Œç›´æ¥ä¼ é€’input_ids...
âœ… çº¯æ–‡æœ¬å‰å‘ä¼ æ’­æˆåŠŸ: torch.Size([1, 15, 32000])
âœ… å®Œæ•´å‰å‘ä¼ æ’­æˆåŠŸ
  è¾“å‡ºlogitså½¢çŠ¶: torch.Size([2, 670, 32000])
  è¾“å‡ºç±»å‹: <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>
ğŸ”„ æµ‹è¯•ä»…å›¾åƒå‰å‘ä¼ æ’­...
ğŸ” TokenèŒƒå›´æ£€æŸ¥: vocab_size=32000, input_ids range=(-200, 29973)
ğŸ” Attention maskæ£€æŸ¥: dtype=torch.int64, å€¼åŸŸ=(0, 1)
ğŸ”¥ TriperModel.forward called:
  input_ids: torch.Size([2, 31])
  images: torch.Size([2, 3, 336, 336])
  audio_features: None
  past_key_values: 0 layers
  ğŸ“¸ LLaVAå¤„ç†å›¾åƒ...
  LLaVAå¤„ç†åembeds: torch.Size([2, 606, 5120])
  ğŸ” æœ€ç»ˆéªŒè¯:
    inputs_embeds: torch.Size([2, 606, 5120])
    attention_mask: torch.Size([2, 606])
âœ… ä»…å›¾åƒå‰å‘ä¼ æ’­æˆåŠŸ: torch.Size([2, 606, 32000])

==================================================
ğŸš€ æµ‹è¯•4: æ¨¡å‹ç”Ÿæˆèƒ½åŠ›
==================================================
ğŸ“ å¯¹è¯é¢„æµ‹æ ¼å¼:
<image>
USER: Based on what you see and hear in this scene, what would Monica Geller say?
ASSISTANT:
ğŸ“ åŸå§‹æ–‡æœ¬é•¿åº¦èŒƒå›´: 30 - 30
âœ… æ‰¹é‡tokenizationå®Œæˆ: input_ids shape: torch.Size([1, 30])
âœ… æ‰€æœ‰æ ·æœ¬æ–‡æœ¬é•¿åº¦ç»Ÿä¸€ä¸º: 30
âœ… å›¾åƒå¤„ç†æˆåŠŸ: torch.Size([1, 3, 336, 336])
âœ… éŸ³é¢‘æ‰¹é‡å¤„ç†å®Œæˆ: torch.Size([1, 64, 1280])
ğŸ”„ æµ‹è¯•1: çº¯LLaVAç”Ÿæˆï¼ˆå›¾åƒ+æ–‡æœ¬ï¼‰...
ğŸš€ TriperModel.generate called:
  input_ids: torch.Size([1, 30])
  images: torch.Size([1, 3, 336, 336])
  audio_features: None
ğŸ“ æ— éŸ³é¢‘è¾“å…¥ï¼Œç›´æ¥ä½¿ç”¨LLaVA...
âš ï¸ LLaVAç”Ÿæˆé•¿åº¦å¼‚å¸¸: torch.Size([1, 30])
ğŸ”„ æµ‹è¯•2: å®Œæ•´Triperç”Ÿæˆï¼ˆå›¾åƒ+éŸ³é¢‘+æ–‡æœ¬ï¼‰...
ğŸš€ TriperModel.generate called:
  input_ids: torch.Size([1, 30])
  images: torch.Size([1, 3, 336, 336])
  audio_features: torch.Size([1, 64, 1280])
ğŸµ æ£€æµ‹åˆ°éŸ³é¢‘è¾“å…¥ï¼Œå‡†å¤‡å¤šæ¨¡æ€embeddings...
ğŸ“¸ LLaVAå¤„ç†å›¾åƒ...
LLaVAå¤„ç†åembeds: torch.Size([1, 605, 5120])
ğŸµ é›†æˆéŸ³é¢‘ç‰¹å¾...
ğŸ”„ AudioProjector forward called with input shape: torch.Size([1, 64, 1280])
ğŸµ éŸ³é¢‘ç‰¹å¾æ’å…¥å®Œæˆ:
  åŸå§‹embeds: torch.Size([1, 605, 5120])
  éŸ³é¢‘embeds: torch.Size([1, 64, 5120])
  åˆå¹¶åembeds: torch.Size([1, 669, 5120])
  åˆå¹¶åattention_mask: torch.Size([1, 669])
æœ€ç»ˆembeds: torch.Size([1, 669, 5120])
æœ€ç»ˆattention_mask: torch.Size([1, 669])
ğŸš€ è°ƒç”¨LLaVA.generate with inputs_embeds...
âœ… Triperç”ŸæˆæˆåŠŸ: '.com...'
ğŸ”„ æµ‹è¯•3: ç®€åŒ–promptç”Ÿæˆ...
âœ… ç®€åŒ–promptç”ŸæˆæˆåŠŸ: 'appears to be engaged in a conversation or using the phone for some purpose. The room has a cozy atm...'

==================================================
ğŸµ æµ‹è¯•5: éŸ³é¢‘æŠ•å½±å™¨
==================================================
ğŸ”„ æµ‹è¯•éŸ³é¢‘æŠ•å½±å™¨...
  è¾“å…¥å½¢çŠ¶: torch.Size([1, 64, 1280])
ğŸ”„ AudioProjector forward called with input shape: torch.Size([1, 64, 1280])
  è¾“å‡ºå½¢çŠ¶: torch.Size([1, 64, 5120])
  è¾“å‡ºç»´åº¦: 5120
âœ… éŸ³é¢‘æŠ•å½±å™¨æµ‹è¯•é€šè¿‡
ğŸ”„ æµ‹è¯•éŸ³é¢‘ç‰¹å¾æ’å…¥...
ğŸ”„ AudioProjector forward called with input shape: torch.Size([1, 64, 1280])
ğŸµ éŸ³é¢‘ç‰¹å¾æ’å…¥å®Œæˆ:
  åŸå§‹embeds: torch.Size([1, 50, 5120])
  éŸ³é¢‘embeds: torch.Size([1, 64, 5120])
  åˆå¹¶åembeds: torch.Size([1, 114, 5120])
  åˆå¹¶åattention_mask: torch.Size([1, 114])
âœ… éŸ³é¢‘ç‰¹å¾æ’å…¥æµ‹è¯•é€šè¿‡

==================================================
ğŸ“Š æµ‹è¯•6: å‚æ•°ç»Ÿè®¡å’Œé…ç½®
==================================================
ğŸ“Š å‚æ•°ç»Ÿè®¡:
  æ€»å‚æ•°: 13,383,627,776
  å¯è®­ç»ƒå‚æ•°: 32,788,480
  å¯è®­ç»ƒæ¯”ä¾‹: 0.24%
  llava: 13,350,839,296 æ€»è®¡, 0 å¯è®­ç»ƒ
  audio_projector: 32,788,480 æ€»è®¡, 32,788,480 å¯è®­ç»ƒ
ğŸ”„ æµ‹è¯•è®¾å¤‡ç§»åŠ¨...
  å½“å‰è®¾å¤‡: cuda:3
âœ… å‚æ•°ç»Ÿè®¡å’Œé…ç½®æµ‹è¯•é€šè¿‡

==================================================
ğŸ’¬ æµ‹è¯•7: å¯¹è¯é¢„æµ‹ä»»åŠ¡
==================================================
ğŸ“ å¯¹è¯é¢„æµ‹æ ¼å¼:
<image>
USER: Based on what you see and hear in this scene, what would Monica Geller say?
ASSISTANT:
ğŸ“ åŸå§‹æ–‡æœ¬é•¿åº¦èŒƒå›´: 30 - 30
âœ… æ‰¹é‡tokenizationå®Œæˆ: input_ids shape: torch.Size([1, 30])
âœ… æ‰€æœ‰æ ·æœ¬æ–‡æœ¬é•¿åº¦ç»Ÿä¸€ä¸º: 30
âœ… å›¾åƒå¤„ç†æˆåŠŸ: torch.Size([1, 3, 336, 336])
âœ… éŸ³é¢‘æ‰¹é‡å¤„ç†å®Œæˆ: torch.Size([1, 64, 1280])
ğŸ”„ æµ‹è¯•prompt 1: <image>
USER: What do you see in this image?
ASSIS...
  âœ… ç”ŸæˆæˆåŠŸ: 'ently placed in the center of the scene. The hands are positioned on either side...'
ğŸ”„ æµ‹è¯•prompt 2: <image>
USER: In this scene, someone says 'Hello t...
  âœ… ç”ŸæˆæˆåŠŸ: 'you?" or "What can I help you with?" depending on the context of the scene....'
ğŸ”„ æµ‹è¯•prompt 3: <image>
USER: Based on what you see and hear, what...
  âœ… ç”ŸæˆæˆåŠŸ: '. The woman is holding a cell phone and the other person is wearing a tie, which...'

==================================================
ğŸ” æµ‹è¯•8: è¾¹ç•Œæƒ…å†µå’Œé”™è¯¯å¤„ç†
==================================================
ğŸ”„ æµ‹è¯•ç©ºéŸ³é¢‘è¾“å…¥...
ğŸ“ å¯¹è¯é¢„æµ‹æ ¼å¼:
<image>
USER: Based on what you see and hear in this scene, what would Monica Geller say?
ASSISTANT:
ğŸ“ åŸå§‹æ–‡æœ¬é•¿åº¦èŒƒå›´: 30 - 30
âœ… æ‰¹é‡tokenizationå®Œæˆ: input_ids shape: torch.Size([1, 30])
âœ… æ‰€æœ‰æ ·æœ¬æ–‡æœ¬é•¿åº¦ç»Ÿä¸€ä¸º: 30
âœ… å›¾åƒå¤„ç†æˆåŠŸ: torch.Size([1, 3, 336, 336])
âœ… éŸ³é¢‘æ‰¹é‡å¤„ç†å®Œæˆ: torch.Size([1, 64, 1280])
ğŸ“ çº¯æ–‡æœ¬å†…å®¹: USER: What is artificial intelligence?
ASSISTANT:
ğŸ“ TokenèŒƒå›´: (1, 29973)
ğŸ” TokenèŒƒå›´æ£€æŸ¥: vocab_size=32000, input_ids range=(1, 29973)
ğŸ”¥ TriperModel.forward called:
  input_ids: torch.Size([1, 14])
  images: None
  audio_features: None
  past_key_values: 0 layers
  ğŸ“ çº¯æ–‡æœ¬è¾“å…¥ï¼Œç›´æ¥ä¼ é€’input_ids...
âœ… çº¯æ–‡æœ¬è¾“å…¥æµ‹è¯•é€šè¿‡
ğŸ”„ æµ‹è¯•å¼‚å¸¸éŸ³é¢‘ç‰¹å¾...
ğŸ”„ AudioProjector forward called with input shape: torch.Size([1, 32, 1280])
âœ… éŸ³é¢‘æŠ•å½±å™¨èƒ½å¤„ç†ä¸åŒé•¿åº¦: torch.Size([1, 32, 5120])
ğŸ”„ æµ‹è¯•é•¿è¾“å…¥...
âœ… é•¿è¾“å…¥æµ‹è¯•é€šè¿‡: torch.Size([1, 10])

================================================================================
ğŸ“‹ TRIPER MODEL TEST REPORT
================================================================================

ğŸ“Š æµ‹è¯•æ¦‚å†µ:
  æ€»æµ‹è¯•æ•°: 7
  é€šè¿‡æµ‹è¯•: 7
  å¤±è´¥æµ‹è¯•: 0
  é€šè¿‡ç‡: 100.0%

ğŸ“‹ è¯¦ç»†ç»“æœ:
  model_loading: âœ… PASSED
  data_loading: âœ… PASSED
  generation_capability: âœ… PASSED
  audio_projector: âœ… PASSED
  parameter_statistics: âœ… PASSED
  conversation_prediction: âœ… PASSED
  edge_cases: âœ… PASSED

ğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜è‡³: /home/wly/szl_all_code/triper-project/test_report.json