import torch
from dataclasses import dataclass
from typing import Dict, Sequence, Optional
import transformers
from PIL import Image
from triper.constants import DEFAULT_IMAGE_TOKEN
from llava.mm_utils import tokenizer_image_token_batch, process_images
from llava.constants import IMAGE_TOKEN_INDEX

@dataclass
class TriperDataCollator:
    
    tokenizer: transformers.PreTrainedTokenizer
    image_processor: Optional[object] = None
    audio_processor: Optional[object] = None
    model_cfg: Optional[object] = None
    max_length: int = 2048

    def _build_conversation_text(self, instance: Dict) -> str:
        """æ„å»ºå¯¹è¯é¢„æµ‹ä»»åŠ¡çš„LLaVAæ ¼å¼"""
        text_parts = []
        
        # æ·»åŠ å›¾åƒtoken
        if instance.get('has_image', False):
            text_parts.append(DEFAULT_IMAGE_TOKEN)
        
        conversation = instance.get('conversation', [])
        
        if conversation:
            # ğŸ¯ ç­–ç•¥1ï¼šç»™å‡ºéƒ¨åˆ†å¯¹è¯ï¼Œè®©æ¨¡å‹ç»­å†™
            dialogue_lines = []
            for i, turn in enumerate(conversation[:-1]):  # é™¤äº†æœ€åä¸€å¥
                speaker = turn.get('speaker', 'Person')
                text = turn.get('text', '')
                if text.strip():
                    dialogue_lines.append(f"{speaker}: {text}")
            
            # æœ€åä¸€å¥ä½œä¸ºç›®æ ‡é¢„æµ‹
            target_turn = conversation[-1]
            target_speaker = target_turn.get('speaker', 'Person')
            
            if dialogue_lines:
                context = "\n".join(dialogue_lines)
                text_parts.append(f"USER: Based on this conversation context and what you see/hear:\n{context}\n\nWhat would {target_speaker} say next?")
            else:
                text_parts.append(f"USER: Based on what you see and hear in this scene, what would {target_speaker} say?")
        else:
            text_parts.append("USER: Based on what you see and hear, what conversation would happen in this scene?")
        
        text_parts.append("ASSISTANT:")
        
        result = "\n".join(text_parts)
        print(f"ğŸ“ å¯¹è¯é¢„æµ‹æ ¼å¼:\n{result}")
        return result

    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:
        """å¤„ç†ä¸€ä¸ªbatchçš„æ•°æ®"""
        batch_size = len(instances)
        
        # 1. æ–‡æœ¬å¤„ç†ï¼ˆç°æœ‰é€»è¾‘ä¿æŒä¸å˜ï¼‰
        conversations = [self._build_conversation_text(inst) for inst in instances]
        
        from llava.mm_utils import tokenizer_image_token
        
        input_ids_list = []
        for conv in conversations:
            input_ids = tokenizer_image_token(
                conv,
                tokenizer=self.tokenizer,
                image_token_index=IMAGE_TOKEN_INDEX,
                return_tensors='pt'
            )
            if len(input_ids.shape) == 1:
                input_ids = input_ids.unsqueeze(0)
            input_ids_list.append(input_ids.squeeze(0))
        
        # ğŸ”§ ä¸¥æ ¼éªŒè¯å’Œpadding
        original_lengths = [ids.shape[0] for ids in input_ids_list]
        max_length = max(original_lengths)
        min_length = min(original_lengths)
        
        print(f"ğŸ“ åŸå§‹æ–‡æœ¬é•¿åº¦èŒƒå›´: {min_length} - {max_length}")
        if max_length != min_length:
            print(f"âš ï¸ æ–‡æœ¬é•¿åº¦ä¸ä¸€è‡´ï¼Œå°†paddingåˆ°: {max_length}")
        
        # ç¡®ä¿paddingåæ‰€æœ‰æ ·æœ¬é•¿åº¦å®Œå…¨ä¸€è‡´
        padded_input_ids = []
        attention_masks = []
        labels_list = []
        
        pad_token_id = self.tokenizer.pad_token_id
        if not isinstance(pad_token_id, int):
            raise ValueError("tokenizer.pad_token_id must be set to an integer for padding.")
        for i, input_ids in enumerate(input_ids_list):
            original_len = input_ids.shape[0]
            pad_length = max_length - original_len
            
            # åˆ›å»ºattention_mask
            attention_mask = torch.ones(original_len, dtype=torch.long)
            
            # åˆ›å»ºlabels
            labels = input_ids.clone()
            
            if pad_length > 0:
                # å³ä¾§padding
                padded_ids = torch.cat([
                    input_ids,
                    torch.full((pad_length,), pad_token_id, dtype=input_ids.dtype)
                ])
                attention_mask = torch.cat([
                    attention_mask,
                    torch.zeros(pad_length, dtype=attention_mask.dtype)
                ])
                labels = torch.cat([
                    labels,
                    torch.full((pad_length,), -100, dtype=labels.dtype)
                ])
            else:
                padded_ids = input_ids
            
            # ğŸ”§ æœ€ç»ˆéªŒè¯é•¿åº¦
            assert padded_ids.shape[0] == max_length, f"æ ·æœ¬ {i} paddingå¤±è´¥: {padded_ids.shape[0]} != {max_length}"
            assert attention_mask.shape[0] == max_length, f"æ ·æœ¬ {i} attention_maské•¿åº¦é”™è¯¯"
            assert labels.shape[0] == max_length, f"æ ·æœ¬ {i} labelsé•¿åº¦é”™è¯¯"
            
            padded_input_ids.append(padded_ids)
            attention_masks.append(attention_mask)
            labels_list.append(labels)
        
        batch = {
            "input_ids": torch.stack(padded_input_ids),
            "attention_mask": torch.stack(attention_masks),
            "labels": torch.stack(labels_list)
        }
        
        # ğŸ”§ æœ€ç»ˆæ‰¹é‡éªŒè¯
        assert batch["input_ids"].shape[0] == batch_size, "æ‰¹é‡å¤§å°ä¸åŒ¹é…"
        assert batch["input_ids"].shape[1] == max_length, f"æ–‡æœ¬åºåˆ—é•¿åº¦ä¸ä¸€è‡´: {batch['input_ids'].shape[1]} != {max_length}"
        assert batch["attention_mask"].shape == batch["input_ids"].shape, "attention_maskå½¢çŠ¶ä¸åŒ¹é…"
        assert batch["labels"].shape == batch["input_ids"].shape, "labelså½¢çŠ¶ä¸åŒ¹é…"
        
        print(f"âœ… æ‰¹é‡tokenizationå®Œæˆ: input_ids shape: {batch['input_ids'].shape}")
        print(f"âœ… æ‰€æœ‰æ ·æœ¬æ–‡æœ¬é•¿åº¦ç»Ÿä¸€ä¸º: {max_length}")
    
        # 2. ğŸ”§ å›ºå®šé•¿åº¦çš„å›¾åƒå¤„ç†
        image_feature_length = 576  # LLaVA-1.5 å›ºå®šé•¿åº¦
        if any('image_path' in inst for inst in instances):
            pil_images = []
            for inst in instances:
                if inst.get('has_image', False) and 'image_path' in inst:
                    try:
                        image = Image.open(inst['image_path']).convert('RGB')
                    except Exception as e:
                        print(f"âŒ åŠ è½½å›¾åƒå¤±è´¥ {inst.get('image_path', 'Unknown')}: {e}")
                        image = Image.new('RGB', (336, 336), (255, 255, 255))
                else:
                    # åˆ›å»ºç©ºç™½å›¾åƒ - ç¡®ä¿æ‰¹é‡ä¸­æ‰€æœ‰æ ·æœ¬éƒ½æœ‰å›¾åƒ
                    image = Image.new('RGB', (336, 336), (255, 255, 255))
                pil_images.append(image)
            
            if self.image_processor:
                try:
                    processed_images = process_images(
                        images=pil_images,
                        image_processor=self.image_processor,
                        model_cfg=self.model_cfg
                    )
                    
                    if isinstance(processed_images, list):
                        batch['images'] = torch.stack(processed_images)
                    else:
                        batch['images'] = processed_images
                    
                    print(f"âœ… å›¾åƒå¤„ç†æˆåŠŸ: {batch['images'].shape}")
                    
                    # ğŸ”§ éªŒè¯å›¾åƒæ‰¹é‡çš„ä¸€è‡´æ€§
                    assert batch['images'].shape[0] == batch_size, f"å›¾åƒæ‰¹é‡å¤§å°ä¸åŒ¹é…: {batch['images'].shape[0]} != {batch_size}"
                    
                except Exception as e:
                    print(f"âŒ å›¾åƒå¤„ç†å¤±è´¥: {e}")
                    # å¤‡ç”¨ï¼šåˆ›å»ºç©ºç™½å›¾åƒå¼ é‡
                    batch['images'] = torch.zeros(batch_size, 3, 336, 336)
            else:
                print("âš ï¸ æ²¡æœ‰å›¾åƒå¤„ç†å™¨ï¼Œåˆ›å»ºç©ºç™½å›¾åƒ")
                batch['images'] = torch.zeros(batch_size, 3, 336, 336)

        # 3. ğŸ”§ å›ºå®šé•¿åº¦çš„éŸ³é¢‘å¤„ç†
        audio_feature_length = 64  # å›ºå®šå‹ç¼©é•¿åº¦
        if any('audio_path' in inst for inst in instances):
            audio_features = []
            
            for inst in instances:
                if inst.get('has_audio', False) and 'audio_path' in inst:
                    audio_path = inst['audio_path']
                    
                    if self.audio_processor:
                        try:
                            audio_feat = self.audio_processor(audio_path)
                            
                            # å¤„ç†ä¸åŒçš„è¿”å›æ ¼å¼
                            if hasattr(audio_feat, 'data'):
                                audio_feat = audio_feat.data
                            elif isinstance(audio_feat, dict):
                                for key, value in audio_feat.items():
                                    if isinstance(value, torch.Tensor):
                                        audio_feat = value
                                        break
                            
                            # ç¡®ä¿ç»´åº¦æ­£ç¡®
                            if audio_feat.dim() == 3 and audio_feat.shape[0] == 1:
                                audio_feat = audio_feat.squeeze(0)
                            
                            # ğŸ”§ éªŒè¯éŸ³é¢‘ç‰¹å¾é•¿åº¦
                            expected_shape = (audio_feature_length, 1280)  # (64, 1280)
                            if audio_feat.shape != expected_shape:
                                print(f"âš ï¸ éŸ³é¢‘ç‰¹å¾å½¢çŠ¶ä¸åŒ¹é…: {audio_feat.shape} != {expected_shape}")
                                audio_feat = torch.zeros(*expected_shape)
                            
                        except Exception as e:
                            print(f"âŒ éŸ³é¢‘å¤„ç†å¤±è´¥ {audio_path}: {e}")
                            audio_feat = torch.zeros(audio_feature_length, 1280)
                    else:
                        audio_feat = torch.zeros(audio_feature_length, 1280)
                else:
                    # ç©ºç™½éŸ³é¢‘ç‰¹å¾
                    audio_feat = torch.zeros(audio_feature_length, 1280)
                    
                audio_features.append(audio_feat)
            
            # ğŸ”§ éªŒè¯æ‰€æœ‰éŸ³é¢‘ç‰¹å¾å½¢çŠ¶ä¸€è‡´
            shapes = [feat.shape for feat in audio_features]
            if len(set(shapes)) > 1:
                print(f"âŒ éŸ³é¢‘ç‰¹å¾å½¢çŠ¶ä¸ä¸€è‡´: {shapes}")
                # å¼ºåˆ¶ç»Ÿä¸€
                audio_features = [torch.zeros(audio_feature_length, 1280) for _ in range(batch_size)]
            
            batch['audio_features'] = torch.stack(audio_features)
            print(f"âœ… éŸ³é¢‘æ‰¹é‡å¤„ç†å®Œæˆ: {batch['audio_features'].shape}")
            
            # ğŸ”§ æœ€ç»ˆéªŒè¯
            expected_audio_shape = (batch_size, audio_feature_length, 1280)
            assert batch['audio_features'].shape == expected_audio_shape, \
                f"éŸ³é¢‘æ‰¹é‡å½¢çŠ¶é”™è¯¯: {batch['audio_features'].shape} != {expected_audio_shape}"

        # åœ¨TriperDataCollatorä¸­åªå¤„ç†åŸºæœ¬çš„æ–‡æœ¬å¯¹é½
        return {
            'input_ids': batch['input_ids'],  # [batch, text_len]
            'attention_mask': batch['attention_mask'],  # [batch, text_len] - åªå¯¹åº”æ–‡æœ¬
            'labels': batch['labels'],  # [batch, text_len]
            'images': batch.get('images'),  # å¯é€‰ï¼Œæ ¹æ®éœ€è¦è¿”å›
            'audio_features': batch.get('audio_features'),  # å¯é€‰ï¼Œæ ¹æ®éœ€è¦è¿”å›
            # ä¸å†é¢„è®¡ç®—å¤šæ¨¡æ€æ€»é•¿åº¦
        } # type: ignore