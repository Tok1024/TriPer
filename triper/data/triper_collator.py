import torch
from dataclasses import dataclass
from typing import Dict, Sequence, Optional
import transformers
from PIL import Image
from triper.constants import DEFAULT_IMAGE_TOKEN
from llava.mm_utils import tokenizer_image_token_batch, process_images
from llava.constants import IMAGE_TOKEN_INDEX

@dataclass
class TriperDataCollator:
    
    tokenizer: transformers.PreTrainedTokenizer
    image_processor: Optional[object] = None
    audio_processor: Optional[object] = None
    model_cfg: Optional[object] = None  # ğŸ”§ æ–°å¢ï¼šæ¥æ”¶æ¨¡å‹é…ç½®
    max_length: int = 2048

    def _build_conversation_text(self, instance: Dict) -> str:
        """æ„å»ºåŒ…å«å›¾åƒtokençš„å¯¹è¯æ–‡æœ¬"""
        text_parts = []
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¦‚æœæœ‰å›¾åƒï¼Œåœ¨å¼€å¤´æ’å…¥å›¾åƒtoken
        if instance.get('has_image', False):
            text_parts.append(DEFAULT_IMAGE_TOKEN)
        
        conversation = instance.get('conversation', [])
        for turn in conversation:
            speaker = turn.get('speaker', 'Unknown')
            text = turn.get('text', '')
            emotion = turn.get('emotion', 'neutral')
            if text:
                text_parts.append(f"{speaker} ({emotion}): {text}")
        
        if len(text_parts) <= 1:  # åªæœ‰å›¾åƒtokenæˆ–ä¸ºç©º
            text_parts.append("No conversation available.")
            
        result = "\n".join(text_parts)
        print(f"ğŸ“ æ„å»ºçš„å¯¹è¯æ–‡æœ¬ï¼ˆåŒ…å«å›¾åƒtokenï¼‰: {result[:100]}...")
        return result

    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:
        """å¤„ç†ä¸€ä¸ªbatchçš„æ•°æ®"""
        batch_size = len(instances)
        
        # 1. å¤„ç†å¯¹è¯æ–‡æœ¬
        conversations = [self._build_conversation_text(inst) for inst in instances]
        
        from llava.mm_utils import tokenizer_image_token
        
        input_ids_list = []
        for conv in conversations:
            input_ids = tokenizer_image_token(
                conv,
                tokenizer=self.tokenizer,
                image_token_index=IMAGE_TOKEN_INDEX,
                return_tensors='pt'
            )
            if len(input_ids.shape) == 1:
                input_ids = input_ids.unsqueeze(0)
            input_ids_list.append(input_ids.squeeze(0))
        
        # æ‰‹åŠ¨padding
        max_length = max(ids.shape[0] for ids in input_ids_list)
        padded_input_ids = []
        attention_masks = []
        labels_list = []
        
        for input_ids in input_ids_list:
            pad_length = max_length - input_ids.shape[0]
            
            # åˆ›å»ºattention_mask
            attention_mask = torch.ones(input_ids.shape[0], dtype=torch.long)
            
            # åˆ›å»ºlabels (ç®€å•ç‰ˆæœ¬ï¼šæ‰€æœ‰tokenéƒ½ä½œä¸ºç›®æ ‡)
            labels = input_ids.clone()
            
            if pad_length > 0:
                # å³ä¾§padding
                input_ids = torch.cat([
                    input_ids,
                    torch.full((pad_length,), self.tokenizer.pad_token_id, dtype=input_ids.dtype)
                ])
                attention_mask = torch.cat([
                    attention_mask,
                    torch.zeros(pad_length, dtype=attention_mask.dtype)
                ])
                labels = torch.cat([
                    labels,
                    torch.full((pad_length,), -100, dtype=labels.dtype)  # paddingéƒ¨åˆ†å¿½ç•¥
                ])
            
            padded_input_ids.append(input_ids)
            attention_masks.append(attention_mask)
            labels_list.append(labels)
        
        batch = {
            "input_ids": torch.stack(padded_input_ids),
            "attention_mask": torch.stack(attention_masks),
            "labels": torch.stack(labels_list)
        }
        
        print(f"ğŸ“ æ‰¹é‡tokenizationå®Œæˆ: input_ids shape: {batch['input_ids'].shape}")
    
        # 2. å¤„ç†å›¾åƒæ•°æ® - ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„model_cfg
        image_feature_length = 0
        if 'image_path' in instances[0]:
            # å…ˆæ”¶é›†æ‰€æœ‰å›¾åƒ
            pil_images = []
            for inst in instances:
                if inst.get('has_image', False):
                    image_path = inst['image_path']
                    image = Image.open(image_path).convert('RGB')
                else:
                    # åˆ›å»ºç©ºç™½å›¾åƒ
                    image = Image.new('RGB', (336, 336), (255, 255, 255))
                pil_images.append(image)
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„model_cfg
            if self.image_processor:
                try:
                    # è°ƒç”¨ process_images å‡½æ•°ï¼Œä½¿ç”¨ä¼ å…¥çš„model_cfg
                    processed_images = process_images(
                        images=pil_images,
                        image_processor=self.image_processor,
                        model_cfg=self.model_cfg  # ğŸ”§ ä½¿ç”¨æ­£ç¡®çš„model_cfg
                    )
                    
                    # process_images è¿”å›çš„å¯èƒ½æ˜¯ tensor æˆ– tensor åˆ—è¡¨
                    if isinstance(processed_images, list):
                        batch['images'] = torch.stack(processed_images)
                    else:
                        batch['images'] = processed_images
                    
                    image_feature_length = 576  # LLaVA-1.5 çš„å›¾åƒç‰¹å¾é•¿åº¦
                    print(f"ğŸ–¼ï¸ LLaVA process_images å¤„ç†æˆåŠŸï¼Œshape: {batch['images'].shape}")
                    
                except Exception as e:
                    print(f"âŒ LLaVA process_images å¤±è´¥: {e}")
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥ä½¿ç”¨ image_processor
                    processed = self.image_processor(pil_images, return_tensors="pt")
                    batch['images'] = processed['pixel_values']
                    image_feature_length = 576
                    print(f"ğŸ–¼ï¸ å¤‡ç”¨å›¾åƒå¤„ç†æˆåŠŸï¼Œshape: {batch['images'].shape}")

        # 3. å¤„ç†éŸ³é¢‘æ•°æ®
        audio_feature_length = 0
        if 'audio_path' in instances[0]:
            audio_features = []
            for inst in instances:
                if inst.get('has_audio', False):
                    audio_path = inst['audio_path']
                    
                    if self.audio_processor:
                        try:
                            audio_feat = self.audio_processor(audio_path)
                            
                            if hasattr(audio_feat, 'data'):
                                audio_feat = audio_feat.data
                            elif isinstance(audio_feat, dict):
                                for key, value in audio_feat.items():
                                    if isinstance(value, torch.Tensor):
                                        audio_feat = value
                                        break
                            
                            if audio_feat.dim() == 3 and audio_feat.shape[0] == 1:
                                audio_feat = audio_feat.squeeze(0)
                            
                            audio_feature_length = audio_feat.shape[0]
                            
                        except Exception as e:
                            print(f"âŒ éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
                            audio_feat = torch.zeros(64, 1280)
                            audio_feature_length = 64
                    else:
                        audio_feat = torch.zeros(64, 1280)
                        audio_feature_length = 64
                else:
                    audio_feat = torch.zeros(64, 1280)
                    audio_feature_length = 64
                    
                audio_features.append(audio_feat)
            
            if audio_features:
                batch['audio_features'] = torch.stack(audio_features)
                print(f"ğŸµ Final audio batch shape: {batch['audio_features'].shape}")

        # 4. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        text_length = batch["input_ids"].shape[1]
        total_length = text_length + image_feature_length + audio_feature_length
        
        print(f"ğŸ“ åºåˆ—é•¿åº¦: æ–‡æœ¬={text_length}, å›¾åƒ={image_feature_length}, éŸ³é¢‘={audio_feature_length}, æ€»è®¡={total_length}")
        print(f"ğŸ“ Labels shape: {batch['labels'].shape}")

        return batch