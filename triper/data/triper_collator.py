import torch
from dataclasses import dataclass
from typing import Dict, Sequence, Optional
import transformers
from PIL import Image

@dataclass
class TriperDataCollator:
    """Triperå¤šæ¨¡æ€æ•°æ®æ•´ç†å™¨ - åœ¨è¿™é‡Œè¿›è¡Œå®é™…çš„æ•°æ®å¤„ç†"""
    
    tokenizer: transformers.PreTrainedTokenizer
    image_processor: Optional[object] = None
    audio_processor: Optional[object] = None
    max_length: int = 2048

    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:
        """å¤„ç†ä¸€ä¸ªbatchçš„æ•°æ®"""
        batch_size = len(instances)
        
        # 1. å¤„ç†å¯¹è¯æ–‡æœ¬
        conversations = [self._build_conversation_text(inst) for inst in instances]
        
        # Tokenizeå¯¹è¯
        tokenized = self.tokenizer(
            conversations,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=self.max_length
        )
        
        batch = {
            "input_ids": tokenized.input_ids,
            "attention_mask": tokenized.attention_mask,
            "labels": tokenized.input_ids.clone(),  # ç®€å•æƒ…å†µä¸‹
        }

        # 2. å¤„ç†å›¾åƒæ•°æ® - ä¿®æ­£å›¾åƒå°ºå¯¸ï¼
        if 'image_path' in instances[0]:
            images = []
            for inst in instances:
                if inst.get('has_image', False):
                    # åŠ¨æ€åŠ è½½å›¾åƒ
                    from triper.data.triper_dataset import TriperDataset
                    temp_dataset = TriperDataset.__new__(TriperDataset)
                    temp_dataset.default_image_size = (336, 336)  # ä¿®æ”¹ä¸ºæ¨¡å‹æœŸæœ›çš„å°ºå¯¸
                    image = temp_dataset._load_image_raw(inst['image_path'])
                else:
                    image = Image.new('RGB', (336, 336), (255, 255, 255))  # ä¿®æ”¹ä¸º336x336
                images.append(image)
            
            # åº”ç”¨å›¾åƒå¤„ç†å™¨
            if self.image_processor:
                try:
                    # æ‰¹é‡å¤„ç†å›¾åƒï¼Œè®©å¤„ç†å™¨è‡ªå·±å†³å®šå°ºå¯¸
                    processed = self.image_processor(images, return_tensors="pt")
                    
                    # å¤„ç†ä¸åŒç±»å‹çš„è¿”å›å€¼
                    if isinstance(processed, dict):
                        if 'pixel_values' in processed:
                            batch['images'] = processed['pixel_values']
                            print(f"ğŸ–¼ï¸ Image batch shape: {batch['images'].shape}")
                        elif 'input_ids' in processed:
                            batch['images'] = processed['input_ids']
                        else:
                            # å¦‚æœæœ‰å…¶ä»–é”®ï¼Œå–ç¬¬ä¸€ä¸ªå¼ é‡
                            for key, value in processed.items():
                                if isinstance(value, torch.Tensor):
                                    batch['images'] = value
                                    print(f"ğŸ–¼ï¸ Image batch shape: {batch['images'].shape}")
                                    break
                    else:
                        # å¦‚æœç›´æ¥è¿”å›å¼ é‡
                        batch['images'] = processed
                        print(f"ğŸ–¼ï¸ Image batch shape: {batch['images'].shape}")
                        
                except Exception as e:
                    print(f"âŒ å›¾åƒå¤„ç†å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    # ä½¿ç”¨é»˜è®¤å›¾åƒå¼ é‡ï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„å°ºå¯¸
                    import torchvision.transforms as transforms
                    transform = transforms.Compose([
                        transforms.Resize((336, 336)),  # ä¿®æ”¹ä¸º336x336
                        transforms.ToTensor(),
                    ])
                    image_tensors = [transform(img) for img in images]
                    batch['images'] = torch.stack(image_tensors)
            else:
                # å¦‚æœæ²¡æœ‰å¤„ç†å™¨ï¼Œæ‰‹åŠ¨è½¬æ¢ä¸ºå¼ é‡ï¼Œä½¿ç”¨æ­£ç¡®çš„å°ºå¯¸
                import torchvision.transforms as transforms
                transform = transforms.Compose([
                    transforms.Resize((336, 336)),  # ä¿®æ”¹ä¸º336x336
                    transforms.ToTensor(),
                ])
                image_tensors = [transform(img) for img in images]
                batch['images'] = torch.stack(image_tensors)

        # 3. å¤„ç†éŸ³é¢‘æ•°æ®
        if 'audio_path' in instances[0]:
            audio_features = []
            for inst in instances:
                if inst.get('has_audio', False):
                    audio_path = inst['audio_path']
                    
                    if self.audio_processor:
                        try:
                            # ç›´æ¥ä¼ å…¥éŸ³é¢‘è·¯å¾„ï¼Œè®©éŸ³é¢‘ç¼–ç å™¨å¤„ç†
                            audio_feat = self.audio_processor(audio_path)
                            
                            # ç¡®ä¿è¿”å›çš„æ˜¯çº¯å¼ é‡
                            if hasattr(audio_feat, 'data'):
                                audio_feat = audio_feat.data
                            elif isinstance(audio_feat, dict):
                                # å¦‚æœæ˜¯å­—å…¸ï¼Œå–ç¬¬ä¸€ä¸ªå¼ é‡
                                for key, value in audio_feat.items():
                                    if isinstance(value, torch.Tensor):
                                        audio_feat = value
                                        break
                            
                            # å¦‚æœè¿”å›çš„æ˜¯batchæ ¼å¼ï¼Œå–ç¬¬ä¸€ä¸ª
                            if audio_feat.dim() == 3 and audio_feat.shape[0] == 1:
                                audio_feat = audio_feat.squeeze(0)  # [seq_len, hidden_dim]
                            
                            print(f"ğŸµ Audio feature shape: {audio_feat.shape}")
                            
                        except Exception as e:
                            print(f"âŒ éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
                            import traceback
                            traceback.print_exc()
                            # ä½¿ç”¨é»˜è®¤ç»´åº¦çš„é›¶å¼ é‡ä½œä¸ºå¤‡ç”¨
                            audio_feat = torch.zeros(64, 1280)  # [seq_len, hidden_dim]
                    else:
                        # å¦‚æœæ²¡æœ‰éŸ³é¢‘å¤„ç†å™¨ï¼ŒåŠ è½½åŸå§‹éŸ³é¢‘
                        from triper.data.triper_dataset import TriperDataset
                        temp_dataset = TriperDataset.__new__(TriperDataset)
                        temp_dataset.max_audio_length = 16000 * 10
                        waveform = temp_dataset._load_audio_raw(audio_path)
                        audio_feat = waveform.mean(dim=0) if waveform.dim() > 1 else waveform
                else:
                    # æ²¡æœ‰éŸ³é¢‘æ–‡ä»¶æ—¶çš„é»˜è®¤å€¼
                    audio_feat = torch.zeros(64, 1280)  # [seq_len, hidden_dim]
                    
                audio_features.append(audio_feat)
            
            # Stackæ‰€æœ‰éŸ³é¢‘ç‰¹å¾
            if audio_features:
                batch['audio_features'] = torch.stack(audio_features)
                print(f"ğŸµ Final audio batch shape: {batch['audio_features'].shape}")

        return batch

    def _build_conversation_text(self, instance: Dict) -> str:
        """æ„å»ºç”¨äºè®­ç»ƒçš„å¯¹è¯æ–‡æœ¬"""
        text_parts = []
        
        # æ·»åŠ åœºæ™¯æè¿°
        scene_desc = instance.get('scene_description', '')
        if scene_desc:
            text_parts.append(f"Scene: {scene_desc}")
        
        # æ·»åŠ å¯¹è¯å†…å®¹
        conversation = instance.get('conversation', [])
        for turn in conversation:
            speaker = turn.get('speaker', 'Unknown')
            text = turn.get('text', '')
            emotion = turn.get('emotion', 'neutral')
            if text:  # åªæ·»åŠ éç©ºå¯¹è¯
                text_parts.append(f"{speaker} ({emotion}): {text}")
        
        # å¦‚æœæ²¡æœ‰ä»»ä½•å†…å®¹ï¼Œè¿”å›é»˜è®¤æ–‡æœ¬
        if not text_parts:
            text_parts.append("No conversation available.")
            
        return "\n".join(text_parts)