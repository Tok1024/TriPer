import torch
import torch.nn as nn
import torchaudio
from transformers import WhisperFeatureExtractor
from speech_tokenizer.modeling_whisper import WhisperVQEncoder as GLMWhisperVQEncoder

import torch.nn as nn

class AudioCompressor(nn.Module):
    """å¯å­¦ä¹ çš„éŸ³é¢‘ç‰¹å¾å‹ç¼©å™¨"""
    
    def __init__(self, input_dim, output_seq_len, hidden_dim=None):
        super().__init__()
        self.output_seq_len = output_seq_len
        hidden_dim = hidden_dim or input_dim
        
        # æ³¨æ„åŠ›æ± åŒ–
        self.attention_pool = nn.MultiheadAttention(
            embed_dim=input_dim,
            num_heads=8,
            batch_first=True
        )
        
        # å¯å­¦ä¹ çš„æŸ¥è¯¢å‘é‡
        self.queries = nn.Parameter(torch.randn(output_seq_len, input_dim))
        
    def forward(self, x):
        """
        Args:
            x: [batch, seq_len, input_dim]
        Returns:
            compressed: [batch, output_seq_len, input_dim]
        """
        batch_size = x.shape[0]
        
        # æ‰©å±•æŸ¥è¯¢å‘é‡åˆ°batchç»´åº¦
        queries = self.queries.unsqueeze(0).expand(batch_size, -1, -1)  # [batch, output_seq_len, input_dim]
        compressed, _ = self.attention_pool(queries, x, x)
        
        return compressed

class WhisperVQEncoder(nn.Module):
    """Triperçš„éŸ³é¢‘ç¼–ç å™¨ - åŒ…è£…GLMçš„WhisperVQæ¨¡å‹"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # ğŸ”§ ä»é…ç½®è·å–å‚æ•°
        self.hidden_size = getattr(config, 'audio_hidden_size', 768)
        self.model_path = getattr(config, 'audio_model_path', "/sda1/glm-4-voice-tokenizer")
        if getattr(config, 'use_audio_compressor', True):
            self.target_seq_len = getattr(config, 'audio_target_seq_len', 64)
            self.audio_compressor = AudioCompressor(
                input_dim=self.hidden_size,
                output_seq_len=self.target_seq_len
            )

        # ğŸµ åŠ è½½Whisperæ¨¡å‹
        try:
            self.whisper_model = GLMWhisperVQEncoder.from_pretrained(self.model_path).eval()
            # ğŸ”§ æ˜ç¡®å†»ç»“æ‰€æœ‰å‚æ•°
            for param in self.whisper_model.parameters():
                param.requires_grad = False
            
            self.feature_extractor = WhisperFeatureExtractor.from_pretrained(self.model_path)
            
            # è·å–å®é™…çš„éšè—ç»´åº¦
            self.actual_hidden_size = self.whisper_model.config.d_model
            
            print(f"âœ… WhisperVQEncoder loaded from {self.model_path}")
            
        except Exception as e:
            print(f"âŒ Failed to load Whisper model: {e}")
            self.whisper_model = None
            self.feature_extractor = None
            self.actual_hidden_size = self.hidden_size
    
    def preprocess_audio(self, audio_path_or_tensor):
        """é¢„å¤„ç†éŸ³é¢‘æ•°æ®"""
        if isinstance(audio_path_or_tensor, str):
            # ä»æ–‡ä»¶è·¯å¾„åŠ è½½
            audio, sample_rate = torchaudio.load(audio_path_or_tensor)
        else:
            # ç›´æ¥ä½¿ç”¨tensor
            audio = audio_path_or_tensor
            sample_rate = 16000  # å‡è®¾å·²ç»æ˜¯16kHz
        
        # é‡é‡‡æ ·åˆ°16kHz
        if sample_rate != 16000:
            resampler = torchaudio.transforms.Resample(
                orig_freq=sample_rate, new_freq=16000
            )
            audio = resampler(audio)
        
        # è½¬ä¸ºå•å£°é“
        if audio.shape[0] > 1:
            audio = audio.mean(dim=0, keepdim=True)
        
        return audio[0].numpy()  # è¿™é‡Œä»ç„¶è¿”å›numpyï¼Œå› ä¸ºfeature_extractoréœ€è¦
    
    def extract_features_from_audio(self, audio_path_or_tensor):
        """ä»éŸ³é¢‘æå–ç‰¹å¾"""
        if self.whisper_model is None or self.feature_extractor is None:
            raise ValueError("Whisper model not loaded")
        
        # é¢„å¤„ç†éŸ³é¢‘
        audio_array = self.preprocess_audio(audio_path_or_tensor)
        
        # æå–melç‰¹å¾
        features = self.feature_extractor(
            audio_array, 
            sampling_rate=16000,
            return_attention_mask=True, 
            return_tensors="pt"
        )
        
        # ğŸ”§ ç¡®ä¿ç‰¹å¾åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        device = next(self.whisper_model.parameters()).device
        features = features.to(device)
        
        # é€šè¿‡Whisperç¼–ç å™¨
        with torch.no_grad():
            outputs = self.whisper_model(
                input_features=features.input_features,
                attention_mask=features.attention_mask,
                quantized_token_ids=None  # è·å–è¿ç»­ç‰¹å¾
            )
            continuous_features = outputs.last_hidden_state

            # ç¡®ä¿è¾“å‡ºåœ¨ä¸æ¨¡å‹ç›¸åŒçš„è®¾å¤‡ä¸Š
            if continuous_features.device != device:
                continuous_features = continuous_features.to(device)
        
        # å¦‚æœä½¿ç”¨éŸ³é¢‘å‹ç¼©å™¨ï¼Œåˆ™åº”ç”¨å®ƒ
        if hasattr(self, 'audio_compressor'):
            continuous_features = self.audio_compressor(continuous_features)
        
        return continuous_features
    
    def forward(self, audio_input):
        """
        å‰å‘ä¼ æ’­
        Args:
            audio_input: å¯ä»¥æ˜¯:
                - éŸ³é¢‘æ–‡ä»¶è·¯å¾„ (str)
                - é¢„å¤„ç†çš„éŸ³é¢‘ç‰¹å¾ (torch.Tensor)
                - å·²ç»æå–çš„ç‰¹å¾ (torch.Tensor, shape=[B, T, D])
        """
        if isinstance(audio_input, str):
            # ä»éŸ³é¢‘æ–‡ä»¶æå–ç‰¹å¾
            features = self.extract_features_from_audio(audio_input)
            return features
        elif audio_input.dim() == 2:
            # å‡è®¾æ˜¯é¢„å¤„ç†çš„éŸ³é¢‘ä¿¡å·ï¼Œéœ€è¦æå–ç‰¹å¾
            features = self.extract_features_from_audio(audio_input)
            return features
        elif audio_input.dim() == 3:
            # å‡è®¾å·²ç»æ˜¯ç‰¹å¾ï¼Œç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            device = next(self.parameters()).device if list(self.parameters()) else 'cpu'
            if isinstance(device, torch.device):
                target_device = device
            else:
                target_device = next(self.whisper_model.parameters()).device
            
            if audio_input.device != target_device:
                audio_input = audio_input.to(target_device)
            

            return audio_input
        else:
            raise ValueError(f"Unsupported audio input shape: {audio_input.shape}")

    
# æ„å»ºå‡½æ•°ä¿æŒä¸å˜
def build_audio_encoder(config):
    """æ„å»ºéŸ³é¢‘ç¼–ç å™¨"""
    try:
        mm_audio_encoder = getattr(config, 'mm_audio_encoder', 'whisper_vq')
        
        
        if mm_audio_encoder == 'whisper_vq':
            encoder = WhisperVQEncoder(config)
            return encoder
        else:
            raise ValueError(f"Unknown audio encoder: {mm_audio_encoder}")
            
    except Exception as e:
        print(f"âŒ Failed to build audio encoder: {e}")
        import traceback
        traceback.print_exc()
        return None
