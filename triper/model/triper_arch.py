import torch
import torch.nn as nn
from typing import Optional, Dict, Any, List, Union, Tuple
from transformers.modeling_utils import PreTrainedModel
from transformers.modeling_outputs import CausalLMOutputWithPast
from triper.constants import AUDIO_TOKEN_INDEX, IMAGE_TOKEN_INDEX
from .audio import build_audio_projector
from triper.configs.triper_config import TriperConfig


class TriperModel(PreTrainedModel):
    """
    Triperå¤šæ¨¡æ€æ¨¡å‹ 
    åŒ…å«LLaVAä½œä¸ºè§†è§‰-è¯­è¨€å­æ¨¡å—ï¼Œå¹¶æ·»åŠ éŸ³é¢‘æ”¯æŒ
    """
    config_class = TriperConfig
    
    def __init__(self, config: TriperConfig):
        super().__init__(config)
        self.config = config
        
        # æ ¸å¿ƒç»„ä»¶
        self.llava_model = None
        self.audio_projector = None  # åªæœ‰å¯è®­ç»ƒçš„æŠ•å½±å™¨
        
        # ç§æœ‰å¼•ç”¨çš„å¤–éƒ¨ç»„ä»¶ï¼ˆä¸å‚ä¸è®­ç»ƒï¼‰
        self._tokenizer = None
        self._image_processor = None
        self._context_len = None
        self._audio_encoder = None  # ç§æœ‰å¼•ç”¨ï¼Œä¸å‚ä¸å‚æ•°ç»Ÿè®¡
        
        # æ„å»ºéŸ³é¢‘æ¨¡å—
        self._build_audio_modules()
        
        print(f"âœ… TriperModel initialized with config: {config.model_type}")
        
    
    def _build_audio_modules(self):
        """æ„å»ºéŸ³é¢‘æ¨¡å— - åªæ„å»ºå¯è®­ç»ƒçš„æŠ•å½±å™¨"""
        if hasattr(self.config, "mm_audio_encoder") and self.config.mm_audio_encoder:
            try:
                print(f"ğŸ”„ Building audio projector...")
                self.audio_projector = build_audio_projector(self.config)
                print(f"âœ… Audio projector built: {type(self.audio_projector).__name__}")
                    
            except Exception as e:
                print(f"âŒ Audio projector build failed: {e}")
                self.audio_projector = None
                raise
        else:
            print("âš ï¸ No audio encoder specified in config")
    
    def attach_llava_model(self, llava_model):
        """é™„åŠ LLaVAæ¨¡å‹ä½œä¸ºå­æ¨¡å—"""
        self.llava_model = llava_model
        print(f"âœ… LLaVA model attached: {type(llava_model).__name__}")
        
        if hasattr(self.config, 'freeze_llava') and self.config.freeze_llava:
            self._freeze_llava_parameters()
    
    def set_audio_encoder(self, audio_encoder):
        """è®¾ç½®éŸ³é¢‘ç¼–ç å™¨ï¼ˆå¤–éƒ¨å¼•ç”¨ï¼Œä¸å‚ä¸è®­ç»ƒï¼‰"""
        self._audio_encoder = audio_encoder
        print(f"ğŸµ Audio encoder attached: {type(audio_encoder).__name__}")
    
    def set_components(self, tokenizer, image_processor, context_len):
        """ä¸€æ¬¡æ€§è®¾ç½®æ‰€æœ‰å¤–éƒ¨ç»„ä»¶"""
        self._tokenizer = tokenizer
        self._image_processor = image_processor
        self._context_len = context_len
        print(f"ğŸ“¦ Components set: tokenizer({type(tokenizer).__name__}), "
              f"processor({type(image_processor).__name__}), context_len({context_len})")
    
    def _freeze_llava_parameters(self):
        """å†»ç»“LLaVAæ¨¡å‹å‚æ•°"""
        if self.llava_model is not None:
            for param in self.llava_model.parameters():
                param.requires_grad = False
            print("ğŸ”’ LLaVA model parameters frozen")
    
    # ğŸ¯ ç»Ÿä¸€çš„ç»„ä»¶è®¿é—®å™¨
    @property
    def tokenizer(self):
        """è·å–åˆ†è¯å™¨"""
        return self._tokenizer
    
    @property
    def image_processor(self):
        """è·å–å›¾åƒå¤„ç†å™¨"""
        return self._image_processor
    
    @property
    def context_len(self):
        """è·å–ä¸Šä¸‹æ–‡é•¿åº¦"""
        return self._context_len or 2048
    
    @property
    def audio_encoder(self):
        """è·å–éŸ³é¢‘ç¼–ç å™¨ï¼ˆå¤–éƒ¨å¼•ç”¨ï¼Œä¸å‚ä¸è®­ç»ƒï¼‰"""
        return self._audio_encoder
    
    # ğŸ” LLaVAå­ç»„ä»¶è®¿é—®
    def get_model(self):
        """è·å–åŸºç¡€è¯­è¨€æ¨¡å‹"""
        if self.llava_model is not None:
            return getattr(self.llava_model, 'model', None) or getattr(self.llava_model, 'get_model', lambda: None)()
        return None
    
    def get_vision_tower(self):
        """è·å–è§†è§‰å¡”"""
        return getattr(self.llava_model, 'get_vision_tower', lambda: None)() if self.llava_model else None
    
    def get_vision_projector(self):
        """è·å–è§†è§‰æŠ•å½±å™¨"""
        if not self.llava_model:
            return None
        # å°è¯•å¤šç§å¯èƒ½çš„è®¿é—®è·¯å¾„
        for attr_path in ['mm_projector', 'model.mm_projector']:
            obj = self.llava_model
            for attr in attr_path.split('.'):
                obj = getattr(obj, attr, None)
                if obj is None:
                    break
            if obj is not None:
                return obj
        return None
    
    
    # ğŸ“Š å‚æ•°ç»Ÿè®¡ï¼ˆç²¾ç®€ç‰ˆï¼‰
    def get_parameter_stats(self) -> Dict[str, Any]:
        """è·å–å‚æ•°ç»Ÿè®¡ä¿¡æ¯ - åªç»Ÿè®¡å¯è®­ç»ƒç»„ä»¶"""
        components = {
            'llava': self.llava_model,
            'audio_projector': self.audio_projector,
            # æ³¨æ„ï¼šä¸åŒ…å« audio_encoderï¼Œå› ä¸ºå®ƒä¸å‚ä¸è®­ç»ƒ
        }
        
        stats = {'total_params': 0, 'trainable_params': 0, 'components': {}}
        
        for name, component in components.items():
            comp_stats = {'total': 0, 'trainable': 0}
            if component is not None:
                for param in component.parameters():
                    comp_stats['total'] += param.numel()
                    if param.requires_grad:
                        comp_stats['trainable'] += param.numel()
            
            stats['components'][name] = comp_stats
            stats['total_params'] += comp_stats['total']
            stats['trainable_params'] += comp_stats['trainable']
        
        return stats
    
    def print_model_summary(self):
        """æ‰“å°æ¨¡å‹æ‘˜è¦"""
        print("\nğŸ—ï¸  Triper Model Summary")
        print("=" * 60)
        
        # ç»„ä»¶çŠ¶æ€
        print("ğŸ“¦ Components:")
        components_status = [
            ("ğŸ¦™ LLaVA", self.llava_model),
            ("ğŸµ Audio Encoder", self._audio_encoder, "ğŸ”’ External (Frozen)"),
            ("ğŸ”— Audio Projector", self.audio_projector, "ğŸ”“ Trainable"),
            ("ğŸ“ Tokenizer", self._tokenizer, "ğŸ”’ External"),
            ("ğŸ–¼ï¸ Image Processor", self._image_processor, "ğŸ”’ External")
        ]
        
        for item in components_status:
            if len(item) == 3:
                name, component, note = item
                status = "âœ…" if component is not None else "âŒ"
                type_info = f"({type(component).__name__}) {note}" if component else ""
            else:
                name, component = item
                status = "âœ…" if component is not None else "âŒ"
                type_info = f"({type(component).__name__})" if component else ""
            print(f"  {name}: {status} {type_info}")
        
        # å‚æ•°ç»Ÿè®¡ï¼ˆåªæ˜¾ç¤ºå¯è®­ç»ƒå‚æ•°ï¼‰
        stats = self.get_parameter_stats()
        print(f"\nğŸ“Š Trainable Parameters:")
        print(f"  Total: {stats['total_params']:,}")
        print(f"  Trainable: {stats['trainable_params']:,} "
              f"({stats['trainable_params']/max(stats['total_params'], 1)*100:.1f}%)")
        
        # ç»„ä»¶è¯¦ç»†ç»Ÿè®¡
        for name, comp_stats in stats['components'].items():
            if comp_stats['total'] > 0:
                ratio = comp_stats['trainable'] / comp_stats['total'] * 100
                status = "ğŸ”“" if comp_stats['trainable'] > 0 else "ğŸ”’"
                print(f"    {name}: {comp_stats['total']:,} ({ratio:.1f}% trainable) {status}")
        
        print("=" * 60)
    
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.Tensor]] = None, 
        inputs_embeds: Optional[torch.Tensor] = None,         
        labels: Optional[torch.LongTensor] = None,
        images: Optional[torch.Tensor] = None,                
        image_sizes: Optional[List[List[int]]] = None,
        audio_features: Optional[torch.Tensor] = None,        
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, CausalLMOutputWithPast]:
        """
        Triperæ¨¡å‹çš„å‰å‘ä¼ æ’­ï¼Œæ‰©å±•LLaVAæ”¯æŒéŸ³é¢‘
        """
        if not self.is_ready():
            raise RuntimeError("æ¨¡å‹ç»„ä»¶å°šæœªå®Œå…¨é…ç½®")

        if inputs_embeds is None:
            # 1. å…ˆå¤„ç†å›¾åƒï¼ˆå¤ç”¨LLaVAçš„é€»è¾‘ï¼‰
            if self.llava_model is None:
                raise RuntimeError("LLaVA model is not attached")
            
            multimodal_result = self.llava_model.prepare_inputs_labels_for_multimodal(
                input_ids,
                position_ids,
                attention_mask,
                past_key_values,
                labels,
                images,
                image_sizes
            )
            
            # æ˜ç¡®ç±»å‹è½¬æ¢
            input_ids = multimodal_result[0]
            position_ids = multimodal_result[1] 
            attention_mask = multimodal_result[2]
            past_key_values = multimodal_result[3]
            inputs_embeds = multimodal_result[4]
            labels = multimodal_result[5]
            
        # 2. å¤„ç†éŸ³é¢‘ç‰¹å¾ï¼ˆæ–°å¢é€»è¾‘ï¼‰
        if audio_features is not None:
            inputs_embeds = self._insert_audio_features(
                inputs_embeds, input_ids, audio_features
            )
        
        # 3. è°ƒç”¨åŸºç¡€LLM
        if self.llava_model is None:
            raise RuntimeError("LLaVA model is not attached")
        
        return self.llava_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            labels=labels,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict
        )

    def _insert_audio_features(
        self, 
        inputs_embeds: Optional[torch.Tensor],  
        input_ids: Optional[torch.LongTensor], 
        audio_features: torch.Tensor          
    ) -> torch.Tensor:                        
        """æ’å…¥éŸ³é¢‘ç‰¹å¾åˆ°åµŒå…¥åºåˆ—ä¸­"""

        # å¤„ç†éŸ³é¢‘ç‰¹å¾
        if self._audio_encoder is None:
            raise RuntimeError("Audio encoder is not set")
        
        print(f"ğŸµ Processing audio features:")
        print(f"  Input audio shape: {audio_features.shape}")
        print(f"  Input audio dtype: {audio_features.dtype}")
        print(f"  Input audio device: {audio_features.device}")
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿éŸ³é¢‘ç¼–ç å™¨è¾“å‡ºæ­£ç¡®çš„æ•°æ®ç±»å‹
        with torch.no_grad():
            encoded_audio = self._audio_encoder(audio_features)
            
            # ç¡®ä¿ç¼–ç åçš„éŸ³é¢‘ç‰¹å¾ç±»å‹æ­£ç¡®
            if hasattr(self.llava_model, 'dtype'):
                target_dtype = self.llava_model.dtype
            else:
                # ä»LLaVAæ¨¡å‹çš„å‚æ•°æ¨æ–­æ•°æ®ç±»å‹
                target_dtype = next(self.llava_model.parameters()).dtype
            
            print(f"  Target dtype: {target_dtype}")
            print(f"  Encoded audio dtype: {encoded_audio.dtype}")
            
            if encoded_audio.dtype != target_dtype:
                print(f"  ğŸ”„ Converting encoded audio to {target_dtype}")
                encoded_audio = encoded_audio.to(dtype=target_dtype)
    
        if self.audio_projector is None:
            raise RuntimeError("Audio projector is not set")
        
        # éŸ³é¢‘åµŒå…¥ - æŠ•å½±å™¨ä¼šè‡ªåŠ¨å¤„ç†æ•°æ®ç±»å‹åŒ¹é…
        audio_embeds = self.audio_projector(encoded_audio)
        
        print(f"  Audio embeds shape: {audio_embeds.shape}")
        print(f"  Audio embeds dtype: {audio_embeds.dtype}")
        
        # è¿æ¥åˆ° inputs_embeds æœ«å°¾
        if inputs_embeds is None:
            return audio_embeds
        else:
            print(f"  Inputs embeds shape: {inputs_embeds.shape}")
            print(f"  Inputs embeds dtype: {inputs_embeds.dtype}")
            
            # ğŸ”§ ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
            if audio_embeds.dtype != inputs_embeds.dtype:
                print(f"  ğŸ”„ Converting audio embeds to match inputs_embeds dtype")
                audio_embeds = audio_embeds.to(dtype=inputs_embeds.dtype)
            
            # ç¡®ä¿éŸ³é¢‘ç‰¹å¾ä¸è¾“å…¥åµŒå…¥çš„ç»´åº¦åŒ¹é…
            if audio_embeds.size(-1) != inputs_embeds.size(-1):
                raise ValueError(
                    f"Audio features dimension ({audio_embeds.size(-1)}) "
                    f"does not match input embeddings dimension ({inputs_embeds.size(-1)})"
                )
            
            # æ‹¼æ¥å¼ é‡
            result = torch.cat([inputs_embeds, audio_embeds], dim=1)
            print(f"  Final result shape: {result.shape}")
            print(f"  Final result dtype: {result.dtype}")
            return result

    # ğŸ” ä¾¿æ·æ£€æŸ¥æ–¹æ³•
    def is_ready(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å‡†å¤‡å¥½æ¨ç†"""
        return all([
            self.llava_model is not None,
            self._tokenizer is not None,
            self._image_processor is not None,
            self._audio_encoder is not None
        ])
        
    def to(self, device_or_dtype):
        """é‡å†™toæ–¹æ³•ï¼Œæ”¯æŒè®¾å¤‡å’Œæ•°æ®ç±»å‹è½¬æ¢"""
        # ç§»åŠ¨ä¸»æ¨¡å‹
        super().to(device_or_dtype)
        
        # ç§»åŠ¨å¤–éƒ¨éŸ³é¢‘ç¼–ç å™¨
        if self._audio_encoder is not None:
            self._audio_encoder = self._audio_encoder.to(device_or_dtype)
            print(f"ğŸ”§ Audio encoder moved to: {device_or_dtype}")
        
        return self
    
    def cuda(self, device=None):
        """é‡å†™cudaæ–¹æ³•"""
        return self.to(f'cuda:{device}' if device is not None else 'cuda')
    
    def cpu(self):
        """é‡å†™cpuæ–¹æ³•"""
        return self.to('cpu')
    
    @property
    def device(self):
        """è·å–æ¨¡å‹è®¾å¤‡"""
        if self.llava_model is not None:
            return next(self.llava_model.parameters()).device
        return next(self.parameters()).device

