import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Dict, Any, List, Union, Tuple
from transformers.modeling_utils import PreTrainedModel
from transformers.modeling_outputs import CausalLMOutputWithPast
from triper.constants import AUDIO_TOKEN_INDEX, IMAGE_TOKEN_INDEX
from .audio import build_audio_projector
from triper.configs.triper_config import TriperConfig


class TriperModel(PreTrainedModel):
    """
    Triperå¤šæ¨¡æ€æ¨¡å‹ 
    åŒ…å«LLaVAä½œä¸ºè§†è§‰-è¯­è¨€å­æ¨¡å—ï¼Œå¹¶æ·»åŠ éŸ³é¢‘æ”¯æŒ
    """
    config_class = TriperConfig
    
    def __init__(self, config: TriperConfig):
        super().__init__(config)
        self.config = config
        
        # æ ¸å¿ƒç»„ä»¶
        self.llava_model = None
        self.audio_projector = None  # åªæœ‰å¯è®­ç»ƒçš„æŠ•å½±å™¨
        
        # ç§æœ‰å¼•ç”¨çš„å¤–éƒ¨ç»„ä»¶ï¼ˆä¸å‚ä¸è®­ç»ƒï¼‰
        self._tokenizer = None
        self._image_processor = None
        self._context_len = None
        self._audio_encoder = None  # ç§æœ‰å¼•ç”¨ï¼Œä¸å‚ä¸å‚æ•°ç»Ÿè®¡
        
        # æ„å»ºéŸ³é¢‘æ¨¡å—
        self._build_audio_modules()
        
        print(f"âœ… TriperModel initialized with config: {config.model_type}")
        
    
    def _build_audio_modules(self):
        """æ„å»ºéŸ³é¢‘æ¨¡å— - åªæ„å»ºå¯è®­ç»ƒçš„æŠ•å½±å™¨"""
        if hasattr(self.config, "mm_audio_encoder") and self.config.mm_audio_encoder:
            try:
                print(f"ğŸ”„ Building audio projector...")
                self.audio_projector = build_audio_projector(self.config)
                print(f"âœ… Audio projector built: {type(self.audio_projector).__name__}")
                    
            except Exception as e:
                print(f"âŒ Audio projector build failed: {e}")
                self.audio_projector = None
                raise
        else:
            print("âš ï¸ No audio encoder specified in config")
    
    def attach_llava_model(self, llava_model):
        """é™„åŠ LLaVAæ¨¡å‹ä½œä¸ºå­æ¨¡å—"""
        self.llava_model = llava_model
        print(f"âœ… LLaVA model attached: {type(llava_model).__name__}")
        
        if hasattr(self.config, 'freeze_llava') and self.config.freeze_llava:
            self._freeze_llava_parameters()
    
    def set_audio_encoder(self, audio_encoder):
        """è®¾ç½®éŸ³é¢‘ç¼–ç å™¨ï¼ˆå¤–éƒ¨å¼•ç”¨ï¼Œä¸å‚ä¸è®­ç»ƒï¼‰"""
        self._audio_encoder = audio_encoder
        print(f"ğŸµ Audio encoder attached: {type(audio_encoder).__name__}")
    
    def set_components(self, tokenizer, image_processor, context_len):
        """ä¸€æ¬¡æ€§è®¾ç½®æ‰€æœ‰å¤–éƒ¨ç»„ä»¶"""
        self._tokenizer = tokenizer
        self._image_processor = image_processor
        self._context_len = context_len
        print(f"ğŸ“¦ Components set: tokenizer({type(tokenizer).__name__}), "
              f"processor({type(image_processor).__name__}), context_len({context_len})")
    
    def _freeze_llava_parameters(self):
        """å†»ç»“LLaVAæ¨¡å‹å‚æ•°"""
        if self.llava_model is not None:
            for param in self.llava_model.parameters():
                param.requires_grad = False
            print("ğŸ”’ LLaVA model parameters frozen")
    
    # ğŸ¯ ç»Ÿä¸€çš„ç»„ä»¶è®¿é—®å™¨
    @property
    def tokenizer(self):
        """è·å–åˆ†è¯å™¨"""
        return self._tokenizer
    
    @property
    def image_processor(self):
        """è·å–å›¾åƒå¤„ç†å™¨"""
        return self._image_processor
    
    @property
    def context_len(self):
        """è·å–ä¸Šä¸‹æ–‡é•¿åº¦"""
        return self._context_len or 2048
    
    @property
    def audio_encoder(self):
        """è·å–éŸ³é¢‘ç¼–ç å™¨ï¼ˆå¤–éƒ¨å¼•ç”¨ï¼Œä¸å‚ä¸è®­ç»ƒï¼‰"""
        return self._audio_encoder
    
    # ğŸ” LLaVAå­ç»„ä»¶è®¿é—®
    def get_model(self):
        """è·å–åŸºç¡€è¯­è¨€æ¨¡å‹"""
        if self.llava_model is not None:
            return getattr(self.llava_model, 'model', None) or getattr(self.llava_model, 'get_model', lambda: None)()
        return None
    
    def get_vision_tower(self):
        """è·å–è§†è§‰å¡”"""
        return getattr(self.llava_model, 'get_vision_tower', lambda: None)() if self.llava_model else None
    
    def get_vision_projector(self):
        """è·å–è§†è§‰æŠ•å½±å™¨"""
        if not self.llava_model:
            return None
        # å°è¯•å¤šç§å¯èƒ½çš„è®¿é—®è·¯å¾„
        for attr_path in ['mm_projector', 'model.mm_projector']:
            obj = self.llava_model
            for attr in attr_path.split('.'):
                obj = getattr(obj, attr, None)
                if obj is None:
                    break
            if obj is not None:
                return obj
        return None
    
    
    # ğŸ“Š å‚æ•°ç»Ÿè®¡
    def get_parameter_stats(self) -> Dict[str, Any]:
        """è·å–å‚æ•°ç»Ÿè®¡ä¿¡æ¯ - åªç»Ÿè®¡å¯è®­ç»ƒç»„ä»¶"""
        components = {
            'llava': self.llava_model,
            'audio_projector': self.audio_projector,
            # æ³¨æ„ï¼šä¸åŒ…å« audio_encoderï¼Œå› ä¸ºå®ƒä¸å‚ä¸è®­ç»ƒ
        }
        
        stats = {'total_params': 0, 'trainable_params': 0, 'components': {}}
        
        for name, component in components.items():
            comp_stats = {'total': 0, 'trainable': 0}
            if component is not None:
                for param in component.parameters():
                    comp_stats['total'] += param.numel()
                    if param.requires_grad:
                        comp_stats['trainable'] += param.numel()
            
            stats['components'][name] = comp_stats
            stats['total_params'] += comp_stats['total']
            stats['trainable_params'] += comp_stats['trainable']
        
        return stats
    
    def print_model_summary(self):
        """æ‰“å°æ¨¡å‹æ‘˜è¦"""
        print("\nğŸ—ï¸  Triper Model Summary")
        print("=" * 60)
        
        # ç»„ä»¶çŠ¶æ€
        print("ğŸ“¦ Components:")
        components_status = [
            ("ğŸ¦™ LLaVA", self.llava_model),
            ("ğŸµ Audio Encoder", self._audio_encoder, "ğŸ”’ External (Frozen)"),
            ("ğŸ”— Audio Projector", self.audio_projector, "ğŸ”“ Trainable"),
            ("ğŸ“ Tokenizer", self._tokenizer, "ğŸ”’ External"),
            ("ğŸ–¼ï¸ Image Processor", self._image_processor, "ğŸ”’ External")
        ]
        
        for item in components_status:
            if len(item) == 3:
                name, component, note = item
                status = "âœ…" if component is not None else "âŒ"
                type_info = f"({type(component).__name__}) {note}" if component else ""
            else:
                name, component = item
                status = "âœ…" if component is not None else "âŒ"
                type_info = f"({type(component).__name__})" if component else ""
            print(f"  {name}: {status} {type_info}")
        
        # å‚æ•°ç»Ÿè®¡ï¼ˆåªæ˜¾ç¤ºå¯è®­ç»ƒå‚æ•°ï¼‰
        stats = self.get_parameter_stats()
        print(f"\nğŸ“Š Trainable Parameters:")
        print(f"  Total: {stats['total_params']:,}")
        print(f"  Trainable: {stats['trainable_params']:,} "
              f"({stats['trainable_params']/max(stats['total_params'], 1)*100:.1f}%)")
        
        # ç»„ä»¶è¯¦ç»†ç»Ÿè®¡
        for name, comp_stats in stats['components'].items():
            if comp_stats['total'] > 0:
                ratio = comp_stats['trainable'] / comp_stats['total'] * 100
                status = "ğŸ”“" if comp_stats['trainable'] > 0 else "ğŸ”’"
                print(f"    {name}: {comp_stats['total']:,} ({ratio:.1f}% trainable) {status}")
        
        print("=" * 60)
    
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.Tensor]] = None, 
        inputs_embeds: Optional[torch.Tensor] = None,         
        labels: Optional[torch.LongTensor] = None,
        images: Optional[torch.Tensor] = None,                
        image_sizes: Optional[List[List[int]]] = None,
        audio_features: Optional[torch.Tensor] = None,        
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **kwargs
    ) -> Union[Tuple, CausalLMOutputWithPast]:
        """Triperæ¨¡å‹çš„å‰å‘ä¼ æ’­ï¼Œæ‰©å±•LLaVAæ”¯æŒéŸ³é¢‘"""
        if not self.is_ready():
            raise RuntimeError("æ¨¡å‹ç»„ä»¶å°šæœªå®Œå…¨é…ç½®")

        # ğŸ”§ æ·»åŠ è¾“å…¥éªŒè¯å’Œä¿®å¤
        if input_ids is not None:
            # 1. æ£€æŸ¥tokenèŒƒå›´
            vocab_size = self.llava_model.config.vocab_size
            print(f"ğŸ” TokenèŒƒå›´æ£€æŸ¥: vocab_size={vocab_size}, input_ids range=({input_ids.min()}, {input_ids.max()})")
            
            # ä¿®å¤è¶…å‡ºèŒƒå›´çš„token
            if input_ids.max() >= vocab_size:
                print(f"âš ï¸ å‘ç°è¶…å‡ºè¯æ±‡è¡¨çš„tokenï¼Œæˆªæ–­åˆ°{vocab_size-1}")
                input_ids = torch.clamp(input_ids, 0, vocab_size - 1)
            
            # 2. ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            if input_ids.dtype != torch.long:
                input_ids = input_ids.long()
    
        if attention_mask is not None:
            print(f"ğŸ” Attention maskæ£€æŸ¥: dtype={attention_mask.dtype}, å€¼åŸŸ=({attention_mask.min()}, {attention_mask.max()})")
            
            # 1. ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            if attention_mask.dtype not in [torch.long, torch.int, torch.bool]:
                print(f"âš ï¸ ä¿®å¤attention_mask dtype: {attention_mask.dtype} -> torch.long")
                attention_mask = attention_mask.long()
            
            # 2. ç¡®ä¿å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…[0, 1]
            if attention_mask.min() < 0 or attention_mask.max() > 1:
                print(f"âš ï¸ ä¿®å¤attention_maskå€¼åŸŸ: ({attention_mask.min()}, {attention_mask.max()}) -> [0, 1]")
                attention_mask = torch.clamp(attention_mask, 0, 1)
            
            # 3. æ£€æŸ¥NaN/Inf
            if torch.isnan(attention_mask).any() or torch.isinf(attention_mask).any():
                print("âš ï¸ æ£€æµ‹åˆ°NaN/Infï¼Œé‡ç½®attention_mask...")
                attention_mask = torch.ones_like(attention_mask)

        print(f"ğŸ”¥ TriperModel.forward called:")
        print(f"  input_ids: {input_ids.shape if input_ids is not None else 'None'}")
        print(f"  images: {images.shape if images is not None else 'None'}")
        print(f"  audio_features: {audio_features.shape if audio_features is not None else 'None'}")
        print(f"  past_key_values: {len(past_key_values) if past_key_values else 0} layers")
        
        # ğŸ¯ å…³é”®ä¿®å¤ï¼šå¦‚æœinput_idsä¸ºNoneä½†æœ‰past_key_valuesï¼Œè¯´æ˜æ˜¯åç»­ç”Ÿæˆæ­¥éª¤
        if input_ids is None and past_key_values is not None and len(past_key_values) > 0:
            print("  âš¡ input_idsä¸ºNoneä¸”æœ‰past_key_valuesï¼Œç›´æ¥è°ƒç”¨LLaVA...")
            return self.llava_model.__class__.forward(
                self.llava_model,
                input_ids=input_ids,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_values=past_key_values,
                inputs_embeds=inputs_embeds,
                labels=labels,
                use_cache=use_cache,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
                return_dict=return_dict,
                **kwargs
            )

        # ğŸ”§ ç¡®å®šæ˜¯å¦éœ€è¦å¤„ç†å¤šæ¨¡æ€è¾“å…¥
        need_multimodal_processing = (images is not None) or (audio_features is not None)
        
        if inputs_embeds is None and input_ids is not None and need_multimodal_processing:
            # æœ‰å¤šæ¨¡æ€è¾“å…¥ï¼Œéœ€è¦å¤„ç†
            
            # 1. LLaVAå¤„ç†å›¾åƒ
            if images is not None:
                print("  ğŸ“¸ LLaVAå¤„ç†å›¾åƒ...")
                multimodal_result = self.llava_model.prepare_inputs_labels_for_multimodal(
                    input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes
                )
                input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels = multimodal_result
                print(f"  LLaVAå¤„ç†åembeds: {inputs_embeds.shape}")
            else:
                # æ²¡æœ‰å›¾åƒï¼Œç›´æ¥è·å–æ–‡æœ¬embeds
                inputs_embeds = self.llava_model.get_model().embed_tokens(input_ids)
                print(f"  çº¯æ–‡æœ¬embeds: {inputs_embeds.shape}")

            # 2. æ’å…¥éŸ³é¢‘ç‰¹å¾ï¼ˆåªåœ¨ç¬¬ä¸€æ­¥ä¸”æœ‰input_idsæ—¶ï¼‰
            if audio_features is not None:
                print("  ğŸµ æ’å…¥éŸ³é¢‘ç‰¹å¾...")
                inputs_embeds, attention_mask = self._insert_audio_features(
                    inputs_embeds, input_ids, audio_features, attention_mask
                )
                print(f"  åˆå¹¶åembeds: {inputs_embeds.shape}")
                print(f"  åˆå¹¶åattention_mask: {attention_mask.shape}")

            print(f"  ğŸ” æœ€ç»ˆéªŒè¯:")
            print(f"    inputs_embeds: {inputs_embeds.shape if inputs_embeds is not None else None}")
            print(f"    attention_mask: {attention_mask.shape if attention_mask is not None else None}")
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¤„ç†å®Œå¤šæ¨¡æ€åï¼Œæ¸…ç©ºinput_ids
            input_ids = None  # ç¡®ä¿åªä¼ é€’inputs_embeds
            
        elif inputs_embeds is None and input_ids is not None and not need_multimodal_processing:
            # ğŸ”§ çº¯æ–‡æœ¬æƒ…å†µï¼šç›´æ¥ä¼ é€’input_idsï¼Œä¸ç”Ÿæˆinputs_embeds
            print("  ğŸ“ çº¯æ–‡æœ¬è¾“å…¥ï¼Œç›´æ¥ä¼ é€’input_ids...")
            pass  # ä¿æŒinput_idsï¼Œä¸è®¾ç½®inputs_embeds

        # 3. è°ƒç”¨LLaVAè¿›è¡Œå‰å‘ä¼ æ’­
        return self.llava_model.forward(
            input_ids=input_ids,  # å¤šæ¨¡æ€æ—¶ä¸ºNoneï¼Œçº¯æ–‡æœ¬æ—¶ä¸ºåŸå€¼
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,  # å¤šæ¨¡æ€æ—¶æœ‰å€¼ï¼Œçº¯æ–‡æœ¬æ—¶ä¸ºNone
            labels=labels,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            **kwargs
        ) # type: ignore


    def _insert_audio_features(self, inputs_embeds, input_ids, audio_features, attention_mask=None):
        """æ’å…¥éŸ³é¢‘ç‰¹å¾åˆ°åºåˆ—æœ«å°¾ï¼ŒåŒæ—¶æ›´æ–°attention_mask"""
        if audio_features is None:
            return inputs_embeds, attention_mask

        batch_size = inputs_embeds.shape[0]
        audio_seq_len = audio_features.shape[1]
        
        # 1. ç¡®ä¿attention_maskä¸å½“å‰inputs_embedsé•¿åº¦åŒ¹é…
        if attention_mask is not None and attention_mask.shape[1] != inputs_embeds.shape[1]:
            # åŠ¨æ€è°ƒæ•´åˆ°å®é™…çš„embedsé•¿åº¦
            actual_text_len = inputs_embeds.shape[1]
            if attention_mask.shape[1] > actual_text_len:
                attention_mask = attention_mask[:, :actual_text_len]
            else:
                # å¡«å……åˆ°å®é™…é•¿åº¦
                padding = torch.ones(
                    (batch_size, actual_text_len - attention_mask.shape[1]),
                    dtype=attention_mask.dtype, device=inputs_embeds.device
                )
                attention_mask = torch.cat([attention_mask, padding], dim=1)
    
        # æŠ•å½±éŸ³é¢‘ç‰¹å¾
        audio_embeds = self.audio_projector(audio_features)
        
        # ç±»å‹å¯¹é½
        if audio_embeds.dtype != inputs_embeds.dtype:
            audio_embeds = audio_embeds.to(inputs_embeds.dtype)
        
        # æ‹¼æ¥ç‰¹å¾
        combined_embeds = torch.cat([inputs_embeds, audio_embeds], dim=1)
        
        # 3. æ‰©å±•attention_maskä»¥åŒ¹é…æœ€ç»ˆé•¿åº¦
        audio_mask = torch.ones((batch_size, audio_embeds.shape[1]), dtype=attention_mask.dtype, device=inputs_embeds.device)
        final_attention_mask = torch.cat([attention_mask, audio_mask], dim=1)
        
        print(f"ğŸµ éŸ³é¢‘ç‰¹å¾æ’å…¥å®Œæˆ:")
        print(f"  åŸå§‹embeds: {inputs_embeds.shape}")
        print(f"  éŸ³é¢‘embeds: {audio_embeds.shape}")
        print(f"  åˆå¹¶åembeds: {combined_embeds.shape}")
        print(f"  åˆå¹¶åattention_mask: {final_attention_mask.shape}")
        
        # ğŸ”§ éªŒè¯é•¿åº¦ä¸€è‡´æ€§
        assert combined_embeds.shape[1] == final_attention_mask.shape[1], \
            f"é•¿åº¦ä¸åŒ¹é…: embeds={combined_embeds.shape[1]}, mask={final_attention_mask.shape[1]}"
        
        return combined_embeds, final_attention_mask
    
    # ğŸ” ä¾¿æ·æ£€æŸ¥æ–¹æ³•
    def is_ready(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å‡†å¤‡å¥½æ¨ç†"""
        return all([
            self.llava_model is not None,
            self._tokenizer is not None,
            self._image_processor is not None,
            self._audio_encoder is not None
        ])
        
    def to(self, device_or_dtype):
        """é‡å†™toæ–¹æ³•ï¼Œæ”¯æŒè®¾å¤‡å’Œæ•°æ®ç±»å‹è½¬æ¢"""
        # ç§»åŠ¨ä¸»æ¨¡å‹
        super().to(device_or_dtype)
        
        # ç§»åŠ¨å¤–éƒ¨éŸ³é¢‘ç¼–ç å™¨
        if self._audio_encoder is not None:
            self._audio_encoder = self._audio_encoder.to(device_or_dtype)
            print(f"ğŸ”§ Audio encoder moved to: {device_or_dtype}")
        
        return self
    
    def cuda(self, device=None):
        """é‡å†™cudaæ–¹æ³•"""
        return self.to(f'cuda:{device}' if device is not None else 'cuda')
    
    def cpu(self):
        """é‡å†™cpuæ–¹æ³•"""
        return self.to('cpu')
    
    @property
    def device(self):
        """è·å–æ¨¡å‹è®¾å¤‡"""
        if self.llava_model is not None:
            return next(self.llava_model.parameters()).device
        return next(self.parameters()).device

    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, 
                                      attention_mask=None, inputs_embeds=None, **kwargs):
        """
        ğŸ¯ æ ¸å¿ƒæ–¹æ³•ï¼šä¸ºç”Ÿæˆè¿‡ç¨‹å‡†å¤‡è¾“å…¥
        è¿™ä¸ªæ–¹æ³•æ§åˆ¶generateå¾ªç¯ï¼Œä¸ºæ¯ä¸€æ­¥çš„è§£ç å‡†å¤‡è¾“å…¥
        """
        print(f"ğŸ”§ TriperModel.prepare_inputs_for_generation called:")
        print(f"  input_ids: {input_ids.shape if input_ids is not None else None}")
        print(f"  past_key_values: {len(past_key_values) if past_key_values else 0} layers")
        print(f"  inputs_embeds: {inputs_embeds.shape if inputs_embeds is not None else None}")
        
        # å¦‚æœä¸æ˜¯ç¬¬ä¸€æ­¥ï¼ˆå·²ç»æœ‰ç¼“å­˜çš„key/valueï¼‰ï¼Œé‚£ä¹ˆinput_idså°±åªæ˜¯æœ€æ–°ç”Ÿæˆçš„é‚£ä¸ªtoken
        if past_key_values:
            input_ids = input_ids[:, -1:]
            print(f"  ğŸ”„ åç»­æ­¥éª¤ï¼Œinput_idsæˆªå–ä¸º: {input_ids.shape}")
        
        # è°ƒç”¨LLaVAçš„prepare_inputs_for_generationï¼Œè®©å®ƒå¤„ç†å¤§éƒ¨åˆ†çš„å‡†å¤‡å·¥ä½œ
        model_inputs = self.llava_model.prepare_inputs_for_generation(
            input_ids, 
            past_key_values=past_key_values, 
            attention_mask=attention_mask, 
            inputs_embeds=inputs_embeds, 
            **kwargs
        )
        
        print(f"  âœ… LLaVAå‡†å¤‡çš„inputs: {list(model_inputs.keys())}")
        return model_inputs

    @torch.no_grad()
    def generate(
        self,
        input_ids: torch.LongTensor,
        images: Optional[torch.FloatTensor] = None,
        image_sizes: Optional[List[List[int]]] = None,
        audio_features: Optional[torch.FloatTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        **kwargs
    ):
        """ğŸš€ ç®€åŒ–ç‰ˆçš„Triper generateæ–¹æ³•"""
        if not self.is_ready():
            raise RuntimeError("æ¨¡å‹ç»„ä»¶å°šæœªå®Œå…¨é…ç½®ï¼Œæ— æ³•è¿›è¡Œç”Ÿæˆ")
        
        print(f"ğŸš€ TriperModel.generate called:")
        print(f"  input_ids: {input_ids.shape}")
        print(f"  images: {images.shape if images is not None else None}")
        print(f"  audio_features: {audio_features.shape if audio_features is not None else None}")

        # ğŸ”§ å¦‚æœæ²¡æœ‰éŸ³é¢‘ï¼Œç›´æ¥è°ƒç”¨LLaVAçš„generate
        if audio_features is None:
            print("ğŸ“ æ— éŸ³é¢‘è¾“å…¥ï¼Œç›´æ¥ä½¿ç”¨LLaVA...")
            return self.llava_model.generate(
                inputs=input_ids,  # æ³¨æ„ï¼šLLaVAæœŸæœ›çš„æ˜¯inputsï¼Œä¸æ˜¯input_ids
                images=images,
                image_sizes=image_sizes,
                attention_mask=attention_mask,
                **kwargs
            )
        
        # ğŸ¯ æœ‰éŸ³é¢‘çš„æƒ…å†µï¼šæ‰‹åŠ¨å‡†å¤‡inputs_embedsç„¶åè°ƒç”¨LLaVA
        print("ğŸµ æ£€æµ‹åˆ°éŸ³é¢‘è¾“å…¥ï¼Œå‡†å¤‡å¤šæ¨¡æ€embeddings...")
        
        # ğŸ”§ ä¿®å¤attention_maské•¿åº¦é—®é¢˜
        if attention_mask is not None and input_ids is not None:
            if attention_mask.shape[1] != input_ids.shape[1]:
                print(f"âš ï¸ attention_maské•¿åº¦ä¸åŒ¹é…ï¼Œæˆªå–åˆ°æ–‡æœ¬é•¿åº¦")
                attention_mask = attention_mask[:, :input_ids.shape[1]]
        
        # 1. å‡†å¤‡å¤šæ¨¡æ€inputs_embeds
        if images is not None:
            print("ğŸ“¸ LLaVAå¤„ç†å›¾åƒ...")
            multimodal_result = self.llava_model.prepare_inputs_labels_for_multimodal(
                input_ids, None, attention_mask, None, None, images, image_sizes
            )
            _, _, attention_mask, _, inputs_embeds, _ = multimodal_result
            print(f"LLaVAå¤„ç†åembeds: {inputs_embeds.shape}")
        else:
            # æ²¡æœ‰å›¾åƒï¼Œç›´æ¥è·å–æ–‡æœ¬embeds
            inputs_embeds = self.llava_model.get_model().embed_tokens(input_ids)
            print(f"çº¯æ–‡æœ¬embeds: {inputs_embeds.shape}")

        # 2. é›†æˆéŸ³é¢‘
        print("ğŸµ é›†æˆéŸ³é¢‘ç‰¹å¾...")
        inputs_embeds, attention_mask = self._insert_audio_features(
            inputs_embeds, input_ids, audio_features, attention_mask
        )
        print(f"æœ€ç»ˆembeds: {inputs_embeds.shape}")
        print(f"æœ€ç»ˆattention_mask: {attention_mask.shape}")

        # # 3. ç›´æ¥è°ƒç”¨LLaVAçš„generateï¼Œä¼ å…¥å‡†å¤‡å¥½çš„inputs_embeds
        
        print("ğŸš€ è°ƒç”¨LLaVA.generate with inputs_embeds...")
        from transformers import Llama4ForCausalLM
        return Llama4ForCausalLM.generate(
            self.llava_model,
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            **kwargs
        )