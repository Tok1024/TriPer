import torch
import gc
import os
import sys
import json
from pathlib import Path
import traceback
from typing import Dict, Any, List

# è®¾ç½®è·¯å¾„
sys.path = [p for p in sys.path if 'triper-project' not in p]
sys.path.append('/home/wly/szl_all_code/triper-project')

# å¯¼å…¥æ¨¡å—
from triper.model import from_pretrained_components
from triper.data import TriperDataset, TriperDataCollator
from triper.constants import DEFAULT_IMAGE_TOKEN

class TriperModelTester:
    """Triperæ¨¡å‹å…¨é¢æµ‹è¯•å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.results = {}
        self.tokenizer = None
        self.triper_model = None
        self.image_processor = None
        self.context_len = None
        self.audio_encoder = None
        self.dataset = None
        self.collator = None
        
    def setup_test_environment(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        print("\n" + "="*80)
        print("ğŸš€ TRIPER MODEL COMPREHENSIVE TEST")
        print("="*80)
        
        # æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()
        gc.collect()
        print("âœ… GPUç¼“å­˜å·²æ¸…ç†")
        
        # æ‰“å°é…ç½®
        print(f"\nğŸ“‹ æµ‹è¯•é…ç½®:")
        for key, value in self.config.items():
            print(f"  {key}: {value}")
        
    def test_1_model_loading(self):
        """æµ‹è¯•1: æ¨¡å‹åŠ è½½"""
        print(f"\n{'='*50}")
        print("ğŸ“¦ æµ‹è¯•1: æ¨¡å‹ç»„ä»¶åŠ è½½")
        print("="*50)
        
        try:
            audio_config = {
                'mm_audio_encoder': 'whisper_vq',
                'audio_hidden_size': 1280,
                'audio_model_path': self.config['audio_encoder_path'],
                'audio_projector_type': 'mlp2x_gelu',
                'audio_projector_hidden_dim': 2048,
                'dropout': 0.1
            }
            
            print("ğŸ”„ åŠ è½½æ¨¡å‹ç»„ä»¶...")
            self.tokenizer, self.triper_model, self.image_processor, self.context_len, self.audio_encoder = from_pretrained_components(
                llava_model_path=self.config['llava_model_path'],
                audio_encoder_path=self.config['audio_encoder_path'],
                audio_projector_path=None,
                audio_config=audio_config,
                freeze_llava=True,
                device_map=self.config['device']
            )
            
            print("âœ… æ¨¡å‹ç»„ä»¶åŠ è½½æˆåŠŸ")
            
            # éªŒè¯ç»„ä»¶
            assert self.tokenizer is not None, "TokenizeråŠ è½½å¤±è´¥"
            assert self.triper_model is not None, "TriperModelåŠ è½½å¤±è´¥"
            assert self.image_processor is not None, "ImageProcessoråŠ è½½å¤±è´¥"
            assert self.audio_encoder is not None, "AudioEncoderåŠ è½½å¤±è´¥"
            
            # æ‰“å°æ¨¡å‹æ‘˜è¦
            self.triper_model.print_model_summary()
            
            # éªŒè¯æ¨¡å‹å°±ç»ªçŠ¶æ€
            assert self.triper_model.is_ready(), "æ¨¡å‹æœªå°±ç»ª"
            
            self.results['model_loading'] = "âœ… PASSED"
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            traceback.print_exc()
            self.results['model_loading'] = f"âŒ FAILED: {e}"
            raise
    
    def test_2_data_loading(self):
        """æµ‹è¯•2: æ•°æ®åŠ è½½"""
        print(f"\n{'='*50}")
        print("ğŸ“Š æµ‹è¯•2: æ•°æ®é›†å’ŒCollator")
        print("="*50)
        
        try:
            # åŠ è½½æ•°æ®é›†
            print("ğŸ”„ åŠ è½½æ•°æ®é›†...")
            self.dataset = TriperDataset(
                json_path=self.config['data_path'],
                media_root_path=self.config['media_root'],
            )
            
            print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸï¼Œå…± {len(self.dataset)} ä¸ªæ ·æœ¬")
            
            # åˆ›å»ºcollator
            print("ğŸ”„ åˆ›å»ºæ•°æ®collator...")
            self.collator = TriperDataCollator(
                tokenizer=self.tokenizer,
                image_processor=self.image_processor,
                audio_processor=self.audio_encoder,
                model_cfg=self.triper_model.llava_model.config
            )
            
            print("âœ… Collatoråˆ›å»ºæˆåŠŸ")
            
            # æµ‹è¯•å•ä¸ªæ ·æœ¬
            print("ğŸ”„ æµ‹è¯•å•ä¸ªæ ·æœ¬...")
            sample = self.dataset[0]
            print(f"æ ·æœ¬ç»“æ„: {list(sample.keys())}")
            
            # æµ‹è¯•collatorå¤„ç†
            print("ğŸ”„ æµ‹è¯•collatorå¤„ç†...")
            batch = self.collator([sample])
            
            print(f"æ‰¹é‡æ•°æ®å½¢çŠ¶:")
            for key, value in batch.items():
                if hasattr(value, 'shape'):
                    print(f"  {key}: {value.shape}")
                else:
                    print(f"  {key}: {type(value)}")
            
            self.results['data_loading'] = "âœ… PASSED"
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            traceback.print_exc()
            self.results['data_loading'] = f"âŒ FAILED: {e}"
            raise
    
    def test_3_forward_pass(self):
        """æµ‹è¯•3: å‰å‘ä¼ æ’­"""
        print(f"\n{'='*50}")
        print("ğŸ”¥ æµ‹è¯•3: æ¨¡å‹å‰å‘ä¼ æ’­")
        print("="*50)
        
        try:
            # å‡†å¤‡æµ‹è¯•æ•°æ®
            test_samples = [self.dataset[i] for i in range(min(2, len(self.dataset)))]
            batch = self.collator(test_samples)
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            device_batch = {}
            for k, v in batch.items():
                if hasattr(v, 'to'):
                    device_batch[k] = v.to(self.triper_model.device)
                else:
                    device_batch[k] = v
            
            print("ğŸ”„ æµ‹è¯•å®Œæ•´å‰å‘ä¼ æ’­ï¼ˆå›¾åƒ+éŸ³é¢‘ï¼‰...")
            with torch.no_grad():
                output = self.triper_model(
                    input_ids=device_batch['input_ids'],
                    images=device_batch['images'],
                    audio_features=device_batch['audio_features'],
                    attention_mask=device_batch['attention_mask']
                )
                
            print("ğŸ”„ æµ‹è¯•ä»…å›¾åƒå‰å‘ä¼ æ’­...")
            with torch.no_grad():
                output_img_only = self.triper_model(
                    input_ids=device_batch['input_ids'],
                    images=device_batch['images'],
                    audio_features=None,  # ä¸ä¼ éŸ³é¢‘
                    attention_mask=device_batch['attention_mask']
                )
            print(f"âœ… ä»…å›¾åƒå‰å‘ä¼ æ’­æˆåŠŸ: {output_img_only.logits.shape}")
            print("ğŸ”„ æµ‹è¯•çº¯æ–‡æœ¬å‰å‘ä¼ æ’­...")
            
            # ğŸ”§ åˆ›å»ºçº¯æ–‡æœ¬promptï¼ˆä¸åŒ…å«<image>æ ‡è®°ï¼‰
            pure_text_prompt = "USER: Tell me about artificial intelligence.\nASSISTANT:"
            pure_text_ids = self.tokenizer.encode(pure_text_prompt, return_tensors="pt")
            pure_text_mask = torch.ones_like(pure_text_ids)
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            pure_text_ids = pure_text_ids.to(self.triper_model.device)
            pure_text_mask = pure_text_mask.to(self.triper_model.device)
            
            print(f"ğŸ“ çº¯æ–‡æœ¬prompt: {pure_text_prompt}")
            print(f"ğŸ“ çº¯æ–‡æœ¬input_idså½¢çŠ¶: {pure_text_ids.shape}")
            
            with torch.no_grad():
                output_text_only = self.triper_model(
                    input_ids=pure_text_ids,
                    images=None,  # ä¸ä¼ å›¾åƒ
                    audio_features=None,  # ä¸ä¼ éŸ³é¢‘
                    attention_mask=pure_text_mask
                )
            
            print(f"âœ… çº¯æ–‡æœ¬å‰å‘ä¼ æ’­æˆåŠŸ: {output_text_only.logits.shape}")
            
            
            print(f"âœ… å®Œæ•´å‰å‘ä¼ æ’­æˆåŠŸ")
            print(f"  è¾“å‡ºlogitså½¢çŠ¶: {output.logits.shape}")
            print(f"  è¾“å‡ºç±»å‹: {type(output)}")
            
            # éªŒè¯è¾“å‡ºåˆç†æ€§
            assert output.logits.shape[0] == len(test_samples), "æ‰¹é‡å¤§å°ä¸åŒ¹é…"
            assert not torch.isnan(output.logits).any(), "è¾“å‡ºåŒ…å«NaN"
            assert not torch.isinf(output.logits).any(), "è¾“å‡ºåŒ…å«Inf"
            
            print("ğŸ”„ æµ‹è¯•ä»…å›¾åƒå‰å‘ä¼ æ’­...")
            with torch.no_grad():
                output_img_only = self.triper_model(
                    input_ids=device_batch['input_ids'],
                    images=device_batch['images'],
                    audio_features=None,  # ä¸ä¼ éŸ³é¢‘
                    attention_mask=device_batch['attention_mask']
                )
            
            print(f"âœ… ä»…å›¾åƒå‰å‘ä¼ æ’­æˆåŠŸ: {output_img_only.logits.shape}")
            

            
        except Exception as e:
            print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
            traceback.print_exc()
            self.results['forward_pass'] = f"âŒ FAILED: {e}"
            raise
    
    def test_4_generation_capability(self):
        """æµ‹è¯•4: ç”Ÿæˆèƒ½åŠ›"""
        print(f"\n{'='*50}")
        print("ğŸš€ æµ‹è¯•4: æ¨¡å‹ç”Ÿæˆèƒ½åŠ›")
        print("="*50)
        
        try:
            # å‡†å¤‡å•ä¸ªæ ·æœ¬
            single_sample = [self.dataset[0]]
            single_batch = self.collator(single_sample)
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            device_batch = {}
            for k, v in single_batch.items():
                if hasattr(v, 'to'):
                    device_batch[k] = v.to(self.triper_model.device)
                else:
                    device_batch[k] = v
            
            # ä¿®å¤attention_maské•¿åº¦
            text_len = device_batch['input_ids'].shape[1]
            attention_mask = device_batch['attention_mask'][:, :text_len]
            
            print("ğŸ”„ æµ‹è¯•1: çº¯LLaVAç”Ÿæˆï¼ˆå›¾åƒ+æ–‡æœ¬ï¼‰...")
            response_llava = self.triper_model.generate(
                input_ids=device_batch['input_ids'],
                attention_mask=attention_mask,
                images=device_batch['images'],
                audio_features=None,  # ä¸ä¼ éŸ³é¢‘
                max_new_tokens=30,
                do_sample=True,
                temperature=0.8,
                top_p=0.9,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
            
            # è§£ç ç»“æœ
            original_len = device_batch['input_ids'].shape[1]
            if response_llava.shape[1] > original_len:
                generated_text = self.tokenizer.decode(
                    response_llava[0, original_len:], 
                    skip_special_tokens=True
                )
                print(f"âœ… LLaVAç”ŸæˆæˆåŠŸ: '{generated_text[:100]}...'")
            else:
                print(f"âš ï¸ LLaVAç”Ÿæˆé•¿åº¦å¼‚å¸¸: {response_llava.shape}")
            
            print("ğŸ”„ æµ‹è¯•2: å®Œæ•´Triperç”Ÿæˆï¼ˆå›¾åƒ+éŸ³é¢‘+æ–‡æœ¬ï¼‰...")
            response_triper = self.triper_model.generate(
                input_ids=device_batch['input_ids'],
                attention_mask=attention_mask,
                images=device_batch['images'],
                audio_features=device_batch['audio_features'],  # ä¼ éŸ³é¢‘
                max_new_tokens=30,
                do_sample=True,
                temperature=0.8,
                top_p=0.9,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
            
            if response_triper.shape[1] > 2:  # è‡³å°‘ç”Ÿæˆäº†ä¸€äº›token
                # æ³¨æ„ï¼šTriperå¯èƒ½è¿”å›ä¸åŒæ ¼å¼ï¼Œéœ€è¦çµæ´»å¤„ç†
                if response_triper.shape[1] > original_len:
                    generated_text = self.tokenizer.decode(
                        response_triper[0, original_len:], 
                        skip_special_tokens=True
                    )
                else:
                    generated_text = self.tokenizer.decode(
                        response_triper[0], 
                        skip_special_tokens=True
                    )
                print(f"âœ… Triperç”ŸæˆæˆåŠŸ: '{generated_text[:100]}...'")
            else:
                print(f"âš ï¸ Triperç”Ÿæˆé•¿åº¦è¿‡çŸ­: {response_triper.shape}")
            
            print("ğŸ”„ æµ‹è¯•3: ç®€åŒ–promptç”Ÿæˆ...")
            simple_prompt = f"{DEFAULT_IMAGE_TOKEN}\nUSER: What do you see in this image?\nASSISTANT:"
            simple_ids = self.tokenizer.encode(simple_prompt, return_tensors="pt").to(self.triper_model.device)
            
            simple_response = self.triper_model.llava_model.generate(
                inputs=simple_ids,
                images=device_batch['images'],
                max_new_tokens=50,
                do_sample=False,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
            
            if simple_response.shape[1] > simple_ids.shape[1]:
                simple_text = self.tokenizer.decode(
                    simple_response[0, simple_ids.shape[1]:], 
                    skip_special_tokens=True
                )
                print(f"âœ… ç®€åŒ–promptç”ŸæˆæˆåŠŸ: '{simple_text[:100]}...'")
            else:
                print(f"âš ï¸ ç®€åŒ–promptç”Ÿæˆå¤±è´¥")
            
            self.results['generation_capability'] = "âœ… PASSED"
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
            traceback.print_exc()
            self.results['generation_capability'] = f"âŒ FAILED: {e}"
    
    def test_5_audio_projector(self):
        """æµ‹è¯•5: éŸ³é¢‘æŠ•å½±å™¨"""
        print(f"\n{'='*50}")
        print("ğŸµ æµ‹è¯•5: éŸ³é¢‘æŠ•å½±å™¨")
        print("="*50)
        
        try:
            # æµ‹è¯•éŸ³é¢‘æŠ•å½±å™¨
            assert self.triper_model.audio_projector is not None, "éŸ³é¢‘æŠ•å½±å™¨ä¸ºç©º"
            
            # åˆ›å»ºæµ‹è¯•éŸ³é¢‘ç‰¹å¾
            test_audio = torch.randn(1, 64, 1280).to(self.triper_model.device)
            
            print(f"ğŸ”„ æµ‹è¯•éŸ³é¢‘æŠ•å½±å™¨...")
            print(f"  è¾“å…¥å½¢çŠ¶: {test_audio.shape}")
            
            with torch.no_grad():
                projected_audio = self.triper_model.audio_projector(test_audio)
            
            print(f"  è¾“å‡ºå½¢çŠ¶: {projected_audio.shape}")
            print(f"  è¾“å‡ºç»´åº¦: {projected_audio.shape[-1]}")
            
            # éªŒè¯è¾“å‡º
            expected_dim = self.triper_model.llava_model.config.hidden_size  # åº”è¯¥æ˜¯5120
            assert projected_audio.shape[-1] == expected_dim, f"æŠ•å½±ç»´åº¦é”™è¯¯: {projected_audio.shape[-1]} != {expected_dim}"
            assert not torch.isnan(projected_audio).any(), "æŠ•å½±è¾“å‡ºåŒ…å«NaN"
            
            print(f"âœ… éŸ³é¢‘æŠ•å½±å™¨æµ‹è¯•é€šè¿‡")
            
            # æµ‹è¯•éŸ³é¢‘ç‰¹å¾æ’å…¥
            print(f"ğŸ”„ æµ‹è¯•éŸ³é¢‘ç‰¹å¾æ’å…¥...")
            test_text_embeds = torch.randn(1, 50, expected_dim).to(self.triper_model.device)
            test_attention_mask = torch.ones(1, 50).to(self.triper_model.device)
            
            combined_embeds, combined_mask = self.triper_model._insert_audio_features(
                test_text_embeds, None, test_audio, test_attention_mask
            )
            
            expected_length = 50 + 64  # æ–‡æœ¬é•¿åº¦ + éŸ³é¢‘é•¿åº¦
            assert combined_embeds.shape[1] == expected_length, f"åˆå¹¶é•¿åº¦é”™è¯¯: {combined_embeds.shape[1]} != {expected_length}"
            assert combined_mask.shape[1] == expected_length, f"maské•¿åº¦é”™è¯¯: {combined_mask.shape[1]} != {expected_length}"
            
            print(f"âœ… éŸ³é¢‘ç‰¹å¾æ’å…¥æµ‹è¯•é€šè¿‡")
            
            self.results['audio_projector'] = "âœ… PASSED"
            
        except Exception as e:
            print(f"âŒ éŸ³é¢‘æŠ•å½±å™¨æµ‹è¯•å¤±è´¥: {e}")
            traceback.print_exc()
            self.results['audio_projector'] = f"âŒ FAILED: {e}"
    
    def test_6_parameter_statistics(self):
        """æµ‹è¯•6: å‚æ•°ç»Ÿè®¡"""
        print(f"\n{'='*50}")
        print("ğŸ“Š æµ‹è¯•6: å‚æ•°ç»Ÿè®¡å’Œé…ç½®")
        print("="*50)
        
        try:
            # è·å–å‚æ•°ç»Ÿè®¡
            stats = self.triper_model.get_parameter_stats()
            
            print(f"ğŸ“Š å‚æ•°ç»Ÿè®¡:")
            print(f"  æ€»å‚æ•°: {stats['total_params']:,}")
            print(f"  å¯è®­ç»ƒå‚æ•°: {stats['trainable_params']:,}")
            print(f"  å¯è®­ç»ƒæ¯”ä¾‹: {stats['trainable_params']/max(stats['total_params'], 1)*100:.2f}%")
            
            # éªŒè¯å‚æ•°ç»Ÿè®¡åˆç†æ€§
            assert stats['total_params'] > 0, "æ€»å‚æ•°æ•°é‡ä¸º0"
            assert stats['trainable_params'] >= 0, "å¯è®­ç»ƒå‚æ•°æ•°é‡ä¸ºè´Ÿ"
            
            # æ£€æŸ¥ç»„ä»¶å‚æ•°
            for component, comp_stats in stats['components'].items():
                print(f"  {component}: {comp_stats['total']:,} æ€»è®¡, {comp_stats['trainable']:,} å¯è®­ç»ƒ")
                
                if component == 'audio_projector':
                    assert comp_stats['trainable'] > 0, "éŸ³é¢‘æŠ•å½±å™¨åº”è¯¥æ˜¯å¯è®­ç»ƒçš„"
                elif component == 'llava':
                    if self.config.get('freeze_llava', True):
                        assert comp_stats['trainable'] == 0, "LLaVAåº”è¯¥è¢«å†»ç»“"
            
            # æµ‹è¯•è®¾å¤‡ç§»åŠ¨
            print(f"ğŸ”„ æµ‹è¯•è®¾å¤‡ç§»åŠ¨...")
            original_device = self.triper_model.device
            print(f"  å½“å‰è®¾å¤‡: {original_device}")
            
            # æµ‹è¯•æ¨¡å‹å°±ç»ªçŠ¶æ€
            assert self.triper_model.is_ready(), "æ¨¡å‹åº”è¯¥å¤„äºå°±ç»ªçŠ¶æ€"
            
            # æµ‹è¯•ç»„ä»¶è®¿é—®å™¨
            assert self.triper_model.tokenizer is not None, "tokenizerè®¿é—®å™¨å¤±è´¥"
            assert self.triper_model.image_processor is not None, "image_processorè®¿é—®å™¨å¤±è´¥"
            assert self.triper_model.audio_encoder is not None, "audio_encoderè®¿é—®å™¨å¤±è´¥"
            
            print(f"âœ… å‚æ•°ç»Ÿè®¡å’Œé…ç½®æµ‹è¯•é€šè¿‡")
            
            self.results['parameter_statistics'] = "âœ… PASSED"
            
        except Exception as e:
            print(f"âŒ å‚æ•°ç»Ÿè®¡æµ‹è¯•å¤±è´¥: {e}")
            traceback.print_exc()
            self.results['parameter_statistics'] = f"âŒ FAILED: {e}"
    
    def test_7_conversation_prediction(self):
        """æµ‹è¯•7: å¯¹è¯é¢„æµ‹ä»»åŠ¡"""
        print(f"\n{'='*50}")
        print("ğŸ’¬ æµ‹è¯•7: å¯¹è¯é¢„æµ‹ä»»åŠ¡")
        print("="*50)
        
        try:
            # æµ‹è¯•ä¸åŒç±»å‹çš„å¯¹è¯é¢„æµ‹prompt
            test_prompts = [
                f"{DEFAULT_IMAGE_TOKEN}\nUSER: What do you see in this image?\nASSISTANT:",
                f"{DEFAULT_IMAGE_TOKEN}\nUSER: In this scene, someone says 'Hello there'. What would be a natural response?\nASSISTANT:",
                f"{DEFAULT_IMAGE_TOKEN}\nUSER: Based on what you see and hear, what conversation would happen here?\nASSISTANT:",
            ]
            
            single_batch = self.collator([self.dataset[0]])
            device_batch = {}
            for k, v in single_batch.items():
                if hasattr(v, 'to'):
                    device_batch[k] = v.to(self.triper_model.device)
                else:
                    device_batch[k] = v
            
            for i, prompt in enumerate(test_prompts):
                print(f"ğŸ”„ æµ‹è¯•prompt {i+1}: {prompt[:50]}...")
                
                prompt_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(self.triper_model.device)
                
                try:
                    response = self.triper_model.llava_model.generate(
                        inputs=prompt_ids,
                        images=device_batch['images'],
                        max_new_tokens=50,
                        do_sample=True,
                        temperature=0.7,
                        top_p=0.9,
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id,
                    )
                    
                    if response.shape[1] > prompt_ids.shape[1]:
                        generated = self.tokenizer.decode(
                            response[0, prompt_ids.shape[1]:], 
                            skip_special_tokens=True
                        )
                        print(f"  âœ… ç”ŸæˆæˆåŠŸ: '{generated[:80]}...'")
                    else:
                        print(f"  âš ï¸ ç”Ÿæˆé•¿åº¦å¼‚å¸¸: {response.shape}")
                        
                except Exception as e:
                    print(f"  âŒ Prompt {i+1} å¤±è´¥: {e}")
            
            self.results['conversation_prediction'] = "âœ… PASSED"
            
        except Exception as e:
            print(f"âŒ å¯¹è¯é¢„æµ‹æµ‹è¯•å¤±è´¥: {e}")
            traceback.print_exc()
            self.results['conversation_prediction'] = f"âŒ FAILED: {e}"
    
    def test_8_edge_cases(self):
        """æµ‹è¯•8: è¾¹ç•Œæƒ…å†µ"""
        print(f"\n{'='*50}")
        print("ğŸ” æµ‹è¯•8: è¾¹ç•Œæƒ…å†µå’Œé”™è¯¯å¤„ç†")
        print("="*50)
        
        try:
            # æµ‹è¯•ç©ºè¾“å…¥
            print("ğŸ”„ æµ‹è¯•ç©ºéŸ³é¢‘è¾“å…¥...")
            single_batch = self.collator([self.dataset[0]])
            device_batch = {}
            for k, v in single_batch.items():
                if hasattr(v, 'to'):
                    device_batch[k] = v.to(self.triper_model.device)
                else:
                    device_batch[k] = v
            
            # æµ‹è¯•åªæœ‰æ–‡æœ¬
            try:
                 # åˆ›å»ºçœŸæ­£çš„çº¯æ–‡æœ¬ï¼ˆä¸åŒ…å«IMAGE tokenï¼‰
                pure_text = "USER: What is artificial intelligence?\nASSISTANT:"
                pure_text_ids = self.tokenizer.encode(pure_text, return_tensors="pt")
                pure_text_ids = pure_text_ids.to(self.triper_model.device)
                
                print(f"ğŸ“ çº¯æ–‡æœ¬å†…å®¹: {pure_text}")
                print(f"ğŸ“ TokenèŒƒå›´: ({pure_text_ids.min()}, {pure_text_ids.max()})")
                
                with torch.no_grad():
                    output = self.triper_model(
                        input_ids=pure_text_ids,
                        images=None,
                        audio_features=None,
                    )
                print("âœ… çº¯æ–‡æœ¬è¾“å…¥æµ‹è¯•é€šè¿‡")
            except Exception as e:
                print(f"âš ï¸ çº¯æ–‡æœ¬è¾“å…¥æµ‹è¯•å¤±è´¥: {e}")
            
            # æµ‹è¯•å¼‚å¸¸éŸ³é¢‘ç‰¹å¾
            print("ğŸ”„ æµ‹è¯•å¼‚å¸¸éŸ³é¢‘ç‰¹å¾...")
            try:
                wrong_audio = torch.randn(1, 32, 1280).to(self.triper_model.device)  # é”™è¯¯é•¿åº¦
                projected = self.triper_model.audio_projector(wrong_audio)
                print(f"âœ… éŸ³é¢‘æŠ•å½±å™¨èƒ½å¤„ç†ä¸åŒé•¿åº¦: {projected.shape}")
            except Exception as e:
                print(f"âš ï¸ éŸ³é¢‘æŠ•å½±å™¨å¯¹å¼‚å¸¸è¾“å…¥æ•æ„Ÿ: {e}")
            
            # æµ‹è¯•æé•¿è¾“å…¥
            print("ğŸ”„ æµ‹è¯•é•¿è¾“å…¥...")
            try:
                long_text = "USER: " + "This is a very long text. " * 50 + "\nASSISTANT:"
                long_ids = self.tokenizer.encode(long_text, return_tensors="pt")
                if long_ids.shape[1] > 2000:  # å¦‚æœç¡®å®å¾ˆé•¿
                    long_ids = long_ids[:, :100]  # æˆªæ–­æµ‹è¯•
                
                long_ids = long_ids.to(self.triper_model.device)
                response = self.triper_model.llava_model.generate(
                    inputs=long_ids,
                    max_new_tokens=10,
                    do_sample=False,
                )
                print(f"âœ… é•¿è¾“å…¥æµ‹è¯•é€šè¿‡: {response.shape}")
            except Exception as e:
                print(f"âš ï¸ é•¿è¾“å…¥æµ‹è¯•å¤±è´¥: {e}")
            
            self.results['edge_cases'] = "âœ… PASSED"
            
        except Exception as e:
            print(f"âŒ è¾¹ç•Œæƒ…å†µæµ‹è¯•å¤±è´¥: {e}")
            traceback.print_exc()
            self.results['edge_cases'] = f"âŒ FAILED: {e}"
    
    def generate_test_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print(f"\n{'='*80}")
        print("ğŸ“‹ TRIPER MODEL TEST REPORT")
        print("="*80)
        
        total_tests = len(self.results)
        passed_tests = sum(1 for result in self.results.values() if result.startswith("âœ…"))
        
        print(f"\nğŸ“Š æµ‹è¯•æ¦‚å†µ:")
        print(f"  æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"  é€šè¿‡æµ‹è¯•: {passed_tests}")
        print(f"  å¤±è´¥æµ‹è¯•: {total_tests - passed_tests}")
        print(f"  é€šè¿‡ç‡: {passed_tests/total_tests*100:.1f}%")
        
        print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ:")
        for test_name, result in self.results.items():
            print(f"  {test_name}: {result}")
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = "/home/wly/szl_all_code/triper-project/test_report.json"
        with open(report_path, 'w') as f:
            json.dump({
                'summary': {
                    'total_tests': total_tests,
                    'passed_tests': passed_tests,
                    'pass_rate': passed_tests/total_tests*100
                },
                'results': self.results,
                'config': self.config
            }, f, indent=2)
        
        print(f"\nğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
        
        if passed_tests == total_tests:
            print(f"\nğŸ‰ æ­å–œï¼æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼")
        else:
            print(f"\nâš ï¸ æœ‰ {total_tests - passed_tests} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å…·ä½“é”™è¯¯ã€‚")
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        try:
            self.setup_test_environment()
            self.test_1_model_loading()
            self.test_2_data_loading()
            self.test_3_forward_pass()
            self.test_4_generation_capability()
            self.test_5_audio_projector()
            self.test_6_parameter_statistics()
            self.test_7_conversation_prediction()
            self.test_8_edge_cases()
        finally:
            self.generate_test_report()

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®æµ‹è¯•å‚æ•°
    test_config = {
        'llava_model_path': "/sda1/llava-v1.5-13b",
        'audio_encoder_path': "/sda1/glm-4-voice-tokenizer",
        'data_path': '/home/wly/szl_all_code/triper-project/data/simple_data_20_samples.json',
        'media_root': '/home/wly/szl_all_code/triper-project/data',
        'device': "cuda:3",
        'freeze_llava': True
    }
    
    # åˆ›å»ºæµ‹è¯•å™¨å¹¶è¿è¡Œ
    tester = TriperModelTester(test_config)
    tester.run_all_tests()

if __name__ == "__main__":
    main()