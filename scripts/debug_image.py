#!/usr/bin/env python3
"""
è°ƒè¯•LLaVAçš„prepare_inputs_labels_for_multimodalå‡½æ•°
ç”¨äºæ‰¾å‡ºå›¾åƒç‰¹å¾æ²¡æœ‰è¢«æ­£ç¡®æ’å…¥çš„åŸå› 
"""

import os
import sys
import torch
import types
from typing import Optional, List, Union, Tuple

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/home/wly/szl_all_code/triper-project')

from triper.model import from_pretrained_components
from triper.data import TriperDataset, TriperDataCollator
from llava.constants import IMAGE_TOKEN_INDEX

def setup_environment():
    """è®¾ç½®ç¯å¢ƒ"""
    torch.cuda.empty_cache()
    print("ğŸš€ ç¯å¢ƒè®¾ç½®å®Œæˆ")

def load_models():
    """åŠ è½½æ¨¡å‹å’Œç»„ä»¶"""
    print("ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹...")
    
    audio_config = {
        'mm_audio_encoder': 'whisper_vq',
        'audio_hidden_size': 1280,
        'audio_model_path': '/sda1/glm-4-voice-tokenizer',
        'audio_projector_type': 'mlp2x_gelu',
        'audio_projector_hidden_dim': 2048,
        'dropout': 0.1
    }
    
    tokenizer, triper_model, image_processor, context_len, audio_encoder = from_pretrained_components(
        llava_model_path="/sda1/llava-v1.5-13b",
        audio_encoder_path="/sda1/glm-4-voice-tokenizer",
        audio_projector_path=None,
        audio_config=audio_config,
        freeze_llava=True,
        device_map="cuda:3"
    )
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    triper_model.get_parameter_stats()
    
    return tokenizer, triper_model, image_processor, audio_encoder

def load_dataset(tokenizer, image_processor, audio_encoder, triper_model):
    """åŠ è½½æ•°æ®é›†"""
    print("ğŸ“Š æ­£åœ¨åŠ è½½æ•°æ®é›†...")
    
    dataset = TriperDataset(
        json_path='/home/wly/szl_all_code/triper-project/data/simple_data_20_samples.json',
        media_root_path='/home/wly/szl_all_code/triper-project/data',
        mode="raw"
    )
    
    collator = TriperDataCollator(
        tokenizer=tokenizer,
        image_processor=image_processor,
        audio_processor=audio_encoder,
        model_cfg=triper_model.llava_model.config
    )
    
    print("âœ… æ•°æ®é›†åŠ è½½å®Œæˆ")
    return dataset, collator

def create_debug_function(original_func):
    """åˆ›å»ºè°ƒè¯•ç‰ˆæœ¬çš„prepare_inputs_labels_for_multimodalå‡½æ•°"""
    
    def debug_prepare_inputs_labels_for_multimodal(self, input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes=None):
        print("\n" + "="*60)
        print("ğŸ” è¿›å…¥prepare_inputs_labels_for_multimodalå‡½æ•°")
        print("="*60)
        
        print(f"ğŸ“ å‚æ•°ä¿¡æ¯:")
        print(f"  input_ids shape: {input_ids.shape}")
        print(f"  input_ids content: {input_ids}")
        print(f"  images shape: {images.shape if images is not None else None}")
        print(f"  attention_mask: {attention_mask}")
        print(f"  labels: {labels}")
        print(f"  image_sizes: {image_sizes}")
        print(f"  position_ids: {position_ids}")
        print(f"  past_key_values: {past_key_values}")
        
        # æ£€æŸ¥å›¾åƒtoken
        if images is not None:
            image_token_indices = torch.where(input_ids == IMAGE_TOKEN_INDEX)
            print(f"\nğŸ–¼ï¸ å›¾åƒtokenåˆ†æ:")
            print(f"  IMAGE_TOKEN_INDEX: {IMAGE_TOKEN_INDEX}")
            print(f"  å›¾åƒtokenä½ç½®: {image_token_indices}")
            print(f"  å›¾åƒtokenæ•°é‡: {len(image_token_indices[0]) if len(image_token_indices) > 0 else 0}")
            
            # æ£€æŸ¥æ¯ä¸ªä½ç½®çš„token
            for batch_idx in range(input_ids.shape[0]):
                batch_tokens = input_ids[batch_idx]
                image_positions = torch.where(batch_tokens == IMAGE_TOKEN_INDEX)[0]
                print(f"  æ‰¹æ¬¡ {batch_idx} å›¾åƒtokenä½ç½®: {image_positions.tolist()}")
        
        # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
        print(f"\nğŸ”§ æ¨¡å‹çŠ¶æ€:")
        print(f"  æ¨¡å‹è®­ç»ƒæ¨¡å¼: {self.training}")
        print(f"  Vision towerå­˜åœ¨: {hasattr(self, 'get_vision_tower') and self.get_vision_tower() is not None}")
        
        if hasattr(self, 'get_vision_tower') and self.get_vision_tower() is not None:
            vision_tower = self.get_vision_tower()
            print(f"  Vision towerå·²åŠ è½½: {vision_tower.is_loaded}")
            print(f"  Vision towerè®¾å¤‡: {next(vision_tower.parameters()).device}")
        
        # è®¾ç½®æ–­ç‚¹è¿›è¡Œäº¤äº’å¼è°ƒè¯•
        print(f"\nâš ï¸  å³å°†è¿›å…¥è°ƒè¯•æ¨¡å¼...")
        print(f"è°ƒè¯•æç¤º:")
        print(f"  - ä½¿ç”¨ 'n' ä¸‹ä¸€è¡Œ")
        print(f"  - ä½¿ç”¨ 's' æ­¥å…¥å‡½æ•°")
        print(f"  - ä½¿ç”¨ 'c' ç»§ç»­æ‰§è¡Œ")
        print(f"  - ä½¿ç”¨ 'p å˜é‡å' æŸ¥çœ‹å˜é‡")
        print(f"  - ä½¿ç”¨ 'q' é€€å‡ºè°ƒè¯•")
        
        import pdb; pdb.set_trace()
        
        # è°ƒç”¨åŸå§‹å‡½æ•°
        print(f"\nğŸ”„ è°ƒç”¨åŸå§‹å‡½æ•°...")
        result = original_func(
            input_ids=input_ids,
            position_ids=position_ids, 
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            labels=labels,
            images=images,
            image_sizes=image_sizes
        )
        
        # åˆ†æç»“æœ
        print(f"\nğŸ“Š å‡½æ•°è¿”å›ç»“æœåˆ†æ:")
        if isinstance(result, (list, tuple)) and len(result) >= 6:
            new_input_ids, new_position_ids, new_attention_mask, new_past_key_values, inputs_embeds, new_labels = result
            
            print(f"  new_input_ids: {new_input_ids.shape if new_input_ids is not None else None}")
            print(f"  inputs_embeds: {inputs_embeds.shape if inputs_embeds is not None else None}")
            print(f"  new_attention_mask: {new_attention_mask.shape if new_attention_mask is not None else None}")
            
            if inputs_embeds is not None:
                original_len = input_ids.shape[1]
                new_len = inputs_embeds.shape[1]
                print(f"  é•¿åº¦å˜åŒ–: {original_len} â†’ {new_len}")
                
                if new_len > original_len:
                    print(f"  âœ… å›¾åƒç‰¹å¾å·²æ’å…¥ï¼å¢åŠ äº† {new_len - original_len} ä¸ªtoken")
                else:
                    print(f"  âŒ å›¾åƒç‰¹å¾æœªæ’å…¥ï¼é•¿åº¦æœªå˜åŒ–")
        
        print("="*60)
        return result
    
    return debug_prepare_inputs_labels_for_multimodal

def debug_single_sample(triper_model, dataset, collator):
    """è°ƒè¯•å•ä¸ªæ ·æœ¬"""
    print("\nğŸ§ª å¼€å§‹è°ƒè¯•å•ä¸ªæ ·æœ¬...")
    
    # å‡†å¤‡æ•°æ®
    single_sample = dataset[0]
    batch_result = collator([single_sample])
    batch_result = {k: v.to(triper_model.device) for k, v in batch_result.items()}
    
    input_ids = batch_result['input_ids']
    images = batch_result['images']
    
    print(f"ğŸ“ è¾“å…¥æ•°æ®:")
    print(f"  æ ·æœ¬ç»“æ„: {single_sample.keys()}")
    print(f"  input_ids: {input_ids}")
    print(f"  images shape: {images.shape}")
    
    # ä¿å­˜åŸå§‹å‡½æ•°å¹¶æ›¿æ¢ä¸ºè°ƒè¯•ç‰ˆæœ¬
    original_prepare_func = triper_model.llava_model.prepare_inputs_labels_for_multimodal
    debug_func = create_debug_function(original_prepare_func)
    
    triper_model.llava_model.prepare_inputs_labels_for_multimodal = types.MethodType(
        debug_func, 
        triper_model.llava_model
    )
    
    try:
        print(f"\nğŸš€ å¼€å§‹è°ƒç”¨prepare_inputs_labels_for_multimodal...")
        with torch.no_grad():
            result = triper_model.llava_model.prepare_inputs_labels_for_multimodal(
                input_ids=input_ids,
                position_ids=None,
                attention_mask=None,
                past_key_values=None,
                labels=None,
                images=images,
                image_sizes=None
            )
        
        print(f"\nâœ… è°ƒç”¨å®Œæˆ!")
        if len(result) >= 5:
            inputs_embeds = result[4]
            print(f"  æœ€ç»ˆinputs_embeds shape: {inputs_embeds.shape if inputs_embeds is not None else None}")
            
    except Exception as e:
        print(f"\nâŒ è°ƒç”¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        # æ¢å¤åŸå§‹å‡½æ•°
        triper_model.llava_model.prepare_inputs_labels_for_multimodal = original_prepare_func
        print(f"\nğŸ”„ å·²æ¢å¤åŸå§‹å‡½æ•°")

def test_triper_model(triper_model, dataset, collator):
    """æµ‹è¯•å®Œæ•´çš„TriperModel"""
    print(f"\nğŸ§ª æµ‹è¯•å®Œæ•´çš„TriperModel...")
    
    single_sample = dataset[0]
    batch_result = collator([single_sample])
    batch_result = {k: v.to(triper_model.device) for k, v in batch_result.items()}
    
    try:
        with torch.no_grad():
            output = triper_model(
                input_ids=batch_result['input_ids'],
                images=batch_result['images'],
                audio_features=batch_result['audio_features']
            )
        
        print("âœ… TriperModelæ¨ç†æˆåŠŸï¼")
        print(f"  è¾“å‡ºç»“æ„: {output.keys()}")
        print(f"  è¾“å‡ºlogitså½¢çŠ¶: {output['logits'].shape}")
        
    except RuntimeError as e:
        print(f"âŒ TriperModelæ¨ç†å¤±è´¥: {e}")
        if "device" in str(e).lower():
            print("è¿™å¯èƒ½æ˜¯å¤šGPUæ¨¡å‹çš„è®¾å¤‡åˆ†å¸ƒé—®é¢˜")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ› LLaVAå¤šæ¨¡æ€å‡½æ•°è°ƒè¯•å™¨")
    print("="*60)
    
    # 1. è®¾ç½®ç¯å¢ƒ
    setup_environment()
    
    # 2. åŠ è½½æ¨¡å‹
    tokenizer, triper_model, image_processor, audio_encoder = load_models()
    
    # 3. åŠ è½½æ•°æ®é›†
    dataset, collator = load_dataset(tokenizer, image_processor, audio_encoder, triper_model)
    
    # 4. è°ƒè¯•å•ä¸ªæ ·æœ¬
    debug_single_sample(triper_model, dataset, collator)
    
    # 5. æµ‹è¯•å®Œæ•´æ¨¡å‹
    test_triper_model(triper_model, dataset, collator)
    
    print("\nğŸ‰ è°ƒè¯•å®Œæˆï¼")

if __name__ == "__main__":
    main()